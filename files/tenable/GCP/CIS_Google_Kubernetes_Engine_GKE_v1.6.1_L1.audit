#TRUSTED 327b09632dea470542189960166d53135a527dc68c836706a422b47bcd3fd434160b298014287998a61d8edd1441647a055f1a09aea14bac4490b79663f6a251fa03ae5bcd26d98297668157607d69a94ab09c745e9ce8d135ab18d575945abce372bd7be5fdc58da4189bdca40b5d43af415b6b1f8cd3ec53aff82dd54daa801b63a6789a315ce690afaddd1e24c047705ebd4a3d21fa6f57bbb03ec7037ebd985107f3e009218dc842fdc749fc22c9876ee108a98b0973300b35232cdf335a97d884d3df9438e97f4be73fc83fd7c0ad92fef6e541cf8f4dc62251e81375a2a492a98bb402345a5ebe3bd3a8e31adcce893623cc5e5974f1d62b0793c9aa526225d0d9344f6b17c7a29a3721fc9ff86d4d00b9f8f9a3095b03d058376ca611cfbfdc27c0497f84fdf77a4f5f253abcec9a90876c0de967738f641ed69d10c92ef9ca8d6a390d3131f4cfc46b29815150f5e048f039a379d3c1341fae96bd10b4a2a57211853f8dfc803e2968aa173878bd82423cbc6a0c97b20f38dc2a8ea72bb33eec0d4d6b1ce356e0ec280a9a7a0dfbc7048e2753ae7902cfce4c56b21710d90a0cf7256502d64d2075a3e72c5c033f9aa6f657ed1cc7708c2b5b56e07c12cdeb2fffe4e5ab54a9c8d1f7c4becbb5964febc90c02c9e14ed16ec3c77fdc1f3c7f4ffa19ec12f34979b8a645db26c5a311000f6b6f4e269af212a5b5c107
#TRUST-RSA-SHA256 10ce8459ac9ec19c0cf887c9f6c9ac7788f04360e5de23b719ad96ed60e299a33b90c52431dd055309d6c285918a5b49d14d30cfead20f516771f765238752c71f86252e4a2d6997fb1359742a6d618738238f6c923252e3b3b3e9fd00b6015ec2eef49cc6632eed31398f6838a030c8c372fdcb99b563fa8153c7d3b60b2e3e384cd2a14deb2c5420015b23f68697536e91d4fa86c73d4ae591fe69a42068de30d7d026f96e875fd6531c29c13e0ee9730157e248b1f6ae011fd107dadd7ccaa2cc9b634e7741d19c9a6a5981e23ad85260d45e4963b28892480ef0d51ae640955877f70c5fd74d3e6adfc3cb0ec154a3f6aa9e64d409e48733acbdd702bdb38ead02b9dec106fb4e430ec1a1838b2c03fc1f333a529b8d7b4cfaca9e322ef5fbf1c6a26576c840477cee05227e7e46f1a0276c2962fd8805fb1575d8e7f43ef555197c763892a92e8f6e01190658496b204707914c64913ba390d052daec55d378a017495047acbb347f5020518fd6465fc53b6eca5feedad708cd83eab9477b377042768949632e360982e3357cb7b6065f48bfa07c64c45a308c61db2554c1349f391d372c88d79c1d9e6570e393fc352331bc9155c94a0bf4e475c4e9273d81b71b6e16613e32bd8a896aa45302d8b5a8ec75fa91d8473570dfef0e866ad69b221cb4dda5758022313d188638588e838e6e8589c7a0fd7b19a8e9fb14fe
#
# This script is Copyright (C) 2004-2024 and is owned by Tenable, Inc. or an Affiliate thereof.
#
# This script is released under the Tenable Subscription License and
# may not be used from within scripts released under another license
# without authorization from Tenable, Inc.
#
# See the following licenses for details:
#
# http://static.tenable.com/prod_docs/Nessus_6_SLA_and_Subscription_Agreement.pdf
#
# @PROFESSIONALFEED@
# $Revision: 1.0 $
# $Date: 2024/10/28 $
#
# description : This .audit is designed against the CIS Google Kubernetes Engine (GKE) Benchmark 1.6.1
#
#<ui_metadata>
#<display_name>CIS Google Kubernetes Engine (GKE) v1.6.1 L1</display_name>
#<spec>
#  <type>CIS</type>
#  <name>Google Kubernetes Engine (GKE)</name>
#  <profile>L1</profile>
#  <version>1.6.1</version>
#  <link>https://workbench.cisecurity.org/benchmarks/19166</link>
#</spec>
#<labels>gke,kubernetes</labels>
#<benchmark_refs>CSCv6,CSCv7,CSCv8,LEVEL</benchmark_refs>
#</ui_metadata>

<check_type:"GCP">

<custom_item>
  type           : REST_API
  description    : "2.1.1 Client certificate authentication should not be used for users"
  info           : "Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose.

It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.

With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation.

See also Recommendation 5.8.1 for GKE specifically."
  solution       : "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates.

You can remediate the availability of client certificates in your GKE cluster. See Recommendation 5.8.1.

Impact:

External mechanisms for authentication generally require additional software to be deployed."
  reference      : "800-171|3.1.1,800-53|AC-1,800-53|AC-2,800-53|AC-2(1),800-53r5|AC-1,800-53r5|AC-2,800-53r5|AC-2(1),CN-L3|7.1.3.2(d),CN-L3|8.1.4.2(e),CN-L3|8.1.10.6(c),CSCv7|4.3,CSCv8|6.2,CSF|DE.CM-1,CSF|DE.CM-3,CSF|ID.GV-1,CSF|ID.GV-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|GV.OC-03,CSF2.0|GV.OV-01,CSF2.0|GV.PO-01,CSF2.0|GV.PO-02,CSF2.0|GV.SC-03,CSF2.0|ID.IM-01,CSF2.0|ID.IM-02,CSF2.0|ID.IM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.1.1,ISO/IEC-27001|A.9.2.1,ITSG-33|AC-1,ITSG-33|AC-2,ITSG-33|AC-2(1),LEVEL|1A,NESA|M1.2.2,NIAv2|AM28,NIAv2|AM29,NIAv2|AM30,NIAv2|NS5j,NIAv2|SS14e,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listSecrets_kube-system"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Secret Name: \(.metadata.name)\""
  regex          : "Secret Name: "
  not_expect     : "Secret Name: gke-"
</custom_item>

<custom_item>
  type           : REST_API
  description    : "4.1.1 Ensure that the cluster-admin role is only used where required"
  info           : "The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.

Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding it gives full control over every resource in the rolebinding's namespace, including the namespace itself.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
  solution       : "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.

Where possible, first bind users to a lower-privileged role and then remove the clusterrolebinding to the cluster-admin role :

kubectl delete clusterrolebinding [name]

Impact:

Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components."
  reference      : "800-171|3.1.5,800-171|3.1.6,800-53|AC-6(2),800-53|AC-6(5),800-53r5|AC-6(2),800-53r5|AC-6(5),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.10.6(a),CSCv7|4.3,CSCv8|5.4,CSF|PR.AC-4,CSF2.0|PR.AA-05,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.3,ITSG-33|AC-6(2),ITSG-33|AC-6(5),LEVEL|1A,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.6.1,NIAv2|AM1,NIAv2|AM23f,NIAv2|AM32,NIAv2|AM33,NIAv2|SS13c,NIAv2|SS15c,NIAv2|VL3a,PCI-DSSv3.2.1|7.1.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,QCSC-v1|5.2.2,QCSC-v1|6.2,SWIFT-CSCv1|1.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listClusterRolebindings"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
  regex          : "Role: cluster-admin"
  not_expect     : "Role: cluster-admin"
  severity       : MEDIUM
</custom_item>

<if>
  <condition auto:"FAILED" type:"AND">
    <custom_item>
      type           : REST_API
      description    : "Cluster Role Bindings"
      request        : "listClusterRolebindings"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
      regex          : "Subject: system:authenticated"
      expect         : "Role: system:(basic-user|discovery|public-info-viewer)"
      match_all      : YES
    </custom_item>

    <custom_item>
      type           : REST_API
      description    : "Role Bindings"
      request        : "listRoleBindings"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
      regex          : "Subject: system:authenticated"
      not_expect     : "Subject: system:authenticated"
    </custom_item>
  </condition>

  <then>
    <report type:"PASSED">
      description : "4.1.10 Avoid non-default bindings to system:authenticated"
      info        : "Avoid non-default ClusterRoleBindings and RoleBindings with the group system:authenticated except the ClusterRoleBindings system:basic-user system:discovery and system:public-info-viewer

Google's approach to authentication is to make authenticating to Google Cloud and GKE as simple and secure as possible without adding complex configuration steps. The group system:authenticated includes all users with a Google account, which includes all Gmail accounts. Consider your authorization controls with this extended group scope when granting permissions. Thus, group system:authenticated is not recommended for non-default use.

GKE assigns the group system:authenticated to API server requests made by any user who is signed in with a Google Account, including all Gmail accounts. In practice, this isn't meaningfully different from system:unauthenticated because anyone can create a Google Account.

Binding a role to the group system:authenticated gives any user with a Google Account, including all Gmail accounts, the permissions granted by that role and is strongly discouraged."
      solution    : "Identify all non-default clusterrolebindings and rolebindings to the group system:authenticated Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation.

Strongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.

If there are any non-default, unsafe bindings to the group system:authenticated proceed to delete them after consideration for cluster operations with only necessary, safer bindings.

kubectl delete clusterrolebinding
[CLUSTER_ROLE_BINDING_NAME] kubectl delete rolebinding
[ROLE_BINDING_NAME]
--namespace
[ROLE_BINDING_NAMESPACE]

Impact:

Authenticated users in group system:authenticated should be treated similarly to users in system:unauthenticated having privileges and permissions associated with roles associated with the configured bindings.

Care should be taken before removing any non-default clusterrolebindings or rolebindings from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and authenticated user for cluster operations."
      reference   : "800-171|3.1.1,800-53|AC-2,800-53r5|AC-2,CN-L3|7.1.3.2(d),CSCv7|16.8,CSCv8|5.5,CSF|DE.CM-1,CSF|DE.CM-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.1,ITSG-33|AC-2,LEVEL|1A,NIAv2|AM28,NIAv2|NS5j,NIAv2|SS14e,QCSC-v1|5.2.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2"
      see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
      show_output : YES
    </report>
  </then>
</if>

<report type:"WARNING">
  description : "4.1.2 Minimize access to secrets"
  info        : "The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.

Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Where possible, remove get list and watch access to secret objects in the cluster.

Impact:

Care should be taken not to remove access to secrets to system components which require this for their operation"
  reference   : "800-171|3.4.1,800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-171|3.13.1,800-171|3.13.2,800-53|CM-2,800-53|CM-6,800-53|CM-7,800-53|CM-7(1),800-53|CM-9,800-53|SA-3,800-53|SA-8,800-53|SA-10,800-53r5|CM-1,800-53r5|CM-2,800-53r5|CM-6,800-53r5|CM-7,800-53r5|CM-7(1),800-53r5|CM-9,800-53r5|SA-3,800-53r5|SA-8,800-53r5|SA-10,CSCv7|5.2,CSCv8|4.1,CSF|DE.AE-1,CSF|PR.DS-7,CSF|PR.IP-1,CSF|PR.IP-2,CSF|PR.IP-3,CSF|PR.PT-3,CSF2.0|DE.CM-09,CSF2.0|ID.AM-08,CSF2.0|ID.IM-01,CSF2.0|ID.IM-02,CSF2.0|ID.IM-03,CSF2.0|ID.RA-09,CSF2.0|PR.DS-10,CSF2.0|PR.IR-03,CSF2.0|PR.PS-01,CSF2.0|PR.PS-06,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-2,ITSG-33|CM-6,ITSG-33|CM-7,ITSG-33|CM-7(1),ITSG-33|CM-9,ITSG-33|SA-3,ITSG-33|SA-8,ITSG-33|SA-8a.,ITSG-33|SA-10,LEVEL|1A,NESA|T1.2.1,NESA|T1.2.2,NESA|T3.2.5,NESA|T3.4.1,NESA|T4.5.3,NESA|T4.5.4,NESA|T7.2.1,NESA|T7.5.1,NESA|T7.5.3,NESA|T7.6.1,NESA|T7.6.2,NESA|T7.6.3,NESA|T7.6.5,NIAv2|SS3,NIAv2|SS15a,NIAv2|SS16,NIAv2|VL2,PCI-DSSv3.2.1|2.2.2,QCSC-v1|3.2,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,SWIFT-CSCv1|2.3"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<if>
  <condition auto:"WARNING" type:"AND">
    <custom_item>
      type           : REST_API
      description    : "Cluster Roles"
      request        : "listClusterRoles"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .metadata.name as $roleName | .rules[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($roleName), API Groups: \(.apiGroups), Resources: \(.resources), Verbs: \(.verbs)\""
      regex          : "\*"
      not_expect     : "\*"
      severity       : MEDIUM
    </custom_item>

    <custom_item>
      type           : REST_API
      description    : "Roles"
      request        : "listRoles"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .metadata.name as $roleName | .rules[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($roleName), API Groups: \(.apiGroups), Resources: \(.resources), Verbs: \(.verbs)\""
      regex          : "\*"
      not_expect     : "\*"
      severity       : MEDIUM
    </custom_item>
  </condition>

  <then>
    <report type:"PASSED">
      description : "4.1.3 Minimize wildcard use in Roles and ClusterRoles"
      info        : "Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard \"*\", which matches all items.

Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.

The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
      solution    : "Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions."
      reference   : "800-171|3.5.2,800-53|IA-5(1),800-53r5|IA-5(1),CSCv7|4.4,CSCv8|5.2,CSF|PR.AC-1,CSF2.0|PR.AA-01,CSF2.0|PR.AA-03,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(2)(i),HIPAA|164.312(d),ITSG-33|IA-5(1),LEVEL|1A,NESA|T5.2.3,QCSC-v1|5.2.2,QCSC-v1|13.2,SWIFT-CSCv1|4.1"
      see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
      show_output : YES
    </report>
  </then>
</if>

<report type:"WARNING">
  description : "4.1.4 Ensure that default service accounts are not actively used"
  info        : "The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.

Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod.

Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account.

The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.

Modify the configuration of each default service account to include this value

automountServiceAccountToken: false

Impact:

All workloads which require access to the Kubernetes API will require an explicit service account to be created."
  reference   : "800-171|3.1.1,800-53|AC-2(3),800-53r5|AC-2(3),CN-L3|7.1.3.2(e),CN-L3|8.1.4.2(c),CSCv7|4.3,CSCv7|5.2,CSCv7|16.9,CSCv8|5.3,CSF|PR.AC-1,CSF|PR.AC-4,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.1,ISO/IEC-27001|A.9.2.6,ITSG-33|AC-2(3),LEVEL|1A,NIAv2|AM26,QCSC-v1|5.2.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2,TBA-FIISB|36.2.2"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<report type:"WARNING">
  description : "4.1.5 Ensure that Service Account Tokens are only mounted where necessary"
  info        : "Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server

Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster.

Avoiding mounting these tokens removes this attack avenue.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.

Impact:

Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals."
  reference   : "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|14.7,CSCv8|4.8,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|DE.CM-09,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<custom_item>
  type           : REST_API
  description    : "4.1.6 Avoid use of system:masters group"
  info           : "The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)

The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed.

When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster.

GKE includes the CertificateSubjectRestriction admission controller which rejects requests for the system:masters group.

CertificateSubjectRestriction \"This admission controller observes creation of CertificateSigningRequest resources that have a spec.signerName of kubernetes.io/kube-apiserver-client. It rejects any request that specifies a 'group' (or 'organization attribute') of system:masters.\"

https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction

NOTE: Nessus has provided the target output to assist in reviewing the benchmark to ensure target compliance."
  solution       : "Remove the system:masters group from all users in the cluster.

Impact:

Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required."
  reference      : "800-171|3.1.5,800-171|3.1.6,800-53|AC-6(2),800-53|AC-6(5),800-53r5|AC-6(2),800-53r5|AC-6(5),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.10.6(a),CSCv8|5.4,CSF|PR.AC-4,CSF2.0|PR.AA-05,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.3,ITSG-33|AC-6(2),ITSG-33|AC-6(5),LEVEL|1A,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.6.1,NIAv2|AM1,NIAv2|AM23f,NIAv2|AM32,NIAv2|AM33,NIAv2|SS13c,NIAv2|SS15c,NIAv2|VL3a,PCI-DSSv3.2.1|7.1.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,QCSC-v1|5.2.2,QCSC-v1|6.2,SWIFT-CSCv1|1.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listClusterRolebindings"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
  regex          : "Subject: system:masters"
  not_expect     : "Subject: system:masters"
  severity       : MEDIUM
</custom_item>

<report type:"WARNING">
  description : "4.1.7 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster"
  info        : "Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators

The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level.

Each of these permissions has the potential to allow for privilege escalation to cluster-admin level.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Where possible, remove the impersonate, bind and escalate rights from subjects.

Impact:

There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts."
  reference   : "800-171|3.1.5,800-171|3.1.6,800-53|AC-6(2),800-53|AC-6(5),800-53r5|AC-6(2),800-53r5|AC-6(5),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.10.6(a),CSCv8|5.4,CSF|PR.AC-4,CSF2.0|PR.AA-05,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.3,ITSG-33|AC-6(2),ITSG-33|AC-6(5),LEVEL|1M,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.6.1,NIAv2|AM1,NIAv2|AM23f,NIAv2|AM32,NIAv2|AM33,NIAv2|SS13c,NIAv2|SS15c,NIAv2|VL3a,PCI-DSSv3.2.1|7.1.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,QCSC-v1|5.2.2,QCSC-v1|6.2,SWIFT-CSCv1|1.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<if>
  <condition auto:"FAILED" type:"AND">
    <custom_item>
      type           : REST_API
      description    : "Role Bindings"
      request        : "listRoleBindings"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
      regex          : "Subject: system:unauthenticated"
      not_expect     : "Subject: system:unauthenticated"
    </custom_item>

    <custom_item>
      type           : REST_API
      description    : "Cluster Role Bindings"
      request        : "listClusterRolebindings"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .value.items[] | .roleRef as $role | .subjects[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Role: \($role.name), Subject: \(.name)\""
      regex          : "Subject: system:unauthenticated"
      expect         : "Role: system:public-info-viewer"
      match_all      : YES
    </custom_item>
  </condition>

  <then>
    <report type:"PASSED">
      description : "4.1.9 Avoid non-default bindings to system:unauthenticated"
      info        : "Avoid non-default ClusterRoleBindings and RoleBindings with the group system:unauthenticated except the ClusterRoleBinding system:public-info-viewer

Kubernetes assigns the group system:unauthenticated to API server requests that have no authentication information provided. Binding a role to this group gives any unauthenticated user the permissions granted by that role and is strongly discouraged."
      solution    : "Identify all non-default clusterrolebindings and rolebindings to the group system:unauthenticated Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation.

Strongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.

If there are any non-default, unsafe bindings to the group system:unauthenticated proceed to delete them after consideration for cluster operations with only necessary, safer bindings.

kubectl delete clusterrolebinding
[CLUSTER_ROLE_BINDING_NAME] kubectl delete rolebinding
[ROLE_BINDING_NAME]
--
namespace
[ROLE_BINDING_NAMESPACE]

Impact:

Unauthenticated users will have privileges and permissions associated with roles associated with the configured bindings.

Care should be taken before removing any non-default clusterrolebindings or rolebindings from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and authenticated user for cluster operations."
      reference   : "800-171|3.1.1,800-53|AC-2,800-53r5|AC-2,CN-L3|7.1.3.2(d),CSCv7|16.8,CSCv8|5.5,CSF|DE.CM-1,CSF|DE.CM-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.9.2.1,ITSG-33|AC-2,LEVEL|1A,NIAv2|AM28,NIAv2|NS5j,NIAv2|SS14e,QCSC-v1|5.2.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2"
      see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
      show_output : YES
    </report>
  </then>
</if>

<report type:"WARNING">
  description : "4.2.1 Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces."
  info        : "The Pod Security Standard Baseline profile defines a baseline for container security. You can enforce this by using the built-in Pod Security Admission controller.

Without an active mechanism to enforce the Pod Security Standard Baseline profile, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Ensure that Pod Security Admission is in place for every namespace which contains user workloads.

Run the following command to enforce the Baseline profile in a namespace:

kubectl label namespace pod-security.kubernetes.io/enforce=baseline

Impact:

Enforcing a baseline profile will limit the use of containers."
  reference   : "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|5.1,CSCv7|5.2,CSCv8|16.7,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|DE.CM-09,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1M,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<report type:"WARNING">
  description : "4.3.1 Ensure that the CNI in use supports Network Policies"
  info        : "There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.

Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies.

See also recommendation 5.6.7.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "To use a CNI plugin with Network Policy, enable Network Policy in GKE, and the CNI plugin will be updated. See recommendation 5.6.7.

Impact:

None"
  reference   : "800-171|3.4.8,800-53|CM-7(5),800-53r5|SR-11,CSCv7|18.4,CSCv8|16.5,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.5.1,ISO/IEC-27001|A.12.6.2,ITSG-33|CM-7,LEVEL|1M,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,QCSC-v1|3.2,SWIFT-CSCv1|2.3,TBA-FIISB|44.2.2,TBA-FIISB|49.2.3"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<report type:"WARNING">
  description : "4.6.1 Create administrative boundaries between resources using namespaces"
  info        : "Use namespaces to isolate your Kubernetes objects.

Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Follow the documentation and create namespaces for objects in your deployment as you need them.

Impact:

You need to switch between namespaces for administration."
  reference   : "800-171|3.13.1,800-171|3.13.5,800-53|SC-7,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|12,CSCv8|13,CSF|DE.CM-1,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,CSF2.0|DE.CM-01,CSF2.0|PR.DS-01,CSF2.0|PR.DS-02,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|1M,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,TBA-FIISB|43.1"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<custom_item>
  type           : REST_API
  description    : "5.10.1 Ensure Kubernetes Web UI is Disabled"
  info           : "Note: The Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on.

The Kubernetes Web UI (Dashboard) has been a historical source of vulnerability and should only be deployed when necessary.

You should disable the Kubernetes Web UI (Dashboard) when running on Kubernetes Engine. The Kubernetes Web UI is backed by a highly privileged Kubernetes Service Account.

The Google Cloud Console provides all the required functionality of the Kubernetes Web UI and leverages Cloud IAM to restrict user access to sensitive cluster controls and settings."
  solution       : "Using Google Cloud Console:

Currently not possible, due to the add-on having been removed. Must use the command line.

Using Command Line:

To disable the Kubernetes Dashboard on an existing cluster, run the following command:

gcloud container clusters update <cluster_name> --zone <zone> --update-addons=KubernetesDashboard=DISABLED

Impact:

Users will be required to manage cluster resources using the Google Cloud Console or the command line. These require appropriate permissions. To use the command line, this requires the installation of the command line client, kubectl on the user's device (this is already included in Cloud Shell) and knowledge of command line operations."
  reference      : "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|2.2,CSCv7|18.4,CSCv8|4.8,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|DE.CM-09,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Kubernetes Dashboard - Disabled: \(.addonsConfig.kubernetesDashboard.disabled)\""
  regex          : "Kubernetes Dashboard - Disabled"
  expect         : "Kubernetes Dashboard - Disabled: true"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.10.2 Ensure that Alpha clusters are not used for production workloads"
  info           : "Alpha clusters are not covered by an SLA and are not production-ready.

Alpha clusters are designed for early adopters to experiment with workloads that take advantage of new features before those features are production-ready. They have all Kubernetes API features enabled, but are not covered by the GKE SLA, do not receive security updates, have node auto-upgrade and node auto-repair disabled, and cannot be upgraded. They are also automatically deleted after 30 days."
  solution       : "Alpha features cannot be disabled. To remediate, a new cluster must be created.

Using Google Cloud Console

- Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/
- Click CREATE CLUSTER, and choose \"SWITCH TO STANDARD CLUSTER\" in the upper right corner of the screen.
- Under Features in the the CLUSTER section, \"Enable Kubernetes alpha features in this cluster\" will not be available by default and to use Kubernetes alpha features in this cluster, first disable release channels.Note: It will only be available if the cluster is created with a Static version for the Control plane version, along with both Automatically upgrade nodes to the next available version and Enable auto-repair being checked under the Node pool details for each node.
- Configure the other settings as required and click CREATE.

Using Command Line:

Upon creating a new cluster

gcloud container clusters create [CLUSTER_NAME] \
--zone [COMPUTE_ZONE]

Do not use the --enable-kubernetes-alpha argument.

Impact:

Users and workloads will not be able to take advantage of features included within Alpha clusters."
  reference      : "800-171|3.13.1,800-171|3.13.5,800-53|SC-7,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|18.9,CSCv8|16.8,CSF|DE.CM-1,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,CSF2.0|DE.CM-01,CSF2.0|PR.DS-01,CSF2.0|PR.DS-02,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,TBA-FIISB|43.1"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Enable Kubernetes Alpha: \(.enableKubernetesAlpha)\""
  regex          : "Enable Kubernetes Alpha"
  expect         : "Enable Kubernetes Alpha: (null|false)"
  match_all      : YES
</custom_item>

<report type:"WARNING">
  description : "5.2.1 Ensure GKE clusters are not running using the Compute Engine default service account"
  info        : "Create and use minimally privileged Service accounts to run GKE cluster nodes instead of using the Compute Engine default Service account. Unnecessary permissions could be abused in the case of a node compromise.

A GCP service account (as distinct from a Kubernetes ServiceAccount) is an identity that an instance or an application can be used to run GCP API requests. This identity is used to identify virtual machine instances to other Google Cloud Platform services. By default, Kubernetes Engine nodes use the Compute Engine default service account. This account has broad access by default, as defined by access scopes, making it useful to a wide variety of applications on the VM, but it has more permissions than are required to run your Kubernetes Engine cluster.

A minimally privileged service account should be created and used to run the Kubernetes Engine cluster instead of using the Compute Engine default service account, and create separate service accounts for each Kubernetes Workload (See recommendation 5.2.2).

Kubernetes Engine requires, at a minimum, the node service account to have the monitoring.viewer monitoring.metricWriter and logging.logWriter roles. Additional roles may need to be added for the nodes to pull images from GCR.

NOTE: Nessus has not performed this check. Please review the benchmark to ensure target compliance."
  solution    : "Using Google Cloud Console:

To create a minimally privileged service account:

- Go to Service Accounts by visiting: https://console.cloud.google.com/iam-admin/serviceaccounts
- Click on CREATE SERVICE ACCOUNT
- Enter Service Account Details.
- Click CREATE AND CONTINUE
- Within Service Account permissions add the following roles:
- Logs Writer
- Monitoring Metric Writer
- `Monitoring Viewer.

- Click CONTINUE
- Grant users access to this service account and create keys as required.
- Click DONE

To create a Node pool to use the Service account:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Click on the cluster name within which the Node pool will be launched.
- Click on ADD NODE POOL
- Within the Node Pool details, select the Security subheading, and under `Identity defaults, select the minimally privileged service account from the Service Account drop-down.
- Click `CREATE to launch the Node pool.

Note: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.

Using Command Line:

To create a minimally privileged service account:

gcloud iam service-accounts create <node_sa_name> --display-name \"GKE Node Service Account\"
export NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service Account'

Grant the following roles to the service account:

export PROJECT_ID=gcloud config get-value project
gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.metricWriter
gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.viewer
gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/logging.logWriter

To create a new Node pool using the Service account, run the following command:

gcloud container node-pools create <node_pool> --service-account=<sa_name>@<project_id>.iam.gserviceaccount.com--cluster=<cluster_name> --zone <compute_zone>

Note: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.

Impact:

Instances are automatically granted the

https://www.googleapis.com/auth/cloud-platform

scope to allow full access to all Google Cloud APIs. This is so that the IAM permissions of the instance are completely determined by the IAM roles of the Service account. Thus if Kubernetes workloads were using cluster access scopes to perform actions using Google APIs, they may no longer be able to, if not permitted by the permissions of the Service account. To remediate, follow recommendation 5.2.2.

The Service account roles listed here are the minimum required to run the cluster. Additional roles may be required to pull from a private instance of Google Container Registry (GCR)."
  reference   : "800-171|3.5.2,800-53|IA-5,800-53r5|IA-5,CSCv7|4.3,CSCv8|4.7,CSF|PR.AC-1,CSF2.0|PR.AA-01,CSF2.0|PR.AA-03,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(2)(i),HIPAA|164.312(d),ITSG-33|IA-5,LEVEL|1A,NESA|T5.2.3,QCSC-v1|5.2.2,QCSC-v1|13.2"
  see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
</report>

<custom_item>
  type           : REST_API
  description    : "5.5.1 Ensure Container-Optimized OS (cos_containerd) is used for GKE node images"
  info           : "Use Container-Optimized OS (cos_containerd) as a managed, optimized and hardened base OS that limits the host's attack surface.

COS is an operating system image for Compute Engine VMs optimized for running containers. With COS, the containers can be brought up on Google Cloud Platform quickly, efficiently, and securely.

Using COS as the node image provides the following benefits:

- Run containers out of the box: COS instances come pre-installed with the container runtime and cloud-init With a COS instance, the container can be brought up at the same time as the VM is created, with no on-host setup required.
- Smaller attack surface: COS has a smaller footprint, reducing the instance's potential attack surface.
- Locked-down by default: COS instances include a locked-down firewall and other security settings by default."
  solution       : "Using Google Cloud Console:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Select the Kubernetes cluster which does not use COS.
- Under the Node pools heading, select the Node Pool that requires alteration.
- Click EDIT
- Under the Image Type heading click CHANGE
- From the pop-up menu select Container-optimised OS with containerd (cos_containerd) (default) and click CHANGE
- Repeat for all non-compliant Node pools.

Using Command Line:

To set the node image to cos for an existing cluster's Node pool:

gcloud container clusters upgrade <cluster_name> --image-type cos_containerd --zone <compute_zone> --node-pool <node_pool_name>

Impact:

If modifying an existing cluster's Node pool to run COS, the upgrade operation used is long-running and will block other operations on the cluster (including delete) until it has run to completion.

COS nodes also provide an option with containerd as the main container runtime directly integrated with Kubernetes instead of docker Thus, on these nodes, Docker cannot view or access containers or images managed by Kubernetes. Applications should not interact with Docker directly. For general troubleshooting or debugging, use crictl instead."
  reference      : "800-171|3.4.8,800-53|CM-7(5),800-53|CM-10,800-53r5|CM-7(5),800-53r5|CM-10,CSCv7|5.2,CSCv8|2.5,CSF|DE.CM-3,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|DE.CM-03,CSF2.0|DE.CM-09,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.5.1,ISO/IEC-27001|A.12.6.2,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,QCSC-v1|3.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.3,TBA-FIISB|44.2.2,TBA-FIISB|49.2.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Node Pool Name: \(.name), Image Type: \(.config.imageType)\""
  regex          : "Image Type"
  expect         : "Image Type: COS_CONTAINERD"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.5.4 When creating New Clusters - Automate GKE version management using Release Channels"
  info           : "Subscribe to the Regular or Stable Release Channel to automate version upgrades to the GKE cluster and to reduce version management complexity to the number of features and level of stability required.

Release Channels signal a graduating level of stability and production-readiness. These are based on observed performance of GKE clusters running that version and represent experience and confidence in the cluster version.

The Regular release channel upgrades every few weeks and is for production users who need features not yet offered in the Stable channel. These versions have passed internal validation, but don't have enough historical data to guarantee their stability. Known issues generally have known workarounds.

The Stable release channel upgrades every few months and is for production users who need stability above all else, and for whom frequent upgrades are too risky. These versions have passed internal validation and have been shown to be stable and reliable in production, based on the observed performance of those clusters.

Critical security patches are delivered to all release channels."
  solution       : "Currently, cluster Release Channels are only configurable at cluster provisioning time.

Using Google Cloud Console:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Click CREATE and choose CONFIGURE for the required cluster mode.
- Under the Control plane version heading, click the Release Channels button.
- Select the Regular or Stable channels from the Release Channel drop-down menu.
- Configure the rest of the cluster settings as required.
- Click CREATE

Using Command Line:Create a new cluster by running the following command:

gcloud container clusters create <cluster_name> --zone <cluster_zone> --release-channel <release_channel>

where <release_channel> is stable or regular according to requirements.

Impact:

Once release channels are enabled on a cluster, they cannot be disabled. To stop using release channels, the cluster must be recreated without the --release-channel flag.

Node auto-upgrade is enabled (and cannot be disabled), so the cluster is updated automatically from releases available in the chosen release channel."
  reference      : "800-171|3.11.2,800-171|3.11.3,800-171|3.14.1,800-53|RA-5,800-53|SI-2,800-53|SI-2(2),800-53r5|RA-5,800-53r5|RA-7,800-53r5|SI-2,800-53r5|SI-2(2),CN-L3|8.1.4.4(e),CN-L3|8.1.10.5(a),CN-L3|8.1.10.5(b),CN-L3|8.5.4.1(b),CN-L3|8.5.4.1(d),CN-L3|8.5.4.1(e),CSCv7|3.4,CSCv7|3.5,CSCv8|7.4,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,CSF2.0|GV.SC-10,CSF2.0|ID.IM-01,CSF2.0|ID.IM-02,CSF2.0|ID.IM-03,CSF2.0|ID.RA-01,CSF2.0|ID.RA-08,CSF2.0|PR.PS-02,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,ITSG-33|SI-2,ITSG-33|SI-2(2),LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.6.2,NESA|T7.7.1,NIAv2|PR9,PCI-DSSv3.2.1|6.1,PCI-DSSv3.2.1|6.2,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,PCI-DSSv4.0|6.3.3,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.2,SWIFT-CSCv1|2.7"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Release Channel: \(.releaseChannel.channel)\""
  regex          : "Release Channel:"
  expect         : "Release Channel: (REGULAR|STABLE)"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.5.5 Ensure Shielded GKE Nodes are Enabled"
  info           : "Shielded GKE Nodes provides verifiable integrity via secure boot, virtual trusted platform module (vTPM)-enabled measured boot, and integrity monitoring.

Shielded GKE nodes protects clusters against boot- or kernel-level malware or rootkits which persist beyond infected OS.

Shielded GKE nodes run firmware which is signed and verified using Google's Certificate Authority, ensuring that the nodes' firmware is unmodified and establishing the root of trust for Secure Boot. GKE node identity is strongly protected via virtual Trusted Platform Module (vTPM) and verified remotely by the master node before the node joins the cluster. Lastly, GKE node integrity (i.e., boot sequence and kernel) is measured and can be monitored and verified remotely."
  solution       : "Note: From version 1.18, clusters will have Shielded GKE nodes enabled by default.

Using Google Cloud Console:

To update an existing cluster to use Shielded GKE nodes:

- Navigate to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Select the cluster which for which Shielded GKE Nodes is to be enabled.
- With in the Details pane, under the Security heading, click on the pencil icon named Edit Shields GKE nodes
- Check the box named Enable Shield GKE nodes
- Click SAVE CHANGES

Using Command Line:

To migrate an existing cluster, the flag --enable-shielded-nodes needs to be specified in the cluster update command:

gcloud container clusters update <cluster_name> --zone <cluster_zone> --enable-shielded-nodes

Impact:

After Shielded GKE Nodes is enabled in a cluster, any nodes created in a Node pool without Shielded GKE Nodes enabled, or created outside of any Node pool, aren't able to join the cluster.

Shielded GKE Nodes can only be used with Container-Optimized OS (COS), COS with containerd, and Ubuntu node images."
  reference      : "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|5.3,CSCv7|18.11,CSCv8|16.7,CSF|PR.IP-1,CSF|PR.PT-3,CSF2.0|DE.CM-09,CSF2.0|PR.PS-01,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Shielded Nodes - Enabled: \(.shieldedNodes.enabled)\""
  regex          : "Shielded Nodes - Enabled"
  expect         : "Shielded Nodes - Enabled: true"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.5.6 Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled"
  info           : "Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.

Integrity Monitoring provides active alerting for Shielded GKE nodes which allows administrators to respond to integrity failures and prevent compromised nodes from being deployed into the cluster."
  solution       : "Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created within the cluster with Integrity Monitoring enabled.

Using Google Cloud Console

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- From the list of clusters, click on the cluster requiring the update and click ADD NODE POOL
- Ensure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading.
- Click SAVE

Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation

Using Command Line

To create a Node pool within the cluster with Integrity Monitoring enabled, run the following command:

gcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-integrity-monitoring

Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation

Impact:

None."
  reference      : "800-171|3.11.2,800-171|3.11.3,800-53|RA-5,800-53r5|RA-5,CSCv7|5.3,CSCv8|7.5,CSCv8|7.6,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,CSF2.0|GV.SC-10,CSF2.0|ID.IM-01,CSF2.0|ID.IM-02,CSF2.0|ID.IM-03,CSF2.0|ID.RA-01,CSF2.0|ID.RA-08,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.7.1,PCI-DSSv3.2.1|6.1,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.7"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \($clusterName), Node Pool Name: \(.name), Shielded Instance - Enable Integrity Monitoring: \(.config.shieldedInstanceConfig.enableIntegrityMonitoring)\""
  regex          : "Shielded Instance - Enable Integrity Monitoring"
  expect         : "Shielded Instance - Enable Integrity Monitoring: true"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.6.2 Ensure use of VPC-native clusters"
  info           : "Create Alias IPs for the node network CIDR range in order to subsequently configure IP-based policies and firewalling for pods. A cluster that uses Alias IPs is called a VPC-native cluster.

Using Alias IPs has several benefits:

- Pod IPs are reserved within the network ahead of time, which prevents conflict with other compute resources.
- The networking layer can perform anti-spoofing checks to ensure that egress traffic is not sent with arbitrary source IPs.
- Firewall controls for Pods can be applied separately from their nodes.
- Alias IPs allow Pods to directly access hosted services without using a NAT gateway."
  solution       : "Alias IPs cannot be enabled on an existing cluster. To create a new cluster using Alias IPs, follow the instructions below.

Using Google Cloud Console:

If using Standard configuration mode:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Click CREATE CLUSTER and select Standard configuration mode.
- Configure your cluster as desired , then, click Networking under CLUSTER in the navigation pane.
- In the 'VPC-native' section, leave 'Enable VPC-native (using alias IP)' selected
- Click CREATE.

If using Autopilot configuration mode:

Note that this is VPC-native only and cannot be disable:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Click CREATE CLUSTER, and select Autopilot configuration mode.
- Configure your cluster as required
- Click CREATE

Using Command Line

To enable Alias IP on a new cluster, run the following command:

gcloud container clusters create <cluster_name> --zone <compute_zone> --enable-ip-alias

If using Autopilot configuration mode:

gcloud container clusters create-auto <cluster_name> --zone <compute_zone>

Impact:

You cannot currently migrate an existing cluster that uses routes for Pod routing to a cluster that uses Alias IPs.

Cluster IPs for internal services remain only available from within the cluster. If you want to access a Kubernetes Service from within the VPC, but from outside of the cluster, use an internal load balancer."
  reference      : "800-171|3.13.1,800-171|3.13.5,800-53|CA-9,800-53|SC-7,800-53r5|CA-9,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|14.1,CSCv8|13.4,CSF|DE.CM-1,CSF|ID.AM-3,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,CSF2.0|DE.CM-01,CSF2.0|ID.AM-03,CSF2.0|PR.DS-01,CSF2.0|PR.DS-02,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,GDPR|32.1.d,GDPR|32.2,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.1,TBA-FIISB|43.1"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Use IP Aliases: \(.ipAllocationPolicy.useIpAliases)\""
  regex          : "Use IP Aliases"
  expect         : "Use IP Aliases: true"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.6.5 Ensure clusters are created with Private Nodes"
  info           : "Private Nodes are nodes with no public IP addresses. Disable public IP addresses for cluster nodes, so that they only have private IP addresses.

Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts."
  solution       : "Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated.

Using Google Cloud Console:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Click CREATE CLUSTER.
- Configure the cluster as required then click Networking under CLUSTER in the navigation pane.
- Under IPv4 network access, click the Private cluster radio button.
- Configure the other settings as required, and click CREATE.

Using Command Line:

To create a cluster with Private Nodes enabled, include the --enable-private-nodes flag within the cluster create command:

gcloud container clusters create <cluster_name> --enable-private-nodes

Setting this flag also requires the setting of --enable-ip-alias and --master-ipv4-cidr=<master_cidr_range>

Impact:

To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled.

Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.

To access Google Cloud APIs and services from private nodes, Private Google Access needs to be set on Kubernetes Engine Cluster Subnets."
  reference      : "800-171|3.13.1,800-171|3.13.5,800-171|3.13.6,800-53|CA-9,800-53|SC-7,800-53|SC-7(5),800-53r5|CA-9,800-53r5|SC-7,800-53r5|SC-7(5),CN-L3|7.1.2.2(c),CN-L3|8.1.10.6(j),CSCv8|4.4,CSF|DE.CM-1,CSF|ID.AM-3,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,CSF2.0|DE.CM-01,CSF2.0|ID.AM-03,CSF2.0|PR.DS-01,CSF2.0|PR.DS-02,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,GDPR|32.1.d,GDPR|32.2,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,ITSG-33|SC-7(5),LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,NIAv2|GS7b,NIAv2|NS25,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.1,TBA-FIISB|43.1"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Enable Private Nodes: \(.privateClusterConfig.enablePrivateNodes)\""
  regex          : "Enable Private Nodes"
  expect         : "Enable Private Nodes: true"
  match_all      : YES
</custom_item>

<if>
  <condition auto:"FAILED" type:"AND">
    <custom_item>
      type           : REST_API
      description    : "Logging Service"
      request        : "listContainerClusters"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Logging Service: \(.loggingService)\""
      regex          : "Logging Service"
      expect         : "Logging Service: logging.googleapis.com/kubernetes"
      match_all      : YES
    </custom_item>

    <custom_item>
      type           : REST_API
      description    : "Monitoring Service"
      request        : "listContainerClusters"
      json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Monitoring Service: \(.monitoringService)\""
      regex          : "Monitoring Service"
      expect         : "Monitoring Service: monitoring.googleapis.com/kubernetes"
      match_all      : YES
    </custom_item>
  </condition>

  <then>
    <report type:"PASSED">
      description : "5.7.1 Ensure Logging and Cloud Monitoring is Enabled"
      info        : "Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.

Exporting logs and metrics to a dedicated, persistent datastore such as Cloud Operations for GKE ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources."
      solution    : "Using Google Cloud Console:To enable Logging:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Select the cluster for which Logging is disabled.
- Under the details pane, within the Features section, click on the pencil icon named Edit logging
- Check the box next to Enable Logging
- In the drop-down Components box, select the components to be logged.
- Click SAVE CHANGES and wait for the cluster to update.

To enable Cloud Monitoring:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Select the cluster for which Logging is disabled.
- Under the details pane, within the Features section, click on the pencil icon named Edit Cloud Monitoring
- Check the box next to Enable Cloud Monitoring
- In the drop-down Components box, select the components to be logged.
- Click SAVE CHANGES and wait for the cluster to update.

Using Command Line:To enable Logging for an existing cluster, run the following command:

gcloud container clusters update <cluster_name> --zone <compute_zone> --logging=<components_to_be_logged>

See

https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging

for a list of available components for logging.

To enable Cloud Monitoring for an existing cluster, run the following command:

gcloud container clusters update <cluster_name> --zone <compute_zone> --monitoring=<components_to_be_logged>

See

https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--monitoring

for a list of available components for Cloud Monitoring."
      reference   : "800-171|3.3.1,800-171|3.3.2,800-171|3.3.6,800-53|AU-2,800-53|AU-7,800-53|AU-12,800-53r5|AU-2,800-53r5|AU-7,800-53r5|AU-12,CN-L3|7.1.2.3(c),CN-L3|8.1.4.3(a),CSCv7|6.2,CSCv8|8.2,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,CSF|RS.AN-3,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|DE.CM-09,CSF2.0|PR.PS-04,CSF2.0|RS.AN-03,CSF2.0|RS.AN-06,CSF2.0|RS.AN-07,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(b),ITSG-33|AU-2,ITSG-33|AU-7,ITSG-33|AU-12,LEVEL|1A,NESA|M1.2.2,NESA|M5.5.1,NIAv2|AM7,NIAv2|AM11a,NIAv2|AM11b,NIAv2|AM11c,NIAv2|AM11d,NIAv2|AM11e,NIAv2|SS30,NIAv2|VL8,PCI-DSSv3.2.1|10.1,QCSC-v1|3.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,QCSC-v1|13.2,SWIFT-CSCv1|6.4"
      see_also    : "https://workbench.cisecurity.org/benchmarks/19166"
      show_output : YES
    </report>
  </then>
</if>

<custom_item>
  type           : REST_API
  description    : "5.8.1 Ensure authentication using Client Certificates is Disabled"
  info           : "Disable Client Certificates, which require certificate rotation, for authentication. Instead, use another authentication method like OpenID Connect.

With Client Certificate authentication, a client presents a certificate that the API server verifies with the specified Certificate Authority. In GKE, Client Certificates are signed by the cluster root Certificate Authority. When retrieved, the Client Certificate is only base64 encoded and not encrypted.

GKE manages authentication via gcloud for you using the OpenID Connect token method, setting up the Kubernetes configuration, getting an access token, and keeping it up to date. This means Basic Authentication using static passwords and Client Certificate authentication, which both require additional management overhead of key management and rotation, are not necessary and should be disabled.

When Client Certificate authentication is disabled, you will still be able to authenticate to the cluster with other authentication methods, such as OpenID Connect tokens. See also Recommendation 6.8.1 to disable authentication using static passwords, known as Basic Authentication."
  solution       : "Currently, there is no way to remove a client certificate from an existing cluster. Thus a new cluster must be created.

Using Google Cloud Console

- Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list
- Click CREATE CLUSTER
- Configure as required and the click on 'Availability, networking, security, and additional features' section
- Ensure that the 'Issue a client certificate' checkbox is not ticked
- Click CREATE.

Using Command Line

Create a new cluster without a Client Certificate:

gcloud container clusters create [CLUSTER_NAME] \
--no-issue-client-certificate

Impact:

Users will no longer be able to authenticate with the pre-provisioned x509 certificate. You will have to configure and use alternate authentication mechanisms, such as OpenID Connect tokens."
  reference      : "800-171|3.1.1,800-171|3.1.5,800-171|3.3.8,800-171|3.3.9,800-53|AC-2,800-53|AC-3,800-53|AC-6,800-53|AC-6(1),800-53|AC-6(7),800-53|AU-9(4),800-53r5|AC-2,800-53r5|AC-5,800-53r5|AC-6,800-53r5|AC-6(1),800-53r5|AC-6(7),800-53r5|AU-9(4),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(d),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.4.2(f),CN-L3|8.1.4.3(d),CN-L3|8.1.4.11(b),CN-L3|8.1.10.2(c),CN-L3|8.1.10.6(a),CN-L3|8.5.3.1,CN-L3|8.5.4.1(a),CSCv8|6.8,CSF|DE.CM-1,CSF|DE.CM-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF|PR.DS-5,CSF|PR.PT-1,CSF|PR.PT-3,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),HIPAA|164.312(b),ISO/IEC-27001|A.9.2.1,ISO/IEC-27001|A.9.2.5,ISO/IEC-27001|A.9.4.1,ISO/IEC-27001|A.9.4.4,ISO/IEC-27001|A.9.4.5,ISO/IEC-27001|A.12.4.2,ITSG-33|AC-2,ITSG-33|AC-3,ITSG-33|AC-6,ITSG-33|AC-6(1),ITSG-33|AU-9(4),ITSG-33|AU-9(4)(a),ITSG-33|AU-9(4)(b),LEVEL|1A,NESA|M1.1.3,NESA|M1.2.2,NESA|M5.2.3,NESA|M5.5.2,NESA|T4.2.1,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.2,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM3,NIAv2|AM23f,NIAv2|AM28,NIAv2|AM31,NIAv2|GS3,NIAv2|GS4,NIAv2|GS8c,NIAv2|NS5j,NIAv2|SM5,NIAv2|SM6,NIAv2|SS13c,NIAv2|SS14e,NIAv2|SS15c,NIAv2|SS29,NIAv2|VL3b,PCI-DSSv3.2.1|7.1.2,PCI-DSSv3.2.1|10.5,PCI-DSSv3.2.1|10.5.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,PCI-DSSv4.0|10.3.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Master Auth - Client Key Length: \(.masterAuth.clientKey | length)\""
  regex          : "Master Auth - Client Key Length:"
  expect         : "Master Auth - Client Key Length: 0"
  match_all      : YES
</custom_item>

<custom_item>
  type           : REST_API
  description    : "5.8.3 Ensure Legacy Authorization (ABAC) is Disabled"
  info           : "Legacy Authorization, also known as Attribute-Based Access Control (ABAC) has been superseded by Role-Based Access Control (RBAC) and is not under active development.RBAC is the recommended way to manage permissions in Kubernetes.

In Kubernetes, RBAC is used to grant permissions to resources at the cluster and namespace level. RBAC allows the definition of roles with rules containing a set of permissions, whilst the legacy authorizer (ABAC) in Kubernetes Engine grants broad, statically defined permissions. As RBAC provides significant security advantages over ABAC, it is recommended option for access control. Where possible, legacy authorization must be disabled for GKE clusters."
  solution       : "Using Google Cloud Console:

- Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list
- Select Kubernetes clusters for which Legacy Authorization is enabled.
- Click EDIT.
- Set 'Legacy Authorization' to 'Disabled'.
- Click SAVE.

Using Command Line:

To disable Legacy Authorization for an existing cluster, run the following command:

gcloud container clusters update <cluster_name> --zone <compute_zone> --no-enable-legacy-authorization

Impact:

Once the cluster has the legacy authorizer disabled, the user must be granted the ability to create authorization roles using RBAC to ensure that the role-based access control permissions take effect."
  reference      : "800-171|3.1.1,800-171|3.1.5,800-171|3.3.8,800-171|3.3.9,800-53|AC-2,800-53|AC-3,800-53|AC-6,800-53|AC-6(1),800-53|AC-6(7),800-53|AU-9(4),800-53r5|AC-2,800-53r5|AC-5,800-53r5|AC-6,800-53r5|AC-6(1),800-53r5|AC-6(7),800-53r5|AU-9(4),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(d),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.4.2(f),CN-L3|8.1.4.3(d),CN-L3|8.1.4.11(b),CN-L3|8.1.10.2(c),CN-L3|8.1.10.6(a),CN-L3|8.5.3.1,CN-L3|8.5.4.1(a),CSCv8|6.8,CSF|DE.CM-1,CSF|DE.CM-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF|PR.DS-5,CSF|PR.PT-1,CSF|PR.PT-3,CSF2.0|DE.CM-01,CSF2.0|DE.CM-03,CSF2.0|PR.AA-01,CSF2.0|PR.AA-05,CSF2.0|PR.DS-10,CSF2.0|PR.IR-01,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),HIPAA|164.312(b),ISO/IEC-27001|A.9.2.1,ISO/IEC-27001|A.9.2.5,ISO/IEC-27001|A.9.4.1,ISO/IEC-27001|A.9.4.4,ISO/IEC-27001|A.9.4.5,ISO/IEC-27001|A.12.4.2,ITSG-33|AC-2,ITSG-33|AC-3,ITSG-33|AC-6,ITSG-33|AC-6(1),ITSG-33|AU-9(4),ITSG-33|AU-9(4)(a),ITSG-33|AU-9(4)(b),LEVEL|1A,NESA|M1.1.3,NESA|M1.2.2,NESA|M5.2.3,NESA|M5.5.2,NESA|T4.2.1,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.2,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM3,NIAv2|AM23f,NIAv2|AM28,NIAv2|AM31,NIAv2|GS3,NIAv2|GS4,NIAv2|GS8c,NIAv2|NS5j,NIAv2|SM5,NIAv2|SM6,NIAv2|SS13c,NIAv2|SS14e,NIAv2|SS15c,NIAv2|SS29,NIAv2|VL3b,PCI-DSSv3.2.1|7.1.2,PCI-DSSv3.2.1|10.5,PCI-DSSv3.2.1|10.5.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,PCI-DSSv4.0|10.3.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3"
  see_also       : "https://workbench.cisecurity.org/benchmarks/19166"
  request        : "listContainerClusters"
  json_transform : ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \($projectNumber), Project ID: \($projectId), Cluster Name: \(.name), Legacy ABAC Enabled: \(.legacyAbac.enabled)\""
  regex          : "Legacy ABAC Enabled:"
  not_expect     : "Legacy ABAC Enabled: true"
</custom_item>

</check_type>
