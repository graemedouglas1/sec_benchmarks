---

inspec_rules:
  "2.1.1":
    title: '2.1.1 | Client certificate authentication should not be used for users'
    section: 'Authentication and Authorization'
    description: |
        Kubernetes provides the option to use client certificates for user authentication.  However as there is no way to revoke
        these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose.
        It is not possible to fully disable client certificate use within a cluster as it is used for component to component
        authentication.
    remediation: |
        Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client
        certificates. You can remediate the availability of client certificates in your OKE cluster. Impact: External mechanisms
        for authentication generally require additional software to be deployed.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_2.1.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "2.2.1":
    title: '2.2.1 | Ensure access to OCI Audit service Log for OKE - manual'
    section: 'Logging'
    description: |
        The audit logs are part of the  OKE managed Kubernetes control plane logs managed by OKE.  OKE integrates with Oracle
        Cloud Infrastructure Audit Service. All operations performed by the Kubernetes API server are visible as log events on
        the Oracle Cloud Infrastructure Audit service.
    remediation: |
        No remediation is necessary for this control. Impact: The Control plane audit logs are managed by OKE.  OKE Control
        plane logs are written to the Oracle Cloud Infrastructure Audit Service.  The Oracle Cloud Infrastructure Audit service
        automatically records calls to all supported Oracle Cloud Infrastructure public application programming interface (API)
        endpoints as log events.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_2.2.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "3.1.1":
    title: '3.1.1 | Ensure that the oke kubelet conf.json file permissions are set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubelet
         is running, and if it is using a filebased okekubeletconf.json file, ensure that the proxy okekubeletconf.json file has
        permissions of              644
         or more restrictive.
    remediation: |
        Run the below command (based on the file location on your system) on the each worker
        node. For example, chmod 644 <oke_kubelet_conf.json file>
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.2":
    title: '3.1.2 | Ensure that the proxy oke kubelet conf.json file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubelet
         is running, ensure that the file ownership of its okekubeletconf.json file is set to              rootroot
        .
    remediation: |
        Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root
        <oke_kubelet_conf.json file>
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.1.3":
    title: '3.1.3 | Ensure that the kubelet configuration file has permissions set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet refers to a configuration file with the              config
         argument, that file has permissions of 644 or more restrictive.
    remediation: |
        Run the following command (using the config file location identied in the Audit step) chmod 644
        etc/kubernetes/kubelet.conf
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.4":
    title: '3.1.4 | Ensure that the kubelet configuration file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet refers to a configuration file with the              config
         argument, that file is owned by rootroot.
    remediation: |
        Run the following command (using the config file location identied in the Audit step) chown root:root
        /etc/kubernetes/kubelet.conf
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.2.1":
    title: '3.2.1 | Ensure that the --anonymous-auth argument is set to false - manual'
    section: 'Kubelet'
    description: |
        Disable anonymous requests to the Kubelet server.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --anonymous-auth=false
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of
        "authentication.*anonymous":{"enabled":false}"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Anonymous requests will be rejected.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*anonymous":{"enabled":false}'

  "3.2.2":
    title: '3.2.2 | Ensure that the --authorization-mode argument is not set to AlwaysAllow - manual'
    section: 'Kubelet'
    description: |
        Do not allow all requests. Enable explicit authorization.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --authorization-mode=Webhook
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of
        "authentication.*webhook":{"enabled":true}"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Unauthorized requests will be denied.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*webhook":{"enabled":true'

  "3.2.3":
    title: '3.2.3 | Ensure that the --client-ca-file argument is set as appropriate - manual'
    section: 'Kubelet'
    description: |
        Enable Kubelet authentication using certificates.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --client-ca-file=/etc/kubernetes/ca.crt \
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of
        "authentication.*x509":("clientCAFile":"/etc/kubernetes/pki/ca.crt"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: You require TLS to be configured on apiserver as well as kubelets.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*clientCAFile":".[a-zA-Z]'

  "3.2.4":
    title: '3.2.4 | Ensure that the --read-only-port argument is set to 0'
    section: 'Kubelet'
    description: |
        Disable the readonly port.
    remediation: |
        If modifying the Kubelet config file, edit the kubelet.service file                  /etc/sytemd/system/kubelet.service
         and set the below parameter               --read-only-port=0

        For all remediations:
        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Removal of the read-only port will require that any service which made use of it will need to be
        re-configured to use the main Kubelet API.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "3.2.5":
    title: '3.2.5 | Ensure that the --streaming-connection-idle-timeout argument is not set to 0 - manual'
    section: 'Kubelet'
    description: |
        Do not disable timeouts on streaming connections.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --streaming-connection-idle-timeout
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of
        "streamingConnectionIdleTimeout":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Long-lived connections could be interrupted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.5', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == streamingConnectionIdleTimeout":"4h0m0s'

  "3.2.6":
    title: '3.2.6 | Ensure that the --make-iptables-util-chains argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Allow Kubelet to manage iptables.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --make-iptables-util-chains:true
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of                  "makeIPTablesUtilChains": true
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other
        iptables management solution, then there might be some conflicts.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.6', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == makeIPTablesUtilChains":true'

  "3.2.7":
    title: '3.2.7 | Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture - manual'
    section: 'Kubelet'
    description: |
        Security relevant information should be captured.  The              eventqps
         flag on the Kubelet can be used to limit the rate at which events are gathered.  Setting this too low could result in
        relevant events not being logged, however the unlimited setting of              0
         could result in a denial of service on the kubelet.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --event-qps=0
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of                  "eventRecordQPS"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact:
        Setting this parameter to                    0
         could result in a denial of service condition due to excessive events being created.  The cluster's event processing
        and storage systems should be scaled to handle expected event loads.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.7', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "eventRecordQPS":[0-9]\d*'

  "3.2.8":
    title: '3.2.8 | Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate - manual'
    section: 'Kubelet'
    description: |
        Setup TLS connection on the Kubelets.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter
        Verify that the `tls-cert-file=/var/lib/kubelet/pki/tls.pem`.
        Verify that the `tls-private-key-file=/var/lib/kubelet/pki/tls.key`.
                       Remediation Method 2:
        If using the api configz endpoint consider searching for the status of                  tlsCertFile
         and                  tlsPrivateKeyFile
         are set by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster
        deployment.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.8', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == tlsCertFile":\S+.pem'

  "3.2.9":
    title: '3.2.9 | Ensure that the --rotate-certificates argument is not set to false - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet client certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               Verify that the `--rotate-certificates` is present.
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of                  rotateCertificates
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.9', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == rotateCertificates":true'

  "3.2.10":
    title: '3.2.10 | Ensure that the --rotate-server-certificates argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet server certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet service config file, edit the kubelet.service file
        /etc/systemd/system/kubelet.service
         and set the below parameter               --rotate-server-certificates=true
         Remediation Method 2:
        If using the api configz endpoint consider searching for the status of                  --rotate-server-certificates
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=10.0.10.4 (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.10', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "rotateServerCertificates":true'

  "4.1.1":
    title: '4.1.1 | Ensure that the cluster-admin role is only used where required'
    section: 'RBAC and Service Accounts'
    description: |
        The RBAC role              clusteradmin
         provides wideranging powers over the environment and should be used only where and when needed.
    remediation: |
        Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they
        could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the
        clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
         Impact:
        Care should be taken before removing any                    clusterrolebindings
         from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should
        not be made to                    clusterrolebindings
         with the                    system:
         prefix as they are required for the operation of system components.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.2":
    title: '4.1.2 | Minimize access to secrets'
    section: 'RBAC and Service Accounts'
    description: |
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by
        workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to
        reduce the risk of privilege escalation.
    remediation: |
        Where possible, remove                  get
        ,                  list
         and                  watch
         access to                  secret
         objects in the cluster.               Impact: Care should be taken not to remove access to secrets to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.3":
    title: '4.1.3 | Minimize wildcard use in Roles and ClusterRoles'
    section: 'RBAC and Service Accounts'
    description: |
        Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on
        those objects.  It is possible to set either of these to be the wildcard  which matches all items. Use of wildcards is
        not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are
        added to the Kubernetes API either as CRDs or in later versions of the product.
    remediation: |
        Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.4":
    title: '4.1.4 | Minimize access to create pods'
    section: 'RBAC and Service Accounts'
    description: |
        The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as
        assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data unless Pod
        Security Policies are implemented to restrict this access As such, access to create new pods should be restricted to the
        smallest possible group of users.
    remediation: |
        Where possible, remove                  create
         access to                  pod
         objects in the cluster.               Impact: Care should be taken not to remove access to pods to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.5":
    title: '4.1.5 | Ensure that default service accounts are not actively used.'
    section: 'RBAC and Service Accounts'
    description: |
        The              default
         service account should not be used to ensure that rights granted to applications can be more easily audited and
        reviewed.
    remediation: |
        Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.
        Modify the configuration of each default service account to include this value automountServiceAccountToken: false
         Automatic remediation for the default account: kubectl patch serviceaccount default -p $'automountServiceAccountToken:
        false' Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be
        created.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.5', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.6":
    title: '4.1.6 | Ensure that Service Account Tokens are only mounted where necessary'
    section: 'RBAC and Service Accounts'
    description: |
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to
        communicate with the API server
    remediation: |
        Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
        Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where
        the resource is available to unauthenticated principals.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.6', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.2.1":
    title: '4.2.1 | Minimize the admission of privileged containers'
    section: 'Pod Security Policies'
    description: |
        Do not generally permit containers to be run with the              securityContext.privileged
         flag set to              true
        .
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged
        containers. To enable PSA for a namespace in your cluster, set the pod-security.kubernetes.io/enforce label with the
        policy value you want to enforce. kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
        The above command enforces the restricted policy for the NAMESPACE namespace. You can also enable Pod Security Admission
        for all your namespaces. For example: kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
        Impact:
        Pods defined with                    spec.containers[].securityContext.privileged: true
        ,                    spec.initContainers[].securityContext.privileged: true
         and                    spec.ephemeralContainers[].securityContext.privileged: true
         will not be permitted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.2":
    title: '4.2.2 | Minimize the admission of containers wishing to share the host process ID namespace'
    section: 'Pod Security Policies'
    description: |
        Do not generally permit containers to be run with the              hostPID
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostPID
         containers.               Impact:
        Pods defined with                    spec.hostPID: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.3":
    title: '4.2.3 | Minimize the admission of containers wishing to share the host IPC namespace'
    section: 'Pod Security Policies'
    description: |
        Do not generally permit containers to be run with the              hostIPC
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostIPC
         containers.               Impact:
        Pods defined with                    spec.hostIPC: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.4":
    title: '4.2.4 | Minimize the admission of containers wishing to share the host network namespace'
    section: 'Pod Security Policies'
    description: |
        Do not generally permit containers to be run with the              hostNetwork
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostNetwork
         containers.               Impact:
        Pods defined with                    spec.hostNetwork: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.5":
    title: '4.2.5 | Minimize the admission of containers with allowPrivilegeEscalation'
    section: 'Pod Security Policies'
    description: |
        Do not generally permit containers to be run with the              allowPrivilegeEscalation
         flag set to              true
        . Allowing this right can lead to a process running a container getting more rights than it started with.           Its
        important to note that these rights are still constrained by the overall container sandbox, and this setting does not
        relate to the use of privileged containers.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with
        .spec.allowPrivilegeEscalation
         set to                  true
        .               Impact:
        Pods defined with                    spec.allowPrivilegeEscalation: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.5', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.3.1":
    title: '4.3.1 | Ensure latest CNI version is used'
    section: 'CNI Plugin'
    description: |
        There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may
        not be possible to effectively restrict traffic in the cluster.
    remediation: |
        As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny
        all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.
        Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.3.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.3.2":
    title: '4.3.2 | Ensure that all Namespaces have Network Policies defined'
    section: 'CNI Plugin'
    description: |
        Use network policies to isolate traffic in your cluster network.
    remediation: |
        Follow the documentation and create                  NetworkPolicy
         objects as you need them.               Clusters you create with Container Engine for Kubernetes have flannel installed
        as the default CNI network provider. flannel is a simple overlay virtual network that satisfies the requirements of the
        Kubernetes networking model by attaching IP addresses to containers. Although flannel satisfies the requirements of the
        Kubernetes networking model, it does not support NetworkPolicy resources. If you want to enhance the security of
        clusters you create with Container Engine for Kubernetes by implementing network policies, you have to install and
        configure a network provider that does support NetworkPolicy resources. One such provider is Calico (refer to the
        Kubernetes documentation for a list of other network providers). Calico is an open source networking and network
        security solution for containers, virtual machines, and native host-based workloads. Use the Calico open-source software
        in conjunction with flannel. The Calico Enterprise does not support flannel. Impact: Once network policies are in use
        within a given namespace, traffic not explicitly allowed by a network policy will be denied.  As such it is important to
        ensure that, when introducing network policies, legitimate traffic is not blocked.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.3.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.1":
    title: '4.4.1 | Prefer using secrets as files over secrets as environment variables'
    section: 'Secrets Management'
    description: |
        Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment
        variable secrets.
    remediation: |
        If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.
        Impact: Application code which expects to read secrets in the form of environment variables would need modification
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.4.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "4.4.2":
    title: '4.4.2 | Consider external secret storage - manual'
    section: 'Secrets Management'
    description: |
        Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if
        you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has
        auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.
    remediation: |
        Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. The
        master nodes in a Kubernetes cluster store sensitive configuration data (such as authentication tokens, passwords, and
        SSH keys) as Kubernetes secret objects in etcd. Etcd is an open source distributed key-value store that Kubernetes uses
        for cluster coordination and state management. In the Kubernetes clusters created by Container Engine for Kubernetes,
        etcd writes and reads data to and from block storage volumes in the Oracle Cloud Infrastructure Block Volume service.
        Although the data in block storage volumes is encrypted, Kubernetes secrets at rest in etcd itself are not encrypted by
        default. For additional security, when you create a new cluster you can specify that Kubernetes secrets at rest in etcd
        are to be encrypted using the Oracle Cloud Infrastructure Vault service. Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.4.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.1":
    title: '4.5.1 | Create administrative boundaries between resources using namespaces - manual'
    section: 'General Policies'
    description: |
        Use namespaces to isolate your Kubernetes objects.
    remediation: |
        Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to
        switch between namespaces for administration.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.5.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.2":
    title: '4.5.2 | Apply Security Context to Your Pods and Containers - manual'
    section: 'General Policies'
    description: |
        Apply Security Context to Your Pods and Containers
    remediation: |
        As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular
        namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we
        recommend implementing a more restrictive policy such as this:
        apiVersion: policy/v1beta1
        kind: PodSecurityPolicy
        metadata:
            name: restricted
            annotations:
            seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
            apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
            seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
            apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
        spec:
            privileged: false
            # Required to prevent escalations to root.
            allowPrivilegeEscalation: false
            # This is redundant with non-root + disallow privilege escalation,
            # but we can provide it for defense in depth.
            requiredDropCapabilities:
            - ALL
            # Allow core volume types.
            volumes:
            - 'configMap'
            - 'emptyDir'
            - 'projected'
            - 'secret'
            - 'downwardAPI'
            # Assume that persistentVolumes set up by the cluster admin are safe to use.
            - 'persistentVolumeClaim'
            hostNetwork: false
            hostIPC: false
            hostPID: false
            runAsUser:
            # Require the container to run without root privileges.
            rule: 'MustRunAsNonRoot'
            seLinux:
            # This policy assumes the nodes are using AppArmor rather than SELinux.
            rule: 'RunAsAny'
            supplementalGroups:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            fsGroup:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            readOnlyRootFilesystem: false
                       This policy prevents pods from running as privileged or escalating privileges. It also restricts the
        types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar,
        approach is to start with policy that locks everything down and incrementally add exceptions for applications that need
        looser restrictions such as logging agents which need the ability to mount a host path. Impact: If you incorrectly apply
        security contexts, you may have trouble running the pods.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.5.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.3":
    title: '4.5.3 | The default namespace should not be used'
    section: 'General Policies'
    description: |
        Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing
        objects in this namespace makes application of RBAC and other controls more difficult.
    remediation: |
        Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new
        resources are created in a specific namespace. Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.5.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.1.1":
    title: '5.1.1 | Oracle Cloud Security Penetration and Vulnerability Testing - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Oracle regularly performs penetration and vulnerability testing and security assessments against the Oracle Cloud
        infrastructure, platforms, and applications. These tests are intended to validate and improve the overall security of
        Oracle Cloud services.
    remediation: |
        As a service administrator, you can run tests for some Oracle Cloud services. Before running the tests, you must first
        review the Oracle Cloud Testing Policies section. Note: You must have an Oracle Account with the necessary privileges to
        file service maintenance requests, and you must be signed in to the environment that will be the subject of the
        penetration and vulnerability testing. Submitting a Cloud Security Testing Notification Impact: None.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.2":
    title: '5.1.2 | Minimize user access control to Container Engine for Kubernetes - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Restrict user access to OKE, limiting interaction with build images to only authorized personnel and service accounts.
    remediation: |
        By default, users are not assigned any Kubernetes RBAC roles (or clusterroles) by default. So before attempting to
        create a new role (or clusterrole), you must be assigned an appropriately privileged role (or clusterrole). A number of
        such roles and clusterroles are always created by default, including the cluster-admin clusterrole (for a full list, see
        Default Roles and Role Bindings in the Kubernetes documentation). The cluster-admin clusterrole essentially confers
        super-user privileges. A user granted the cluster-admin clusterrole can perform any operation across all namespaces in a
        given cluster. Note that Oracle Cloud Infrastructure tenancy administrators already have sufficient privileges, and do
        not require the cluster-admin clusterrole.
        See:                  Granting the Kubernetes RBAC cluster-admin clusterrole
                              Impact: Care should be taken not to remove access to Oracle Cloud Infrastructure Registry (OCR)
        for accounts that require this for their operation.
        Any account granted the Storage Object Viewer role at the project level can view all objects stored in OCS for the
        project.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.3":
    title: '5.1.3 | Minimize cluster access to read-only - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Configure the Cluster Service Account to only allow readonly access to OKE.
    remediation: |
        To access a cluster using kubectl, you have to set up a Kubernetes configuration file (commonly known as a 'kubeconfig'
        file) for the cluster. The kubeconfig file (by default named config and stored in the $HOME/.kube directory) provides
        the necessary details to access the cluster. Having set up the kubeconfig file, you can start using kubectl to manage
        the cluster. The steps to follow when setting up the kubeconfig file depend on how you want to access the cluster: To
        access the cluster using kubectl in Cloud Shell, run an Oracle Cloud Infrastructure CLI command in the Cloud Shell
        window to set up the kubeconfig file. To access the cluster using a local installation of kubectl: Generate an API
        signing key pair (if you don't already have one). Upload the public key of the API signing key pair. Install and
        configure the Oracle Cloud Infrastructure CLI. Set up the kubeconfig file.
        See                  Setting Up Local Access to Clusters
                              Impact: A separate dedicated service account may be required for use by build servers and other
        robot users pushing or managing container images.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.4":
    title: '5.1.4 | Minimize Container Registries to only those approved - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Use approved container registries.
    remediation: |
        Impact: All container images to be deployed to the cluster must be hosted within an approved container image registry.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.2.1":
    title: '5.2.1 | Prefer using dedicated Service Accounts'
    section: 'Identity and Access Management (IAM)'
    description: |
        Kubernetes workloads should not use cluster node service accounts to authenticate to Oracle Cloud APIs. Each Kubernetes
        Workload that needs to authenticate to other Oracle services using Cloud IAM should be provisioned a dedicated Service
        account.
    remediation: |
        When you create a pod, if you do not specify a service account, it is automatically assigned the default service account
        in the same namespace. If you get the raw json or yaml for a pod you have created                  (for example, kubectl
        get pods/<podname> -o yaml)
        , you can see the spec.serviceAccountName field has been automatically set.
        See                  Configure Service Accounts for Pods
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.2.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.3.1":
    title: '5.3.1 | Encrypting Kubernetes Secrets at Rest in Etcd - manual'
    section: 'Cloud Key Management Service (Cloud KMS)'
    description: |
        Encrypt Kubernetes secrets, stored in etcd, at the applicationlayer using a customermanaged key.
    remediation: |
        You can create a cluster in one tenancy that uses a master encryption key in a different tenancy. In this case, you have
        to write cross-tenancy policies to enable the cluster in its tenancy to access the master encryption key in the Vault
        service's tenancy. Note that if you want to create a cluster and specify a master encryption key that's in a different
        tenancy, you cannot use the Console to create the cluster.
        For example, assume the cluster is in the ClusterTenancy, and the master encryption key is in the KeyTenancy. Users
        belonging to a group (OKEAdminGroup) in the ClusterTenancy have permissions to create clusters. A dynamic group
        (OKEAdminDynGroup) has been created in the cluster, with the rule ALL                  {resource.type = 'cluster',
        resource.compartment.id = 'ocid1.compartment.oc1..<unique_ID>'}
        , so all clusters created in the ClusterTenancy belong to the dynamic group.               In the root compartment of
        the KeyTenancy, the following policies: use the ClusterTenancy's OCID to map ClusterTenancy to the alias OKE_Tenancy use
        the OCIDs of OKEAdminGroup and OKEAdminDynGroup to map them to the aliases RemoteOKEAdminGroup and
        RemoteOKEClusterDynGroup respectively give RemoteOKEAdminGroup and RemoteOKEClusterDynGroup the ability to list, view,
        and perform cryptographic operations with a particular master key in the KeyTenancy
        Define tenancy OKE_Tenancy as ocid1.tenancy.oc1..<unique_ID>
        Define dynamic-group RemoteOKEClusterDynGroup as ocid1.dynamicgroup.oc1..<unique_ID>
        Define group RemoteOKEAdminGroup as ocid1.group.oc1..<unique_ID>
        Admit dynamic-group RemoteOKEClusterDynGroup of tenancy ClusterTenancy to use keys in tenancy where target.key.id =
        'ocid1.key.oc1..<unique_ID>'
        Admit group RemoteOKEAdminGroup of tenancy ClusterTenancy to use keys in tenancy where target.key.id =
        'ocid1.key.oc1..<unique_ID>'
                       In the root compartment of the ClusterTenancy, the following policies: use the KeyTenancy's OCID to map
        KeyTenancy to the alias KMS_Tenancy give OKEAdminGroup and OKEAdminDynGroup the ability to use master keys in the
        KeyTenancy allow OKEAdminDynGroup to use a specific master key obtained from the KeyTenancy in the ClusterTenancy
        Define tenancy KMS_Tenancy as ocid1.tenancy.oc1..<unique_ID>
        Endorse group OKEAdminGroup to use keys in tenancy KMS_Tenancy
        Endorse dynamic-group OKEAdminDynGroup to use keys in tenancy KMS_Tenancy
        Allow dynamic-group OKEAdminDynGroup to use keys in tenancy where target.key.id = 'ocid1.key.oc1..<unique_ID>'
                       See Accessing Object Storage Resources Across Tenancies for more examples of writing cross-tenancy
        policies. Having entered the policies, you can now run a command similar to the following to create a cluster in the
        ClusterTenancy that uses the master key obtained from the KeyTenancy: oci ce cluster create --name oke-with-cross-kms
        --kubernetes-version v1.16.8 --vcn-id ocid1.vcn.oc1.iad.<unique_ID> --service-lb-subnet-ids
        '["ocid1.subnet.oc1.iad.<unique_ID>"]' --compartment-id ocid1.compartment.oc1..<unique_ID> --kms-key-id
        ocid1.key.oc1.iad.<unique_ID>
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.3.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.4.1":
    title: '5.4.1 | Restrict Access to the Control Plane Endpoint'
    section: 'Cluster Networking'
    description: |
        Enable Master Authorized Networks to restrict access to the clusters control plane master endpoint to only an allowlist
        whitelist of authorized IPs.
    remediation: |
        Impact: When implementing Master Authorized Networks, be careful to ensure all desired networks are on the allowlist
        (whitelist) to prevent inadvertently blocking external access to your cluster's control plane.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.2":
    title: '5.4.2 | Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled'
    section: 'Cluster Networking'
    description: |
        Disable access to the Kubernetes API from outside the node network if it is not required.
    remediation: |
        Impact: This topic gives an overview of the options for enabling private access to services within Oracle Cloud
        Infrastructure. Private access means that traffic does not go over the internet. Access can be from hosts within your
        virtual cloud network (VCN) or your on-premises network. You can enable private access to certain services within Oracle
        Cloud Infrastructure from your VCN or on-premises network by using either a private endpoint or a service gateway. See
        the sections that follow. For each private access option, these services or resource types are available: With a private
        endpoint: Autonomous Database (shared Exadata infrastructure) With a service gateway: Available services With either
        private access option, the traffic stays within the Oracle Cloud Infrastructure network and does not traverse the
        internet. However, if you use a service gateway, requests to the service use a public endpoint for the service. If you
        do not want to access a given Oracle service through a public endpoint, Oracle recommends using a private endpoint in
        your VCN (assuming the service supports private endpoints). A private endpoint is represented as a private IP address
        within a subnet in your VCN.
        See                    About Private Endpoints
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.2', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.3":
    title: '5.4.3 | Ensure clusters are created with Private Nodes'
    section: 'Cluster Networking'
    description: |
        Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with
        no public IP addresses.
    remediation: |
        Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing
        enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet
        access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.3', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.4":
    title: '5.4.4 | Ensure Network Policy is Enabled and set as appropriate'
    section: 'Cluster Networking'
    description: |
        Use Network Policy to restrict pod to pod traffic within a cluster and segregate workloads.
    remediation: |
        Configure Network Policy for the Cluster Impact: Network Policy requires the Network Policy add-on. This add-on is
        included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added
        prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling update of all cluster nodes,
        similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster
        (including delete) until it has run to completion.
        If Network Policy is used, a cluster must have at least 2 nodes of type                    n1-standard-1
         or higher. The recommended minimum size cluster to run Network Policy enforcement is 3                    n1-standard-1
         instances.
        Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory
        footprint of the                    kube-system
         process by approximately 128MB, and requires approximately 300 millicores of CPU.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.4', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.5":
    title: '5.4.5 | Encrypt traffic to HTTPS load balancers with TLS certificates - manual'
    section: 'Cluster Networking'
    description: |
        Encrypt traffic to HTTPS load balancers using TLS certificates.
    remediation: |
        Your load balancer vendor can provide details on configuring HTTPS with TLS.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.4.5', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.5.1":
    title: '5.5.1 | Access Control and Container Engine for Kubernetes - manual'
    section: 'Authentication and Authorization'
    description: |
        Cluster Administrators should leverage Oracle Groups and Cloud IAM to assign Kubernetes user roles to a collection of
        users, instead of to individual emails using only Cloud IAM.
    remediation: |
        Example: Granting the Kubernetes RBAC cluster-admin clusterrole Follow these steps to grant a user who is not a tenancy
        administrator the Kubernetes RBAC cluster-admin clusterrole on a cluster deployed on Oracle Cloud Infrastructure:
        If you haven't already done so, follow the steps to set up the cluster's kubeconfig configuration file and (if
        necessary) set the KUBECONFIG environment variable to point to the file. Note that you must set up your own kubeconfig
        file. You cannot access a cluster using a kubeconfig file that a different user set up. See                      Setting
        Up Cluster Access
        .                   In a terminal window, grant the Kubernetes RBAC cluster-admin clusterrole to the user by entering: $
        kubectl create clusterrolebinding <my-cluster-admin-binding> --clusterrole=cluster-admin --user=<user_OCID>
         where:  is a string of your choice to be used as the name for the binding between the user and the Kubernetes RBAC
        cluster-admin clusterrole. For example, jdoe_clst_adm <user_OCID> is the user's OCID (obtained from the Console ). For
        example, ocid1.user.oc1..aaaaa...zutq (abbreviated for readability). For example: $ kubectl create clusterrolebinding
        jdoe_clst_adm --clusterrole=cluster-admin --user=ocid1.user.oc1..aaaaa...zutq
         Impact: Users must now be assigned to the IAM group created to use this namespace and deploy applications. If they are
        not they will not be able to access the namespace or deploy.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.5.1', 'cis_oracle_cloud_infrastructure_container_engine_for_kubernetes(oke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
