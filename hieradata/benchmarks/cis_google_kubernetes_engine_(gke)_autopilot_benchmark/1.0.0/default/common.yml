---

inspec_rules:
  "4.1.1":
    title: '4.1.1 | Ensure that the cluster-admin role is only used where required'
    section: 'RBAC and Service Accounts'
    description: |
        The RBAC role              clusteradmin
         provides wideranging powers over the environment and should be used only where and when needed.
    remediation: |
        Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they
        could use a role with fewer privileges. Where possible, first bind users to a lower-privileged role and then remove the
        clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
         Impact:
        Care should be taken before removing any                    clusterrolebindings
         from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should
        not be made to                    clusterrolebindings
         with the                    system:
         prefix as they are required for the operation of system components.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.2":
    title: '4.1.2 | Minimize access to secrets'
    section: 'RBAC and Service Accounts'
    description: |
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by
        workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to
        reduce the risk of privilege escalation.
    remediation: |
        Where possible, remove                  get
        ,                  list
         and                  watch
         access to                  secret
         objects in the cluster.               Impact: Care should be taken not to remove access to secrets to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.2', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.3":
    title: '4.1.3 | Minimize wildcard use in Roles and ClusterRoles'
    section: 'RBAC and Service Accounts'
    description: |
        Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on
        those objects.  It is possible to set either of these to be the wildcard , which matches all items. Use of wildcards is
        not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are
        added to the Kubernetes API either as CRDs or in later versions of the product.
    remediation: |
        Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.3', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.4":
    title: '4.1.4 | Ensure that default service accounts are not actively used'
    section: 'RBAC and Service Accounts'
    description: |
        The              default
         service account should not be used to ensure that rights granted to applications can be more easily audited and
        reviewed.
    remediation: |
        Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.
        Modify the configuration of each default service account to include this value automountServiceAccountToken: false
         Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be
        created.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.4', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.5":
    title: '4.1.5 | Ensure that Service Account Tokens are only mounted where necessary'
    section: 'RBAC and Service Accounts'
    description: |
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to
        communicate with the API server
    remediation: |
        Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
        Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where
        the resource is available to unauthenticated principals.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.5', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.6":
    title: '4.1.6 | Avoid use of systemmasters group'
    section: 'RBAC and Service Accounts'
    description: |
        The special group              systemmasters
         should not be used to grant permissions to any user or service account, except where strictly necessary e.g.
        bootstrapping access prior to RBAC being fully available
    remediation: |
        Remove the                  system:masters
         group from all users in the cluster.               Impact:
        Once the RBAC system is operational in a cluster                    system:masters
         should not be specifically required, as ordinary bindings from principals to the                    cluster-admin
         cluster role can be made where unrestricted access is required.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.6', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.7":
    title: '4.1.7 | Limit use of the Bind Impersonate and Escalate permissions in the Kubernetes cluster - manual'
    section: 'RBAC and Service Accounts'
    description: |
        Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly
        required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly
        granted by cluster administrators
    remediation: |
        Where possible, remove the impersonate, bind and escalate rights from subjects. Impact: There are some cases where these
        permissions are required for cluster service operation, and care should be taken before removing these permissions from
        system service accounts.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.1.7', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.1.8":
    title: '4.1.8 | Avoid bindings to systemanonymous'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid ClusterRoleBindings nor RoleBindings with the user              systemanonymous
        .
    remediation: |
        Identify all                  clusterrolebindings
         and                  rolebindings
         to the user system:anonymous. Check if they are used and review the permissions associated with the binding using the
        commands in the Audit section above or refer to GKE                  documentation
        .               Strongly consider replacing unsafe bindings with an authenticated, user-defined group. Where possible,
        bind to non-default, user-defined groups with least-privilege roles.
        If there are any unsafe bindings to the user                  system:anonymous
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact: Unauthenticated users will have privileges and permissions associated with roles associated with
        the configured bindings.
        Care should be taken before removing any                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Use a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.8', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.9":
    title: '4.1.9 | Avoid non-default bindings to systemunauthenticated'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid nondefault              ClusterRoleBindings
         and              RoleBindings
         with the group              systemunauthenticated
        , except the              ClusterRoleBinding
                           systempublicinfoviewer
        .
    remediation: |
        Identify all non-default                  clusterrolebindings
         and                  rolebindings
         to the group                  system:unauthenticated
        . Check if they are used and review the permissions associated with the binding using the commands in the Audit section
        above or refer to GKE                  documentation
        .               Strongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group.
        Where possible, bind to non-default, user-defined groups with least-privilege roles.
        If there are any non-default, unsafe bindings to the group                  system:unauthenticated
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --
        namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact: Unauthenticated users will have privileges and permissions associated with roles associated with
        the configured bindings.
        Care should be taken before removing any non-default                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.9', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.10":
    title: '4.1.10 | Avoid non-default bindings to systemauthenticated'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid nondefault              ClusterRoleBindings
         and              RoleBindings
         with the group              systemauthenticated
        , except the              ClusterRoleBindings
                           systembasicuser
        ,              systemdiscovery
        , and              systempublicinfoviewer
        .
        Googles approach to authentication is to make authenticating to Google Cloud and GKE as simple and secure as possible
        without adding complex configuration steps. The group              systemauthenticated
         includes all users with a Google account, which includes all Gmail accounts. Consider your authorization controls with
        this extended group scope when granting permissions. Thus, group              systemauthenticated
         is not recommended for nondefault use.
    remediation: |
        Identify all non-default                  clusterrolebindings
         and                  rolebindings
         to the group                  system:authenticated
        . Check if they are used and review the permissions associated with the binding using the commands in the Audit section
        above or refer to GKE documentation.               Strongly consider replacing non-default, unsafe bindings with an
        authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.
        If there are any non-default, unsafe bindings to the group                  system:authenticated
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact:
        Authenticated users in group                    system:authenticated
         should be treated similarly to users in                    system:unauthenticated
        , having privileges and permissions associated with roles associated with the configured bindings.
        Care should be taken before removing any non-default                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.10', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.2.1":
    title: '4.2.1 | Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces. - manual'
    section: 'Pod Security Standards'
    description: |
        The Pod Security Standard Baseline profile defines a baseline for container security. You can enforce this by using the
        builtin Pod Security Admission controller.
    remediation: |
        Ensure that Pod Security Admission is in place for every namespace which contains user workloads. Run the following
        command to enforce the Baseline profile in a namespace: kubectl label namespace <namespace-name> pod-
        security.kubernetes.io/enforce=baseline
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.2.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.3.1":
    title: '4.3.1 | Ensure that all Namespaces have Network Policies defined'
    section: 'Network Policies and CNI'
    description: |
        Use network policies to isolate traffic in the cluster network.
    remediation: |
        Follow the documentation and create                  NetworkPolicy
         objects as needed.
        See:                  https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy
         for more information.               Impact: Once network policies are in use within a given namespace, traffic not
        explicitly allowed by a network policy will be denied.  As such it is important to ensure that, when introducing network
        policies, legitimate traffic is not blocked.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.3.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.1":
    title: '4.4.1 | Consider external secret storage - manual'
    section: 'Secrets Management'
    description: |
        Consider the use of an external secrets storage and management system instead of using Kubernetes Secrets directly, if
        more complex secret management is required. Ensure the solution requires authentication to access secrets, has auditing
        of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.
    remediation: |
        Refer to the secrets management options offered by the cloud service provider or a third-party secrets management
        solution. Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.4.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.1":
    title: '4.5.1 | Configure Image Provenance using ImagePolicyWebhook admission controller - manual'
    section: 'Extensible Admission Control'
    description: |
        Configure Image Provenance for the deployment.
    remediation: |
        Follow the Kubernetes documentation and setup image provenance. Impact: Regular maintenance for the provenance
        configuration should be carried out, based on container image updates.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.5.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.1":
    title: '4.6.1 | Create administrative boundaries between resources using namespaces - manual'
    section: 'General Policies'
    description: |
        Use namespaces to isolate your Kubernetes objects.
    remediation: |
        Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to
        switch between namespaces for administration.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.6.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.2":
    title: '4.6.2 | Ensure that the seccomp profile is set to RuntimeDefault in the pod definitions'
    section: 'General Policies'
    description: |
        Enable              RuntimeDefault
         seccomp profile in the pod definitions.
    remediation: |
        Use security context to enable the                  RuntimeDefault
         seccomp profile in your pod definitions. An example is as below:
        {
          "namespace": "kube-system",
          "name": "metrics-server-v0.7.0-dbcc8ddf6-gz7d4",
          "seccompProfile": "RuntimeDefault"
        }
                       Impact:
        If the                    RuntimeDefault
         seccomp profile is too restrictive for you, you would have to create/manage your own                    Localhost
         seccomp profiles.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.6.2', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.6.3":
    title: '4.6.3 | Apply Security Context to Pods and Containers - manual'
    section: 'General Policies'
    description: |
        Apply Security Context to Pods and Containers
    remediation: |
        Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts,
        you may refer to the CIS Google Container-Optimized OS Benchmark. Impact: If you incorrectly apply security contexts,
        there may be issues running the pods.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.6.3', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.4":
    title: '4.6.4 | The default namespace should not be used'
    section: 'General Policies'
    description: |
        Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing
        objects in this namespace makes application of RBAC and other controls more difficult.
    remediation: |
        Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new
        resources are created in a specific namespace. Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.6.4', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.1.1":
    title: '5.1.1 | Ensure Image Vulnerability Scanning is enabled'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, being superseded by Artifact Registry starting 15th May 2024. Runtime Vulnerability scanning
        is available via GKE Security Posture Scan images stored in Google Container Registry GCR or Artifact Registry AR for
        vulnerabilities.
    remediation: |
        For Images Hosted in GCR: Using Google Cloud Console
        Go to GCR by visiting:                    https://console.cloud.google.com/gcr
                                 Select Settings and, under the Vulnerability Scanning heading, click the TURN ON button. Using
        Command Line gcloud services enable containeranalysis.googleapis.com
         For Images Hosted in AR: Using Google Cloud Console
        Go to GCR by visiting:                    https://console.cloud.google.com/artifacts
                                 Select Settings and, under the Vulnerability Scanning heading, click the ENABLE button. Using
        Command Line gcloud services enable containerscanning.googleapis.com
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.1.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.1.2":
    title: '5.1.2 | Minimize user access to Container Image repositories - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, see the references for more details. Restrict user access to GCR or AR, limiting interaction
        with build images to only authorized personnel and service accounts.
    remediation: |
        For Images Hosted in AR: Using Google Cloud Console:
        Go to Artifacts Browser by visiting                    https://console.cloud.google.com/artifacts

        From the list of artifacts select each repository with format                    Docker
                                 Under the Permissions tab, modify the roles for each member and ensure only authorized users
        have the Artifact Registry Administrator, Artifact Registry Reader, Artifact Registry Repository Administrator and
        Artifact Registry Writer roles. Using Command Line: gcloud artifacts repositories set-iam-policy <repository-name>
        <path-to-policy-file> --location <repository-location>

        To learn how to configure policy files see:                  https://cloud.google.com/artifact-registry/docs/access-
        control#grant
                              For Images Hosted in GCR: Using Google Cloud Console: To modify roles granted at the GCR bucket
        level:
        Go to Storage Browser by visiting:                    https://console.cloud.google.com/storage/browser
        .
        From the list of storage buckets, select                    artifacts.<project_id>.appspot.com
         for the GCR bucket
        Under the Permissions tab, modify permissions of the identified member via the drop-down role menu and change the Role
        to                    Storage Object Viewer
         for read-only access.
        For a User or Service account with Project level permissions inherited by the GCR bucket, or the
        Service Account User Role
        :
        Go to IAM by visiting:                    https://console.cloud.google.com/iam-admin/iam
                                 Find the User or Service account to be modified and click on the corresponding pencil icon.
        Remove the                    create
        /                   modify
         role (                   Storage Admin
         /                    Storage Object Admin
         /                    Storage Object Creator
         /                    Service Account User
        ) on the user or service account.
        If required add the                    Storage Object Viewer
         role - note with caution that this permits the account to view all objects stored in GCS for the project.
        Using Command Line: To change roles at the GCR bucket level:
        Firstly, run the following if read permissions are required: gsutil iam ch <type>:<email_address>:objectViewer
        gs://artifacts.<project_id>.appspot.com

        Then remove the excessively privileged role (                 Storage Admin
         /                  Storage Object Admin
         /                  Storage Object Creator
        ) using:               gsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com
         where: <type>
         can be one of the following:
                           user
        , if the                        <email_address>
         is a Google account.                     serviceAccount
        , if                        <email_address>
         specifies a Service account.                     <email_address>
         can be one of the following:

        a Google account (for example,                            someone@example.com
        ).                         a Cloud IAM service account.

                                 To modify roles defined at the project level and subsequently inherited within the GCR bucket,
        or the Service Account User role, extract the IAM policy file, modify it accordingly and apply it using: gcloud projects
        set-iam-policy <project_id> <policy_file>
         Impact: Care should be taken not to remove access to GCR or AR for accounts that require this for their operation.
        Any account granted the Storage Object Viewer role at the project level can view all objects stored in GCS for the
        project.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.2', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.3":
    title: '5.1.3 | Minimize cluster access to read-only for Container Image repositories - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, see the references for more details. Configure the Cluster Service Account with Artifact
        Registry Viewer Role to only allow readonly access to AR repositories.
        Configure the Cluster Service Account with Storage Object Viewer Role to only allow readonly access to GCR.
    remediation: |
        For Images Hosted in AR: Using Google Cloud Console:
        Go to Artifacts Browser by visiting                    https://console.cloud.google.com/artifacts
                                 From the list of repositories, for each repository with Format Docker Under the Permissions
        tab, modify the permissions for GKE Service account and ensure that only the Artifact Registry Viewer role is set. Using
        Command Line:
        Add artifactregistry.reader role
        gcloud artifacts repositories add-iam-policy-binding <repository> \
        --location=<repository-location> \
        --member='serviceAccount:<email-address>' \
        --role='roles/artifactregistry.reader'

        Remove any roles other than                  artifactregistry.reader

        gcloud artifacts repositories remove-iam-policy-binding <repository> \
        --location <repository-location> \
        --member='serviceAccount:<email-address>' \
        --role='<role-name>'
                       For Images Hosted in GCR: Using Google Cloud Console: For an account explicitly granted access to the
        bucket:
        Go to Storage Browser by visiting:                    https://console.cloud.google.com/storage/browser
        .
        From the list of storage buckets, select                    artifacts.<project_id>.appspot.com
         for the GCR bucket.
        Under the Permissions tab, modify permissions of the identified GKE Service Account via the drop-down role menu and
        change to the Role to                    Storage Object Viewer
         for read-only access.                 For an account that inherits access to the bucket through Project level
        permissions:
        Go to IAM console by visiting:                    https://console.cloud.google.com/iam-admin
        .                 From the list of accounts, identify the required service account and select the corresponding pencil
        icon.
        Remove the                    Storage Admin
         /                    Storage Object Admin
         /                    Storage Object Creator
         roles.
        Add the                    Storage Object Viewer
         role - note with caution that this permits the account to view all objects stored in GCS for the project.
        Click                    SAVE
        .                 Using Command Line: For an account explicitly granted to the bucket:
        Firstly add read access to the Kubernetes Service Account: gsutil iam ch <type>:<email_address>:objectViewer
        gs://artifacts.<project_id>.appspot.com
         where: <type>
         can be one of the following:
                           user
        , if the                        <email_address>
         is a Google account.                     serviceAccount
        , if                        <email_address>
         specifies a Service account.                     <email_address>
         can be one of the following:

        a Google account (for example,                            someone@example.com
        ).                         a Cloud IAM service account.


        Then remove the excessively privileged role (                 Storage Admin
         /                  Storage Object Admin
         /                  Storage Object Creator
        ) using:               gsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com
         For an account that inherits access to the GCR Bucket through Project level permissions, modify the Projects IAM policy
        file accordingly, then upload it using: gcloud projects set-iam-policy <project_id> <policy_file>
         Impact: A separate dedicated service account may be required for use by build servers and other robot users pushing or
        managing container images. Any account granted the Storage Object Viewer role at the project level can view all objects
        stored in GCS for the project.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.3', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.4":
    title: '5.1.4 | Ensure only trusted container images are used'
    section: 'Image Registry and Image Scanning'
    description: |
        Use Binary Authorization to allowlist whitelist only approved container registries.
    remediation: |
        Using Google Cloud Console:
        Go to Binary Authorization by visiting:                    https://console.cloud.google.com/security/binary-
        authorization
                                 Enable Binary Authorization API (if disabled).
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes cluster for which Binary Authorization is disabled.
        Within the                    Details
         pane, under the                    Security
         heading, click on the pencil icon called                    Edit binary authorization
        .
        Ensure that                    Enable Binary Authorization
         is checked.
        Click                    SAVE CHANGES
        .
        Return to the Binary Authorization by visiting:                    https://console.cloud.google.com/security/binary-
        authorization
        .                 Set an appropriate policy for the cluster and enter the approved container registries under Image
        paths. Using Command Line: Update the cluster to enable Binary Authorization: gcloud container cluster update
        <cluster_name> --enable-binauthz

        Create a Binary Authorization Policy using the Binary Authorization Policy Reference:
        https://cloud.google.com/binary-authorization/docs/policy-yaml-reference
         for guidance.               Import the policy file into Binary Authorization: gcloud container binauthz policy import
        <yaml_policy>
         Impact: All container images to be deployed to the cluster must be hosted within an approved container image registry.
        If public registries are not on the allowlist, a process for bringing commonly used container images into an approved
        private registry and keeping them up to date will be required.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.1.4', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.2.1":
    title: '5.2.1 | Ensure GKE clusters are not running using the Compute Engine default service account'
    section: 'Identity and Access Management (IAM)'
    description: |
        Create and use minimally privileged Service accounts to run GKE clusters instead of using the Compute Engine default
        Service account. Unnecessary permissions could be abused in the case of a node compromise.
    remediation: |
        Using Google Cloud Console: To create a minimally privileged service account:
        Go to Service Accounts by visiting:                    https://console.cloud.google.com/iam-admin/serviceaccounts
        .
        Click on                    CREATE SERVICE ACCOUNT
        .                 Enter Service Account Details.
        Click                    CREATE AND CONTINUE
        .
        Within Service Account permissions add the following roles:
                           Logs Writer
        .                     Monitoring Metric Writer
        .                     `Monitoring Viewer.

        Click                    CONTINUE
        .                 Grant users access to this service account and create keys as required.
        Click                    DONE
        .                 Note: A new cluster will need to be created specifying the minimally privileged service account, and
        workloads will need to be migrated to the new cluster and the old cluster deleted. Using Command Line: To create a
        minimally privileged service account:
        gcloud iam service-accounts create <node_sa_name> --display-name "GKE Node Service Account"
        export NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service
        Account'
                       Grant the following roles to the service account:
        export PROJECT_ID=gcloud config get-value project
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/monitoring.metricWriter
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/monitoring.viewer
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/logging.logWriter
                       Note: A new cluster will need to be created specifying the minimally privileged service account, and
        workloads will need to be migrated to the new cluster and the old cluster deleted. Impact:
        Instances are automatically granted the                    https://www.googleapis.com/auth/cloud-platform
         scope to allow full access to all Google Cloud APIs. This is so that the IAM permissions of the instance are completely
        determined by the IAM roles of the Service account. Thus if Kubernetes workloads were using cluster access scopes to
        perform actions using Google APIs, they may no longer be able to, if not permitted by the permissions of the Service
        account. To remediate, follow recommendation 5.2.2.                 The Service account roles listed here are the
        minimum required to run the cluster. Additional roles may be required to pull from a private instance of Google
        Container Registry (GCR).
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.2.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.3.1":
    title: '5.3.1 | Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS'
    section: 'Cloud Key Management Service (Cloud KMS)'
    description: |
        Encrypt Kubernetes secrets, stored in etcd, at the applicationlayer using a customermanaged key in Cloud KMS.
    remediation: |
        To enable Application-layer Secrets Encryption, several configuration items are required. These include: A key ring A
        key
        A GKE service account with                    Cloud KMS CryptoKey Encrypter/Decrypter
         role                 Once these are created, Application-layer Secrets Encryption can be enabled on an existing or new
        cluster. Using Google Cloud Console: To create a key
        Go to Cloud KMS by visiting                    https://console.cloud.google.com/security/kms
        .
        Select                    CREATE KEY RING
        .                 Enter a Key ring name and the region where the keys will be stored.
        Click                    CREATE
        .                 Enter a Key name and appropriate rotation period within the Create key pane.
        Click                    CREATE
        .                 To enable on a new cluster
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .
        Click                    CREATE CLUSTER
        , and choose the required cluster mode.
        Within the                    Security
         heading, under                    CLUSTER
        , check                    Encrypt secrets at the application layer
         checkbox.                 Select the kms key as the customer-managed key and, if prompted, grant permissions to the GKE
        Service account.
        Click                    CREATE
        .                 To enable on an existing cluster
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the cluster to be updated. Under the Details pane, within the Security heading, click on the
        pencil named Application-layer secrets encryption.
        Enable                    Encrypt secrets at the application layer
         and choose a kms key.
        Click                    SAVE CHANGES
        .                 Using Command Line: To create a key:
        Create a key ring: gcloud kms keyrings create <ring_name> --location <location> --project <key_project_id>
         Create a key: gcloud kms keys create <key_name> --location <location> --keyring <ring_name> --purpose encryption
        --project <key_project_id>

        Grant the Kubernetes Engine Service Agent service account the                  Cloud KMS CryptoKey Encrypter/Decrypter
         role:               gcloud kms keys add-iam-policy-binding <key_name> --location <location> --keyring <ring_name>
        --member serviceAccount:<service_account_name> --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project
        <key_project_id>
         To create a new cluster with Application-layer Secrets Encryption: gcloud container clusters create <cluster_name>
        --cluster-version=latest --zone <zone> --database-encryption-key
        projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project <cluster_project_id>
         To enable on an existing cluster: gcloud container clusters update <cluster_name> --zone <zone> --database-encryption-
        key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project
        <cluster_project_id>
         Impact: To use the Cloud KMS CryptoKey to protect etcd in the cluster, the 'Kubernetes Engine Service Agent' Service
        account must hold the 'Cloud KMS CryptoKey Encrypter/Decrypter' role.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.3.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.1":
    title: '5.4.1 | Enable VPC Flow Logs and Intranode Visibility'
    section: 'Cluster Networking'
    description: |
        Enable VPC Flow Logs and Intranode Visibility to see podlevel traffic, even for traffic within a worker node.
    remediation: |
        Enable Intranode Visibility:
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes clusters for which intranode visibility is disabled.
        Within the                    Details
         pane, under the                    Network
         section, click on the pencil icon named                    Edit intranode visibility
        .
        Check the box next to                    Enable Intranode visibility
        .
        Click                    SAVE CHANGES
        .                 Using Command Line: To enable intranode visibility on an existing cluster, run the following command:
        gcloud container clusters update <cluster_name> --enable-intra-node-visibility
         Enable VPC Flow Logs:
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes clusters for which VPC Flow Logs are disabled.
        Select                    Nodes
         tab.                 Select Node Pool without VPC Flow Logs enabled. Select an Instance Group within the node pool.
        Select an                    Instance Group Member
        .
        Select the                    Subnetwork
         under Network Interfaces.
        Click on                    EDIT
        .
        Set Flow logs to                    On
        .
        Click                    SAVE
        .                 Using Command Line: Find the subnetwork name associated with the cluster. gcloud container clusters
        describe <cluster_name> --region <cluster_region> --format json | jq '.subnetwork'
         Update the subnetwork to enable VPC Flow Logs. gcloud compute networks subnets update <subnet_name> --enable-flow-logs
         Impact: Enabling it on existing cluster causes the cluster master and the cluster nodes to restart, which might cause
        disruption.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.2":
    title: '5.4.2 | Ensure Control Plane Authorized Networks is Enabled'
    section: 'Cluster Networking'
    description: |
        Enable Control Plane Authorized Networks to restrict access to the clusters control plane to only an allowlist of
        authorized IPs.
    remediation: |
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 Select Kubernetes clusters for which Control Plane Authorized Networks is disabled Within the
        Details pane, under the Networking heading, click on the pencil icon named Edit control plane authorised networks. Check
        the box next to Enable control plane authorised networks. Click SAVE CHANGES. Using Command Line: To enable Control
        Plane Authorized Networks for an existing cluster, run the following sample command changing the IP range for fit your
        network:
        gcloud container clusters update $CLUSTER_NAME --region $REGION --enable-master-authorized-networks --master-authorized-
        networks 192.168.1.0/24

        Along with this, you can list authorized networks using the                  --master-authorized-networks
         flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's control plane
        through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as
        90.90.100.0/24
        ).               Impact: When implementing Control Plane Authorized Networks, be careful to ensure all desired networks
        are on the allowlist to prevent inadvertently blocking external access to your cluster's control plane.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.2', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.3":
    title: '5.4.3 | Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled'
    section: 'Cluster Networking'
    description: |
        Disable access to the Kubernetes API from outside the node network if it is not required.
    remediation: |
        Once a cluster is created without enabling Private Endpoint only, it cannot be remediated. Rather, the cluster must be
        recreated. Using Google Cloud Console:
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 Click CREATE CLUSTER, and choose CONFIGURE for the Standard mode cluster. Configure the cluster
        as required then click Networking under CLUSTER in the navigation pane. Under IPv4 network access, click the Private
        cluster radio button. Uncheck the Access control plane using its external IP address checkbox. In the Control plane IP
        range textbox, provide an IP range for the control plane. Configure the other settings as required, and click CREATE.
        Using Command Line:
        Create a cluster with a Private Endpoint enabled and Public Access disabled by including the                  --enable-
        private-endpoint
         flag within the cluster create command:               gcloud container clusters create-auto <cluster_name> --location
        $LOCATION --enable-private-endpoint

        Setting this flag also requires the setting of                  --enable-private-nodes
         and                  --enable-master-authorized-networks
        .               Impact: To enable a Private Endpoint, the cluster has to also be configured with private nodes, a
        private master IP range and IP aliasing enabled.
        If the Private Endpoint flag                    --enable-private-endpoint
         is passed to the gcloud CLI, or the external IP address undefined in the Google Cloud Console during cluster creation,
        then all access from a public IP address is prohibited.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.3', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.4":
    title: '5.4.4 | Ensure clusters are created with Private Nodes'
    section: 'Cluster Networking'
    description: |
        Private Nodes are nodes with no public IP addresses. Disable public IP addresses for cluster nodes, so that they only
        have private IP addresses.
    remediation: |
        Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated.
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Click CREATE CLUSTER. Configure the cluster as required then click Networking under CLUSTER in the
        navigation pane. Under IPv4 network access, click the Private cluster radio button. Configure the other settings as
        required, and click CREATE. Using Command Line:
        To create a cluster with Private Nodes enabled, include the                  --enable-private-nodes
         flag within the cluster create command:               gcloud container clusters create <cluster_name> --location
        $LOCATION --enable-private-nodes
         Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing
        enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet
        access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway. To access Google Cloud APIs
        and services from private nodes, Private Google Access needs to be set on Kubernetes Engine Cluster Subnets.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.4', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.5":
    title: '5.4.5 | Ensure use of Google-managed SSL Certificates'
    section: 'Cluster Networking'
    description: |
        Encrypt traffic to HTTPS load balancers using Googlemanaged SSL certificates.
    remediation: |
        If services of                  type:LoadBalancer
         are discovered, consider replacing the Service with an Ingress.
        To configure the Ingress and use Google-managed SSL certificates, follow the instructions as listed at:
        https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
        .               Impact: Google-managed SSL Certificates are less flexible than certificates that are self obtained and
        managed. Managed certificates support a single, non-wildcard domain. Self-managed certificates can support wildcards and
        multiple subject alternative names (SANs).
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.5', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "5.5.1":
    title: '5.5.1 | Manage Kubernetes RBAC users with Google Groups for GKE - manual'
    section: 'Authentication and Authorization'
    description: |
        Cluster Administrators should leverage G Suite Groups and Cloud IAM to assign Kubernetes user roles to a collection of
        users, instead of to individual emails using only Cloud IAM.
    remediation: |
        Follow the G Suite Groups instructions at:                  https://cloud.google.com/kubernetes-engine/docs/how-to/role-
        based-access-control#google-groups-for-gke
        .               Then, create a cluster with: gcloud container clusters create <cluster_name> --security-group
        <security_group_name>

        Finally create                  Roles
        ,                  ClusterRoles
        ,                  RoleBindings
        , and                  ClusterRoleBindings
         that reference the G Suite Groups.               Impact:
        When migrating to using security groups, an audit of                    RoleBindings
         and                    ClusterRoleBindings
         is required to ensure all users of the cluster are managed using the new groups and not individually.
        When managing                    RoleBindings
         and                    ClusterRoleBindings
        , be wary of inadvertently removing bindings required by service accounts.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.5.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.6.1":
    title: '5.6.1 | Enable Customer-Managed Encryption Keys CMEK for GKE Persistent Disks PD - manual'
    section: 'Storage'
    description: |
        Use CustomerManaged Encryption Keys CMEK to encrypt dynamicallyprovisioned attached Google Compute Engine Persistent
        Disks PDs using keys managed within Cloud Key Management Service Cloud KMS.
    remediation: |
        This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster
        created. Using Google Cloud Console: This is not possible using Google Cloud Console. Using Command Line:
        Follow the instructions detailed at:                  https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek
        .               Impact: Encryption of dynamically-provisioned attached disks requires the use of the self-provisioned
        Compute Engine Persistent Disk CSI Driver v0.5.1 or higher. If CMEK is being configured with a regional cluster, the
        cluster must run GKE 1.14 or higher.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.6.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.7.1":
    title: '5.7.1 | Enable Security Posture - manual'
    section: 'Other Cluster Configurations'
    description: |
        
    remediation: |
        Enable security posture via the UI, gCloud or API.                  https://cloud.google.com/kubernetes-engine/docs/how-
        to/protect-workload-configuration
                              Impact:
        GKE security posture configuration auditing checks your workloads against a set of defined best practices. Each
        configuration check has its own impact or risk. Learn more about the checks:
        https://cloud.google.com/kubernetes-engine/docs/concepts/about-configuration-scanning
                                 Example: The host namespace check identifies pods that share host namespaces. Pods that share
        host namespaces allow Pod processes to communicate with host processes and gather host information, which could lead to
        a container escape
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.7.1', 'cis_google_kubernetes_engine_(gke)_autopilot_benchmark']
    enabled: false
    properties:
      match: all
      rules:
