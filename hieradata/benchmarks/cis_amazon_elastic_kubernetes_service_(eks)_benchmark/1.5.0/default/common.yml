---

inspec_rules:
  "2.1.1":
    title: '2.1.1 | Enable audit Logs'
    section: 'Logging'
    description: |
        Control plane logs provide visibility into operation of the EKS Control plane component systems. The API server audit
        logs record all accepted and rejected requests in the cluster. When enabled via EKS configuration the control plane logs
        for a cluster are exported to a CloudWatch Log Group for persistence.
    remediation: |
        From Console: For each EKS Cluster in each region; Go to 'Amazon EKS' > 'Clusters' > '' > 'Configuration' > 'Logging'.
        Click 'Manage logging'. Ensure that all options are toggled to 'Enabled'.
        API server: Enabled
        Audit: Enabled
        Authenticator: Enabled
        Controller manager: Enabled
        Scheduler: Enabled
                       Click 'Save Changes'. From CLI:
        # For each EKS Cluster in each region;
        aws eks update-cluster-config \
            --region '${REGION_CODE}' \
            --name '${CLUSTER_NAME}' \
            --logging
        '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":true}]}'
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_2.1.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.1.1":
    title: '3.1.1 | Ensure that the kubeconfig file permissions are set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        If kubelet is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has
        permissions of 644 or more restrictive.
    remediation: |
        Run the below command (based on the file location on your system) on the each worker
        node. For example, chmod 644 <kubeconfig file>
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.2":
    title: '3.1.2 | Ensure that the kubelet kubeconfig file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubelet
         is running, ensure that the file ownership of its kubeconfig file is set to              rootroot
        .
    remediation: |
        Run the below command (based on the file location on your system) on each worker node. For example, chown root:root
        <proxy kubeconfig file>
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.1.3":
    title: '3.1.3 | Ensure that the kubelet configuration file has permissions set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet refers to a configuration file with the              config
         argument, that file has permissions of 644 or more restrictive.
    remediation: |
        Run the following command (using the config file location identified in the Audit step) chmod 644
        /etc/kubernetes/kubelet/kubelet-config.json
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.4":
    title: '3.1.4 | Ensure that the kubelet configuration file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet refers to a configuration file with the              config
         argument, that file is owned by rootroot.
    remediation: |
        Run the following command (using the config file location identified in the Audit step) chown root:root
        /etc/kubernetes/kubelet/kubelet-config.json
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.2.1":
    title: '3.2.1 | Ensure that the Anonymous Auth is Not Enabled - manual'
    section: 'Kubelet'
    description: |
        Disable anonymous requests to the Kubelet server.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Disable Anonymous Authentication by setting the following parameter: "authentication": { "anonymous": { "enabled":
        false } }
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:               --anonymous-auth=false
         For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Anonymous requests will be rejected.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*anonymous.*{.enabled.:false}'

  "3.2.2":
    title: '3.2.2 | Ensure that the --authorization-mode argument is not set to AlwaysAllow - manual'
    section: 'Kubelet'
    description: |
        Do not allow all requests. Enable explicit authorization.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Enable Webhook Authentication by setting the following parameter: "authentication": { "webhook": { "enabled": true } }

        Next, set the Authorization Mode to                  Webhook
         by setting the following parameter:               "authorization": { "mode": "Webhook }

        Finer detail of the                  authentication
         and                  authorization
         fields can be found in the                  Kubelet Configuration documentation
        .               Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:
        --authentication-token-webhook
        --authorization-mode=Webhook
                       For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Unauthorized requests will be denied.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*"webhook":{"enabled":true'

  "3.2.3":
    title: '3.2.3 | Ensure that a Client CA File is Configured - manual'
    section: 'Kubelet'
    description: |
        Enable Kubelet authentication using certificates.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Configure the client certificate authority file by setting the following parameter appropriately: "authentication": {
        "x509": {"clientCAFile": <path/to/client-ca-file> } }"
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:               --client-ca-file=<path/to/client-ca-file>
         For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: You require TLS to be configured on apiserver as well as kubelets.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*clientCAFile":".[a-zA-Z]'

  "3.2.4":
    title: '3.2.4 | Ensure that the --read-only-port is disabled'
    section: 'Kubelet'
    description: |
        Disable the readonly port.
    remediation: |
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to 0               "readOnlyPort": 0

        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --read-only-port=0

        For each remediation:
        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Removal of the read-only port will require that any service which made use of it will need to be
        re-configured to use the main Kubelet API.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "3.2.5":
    title: '3.2.5 | Ensure that the --streaming-connection-idle-timeout argument is not set to 0 - manual'
    section: 'Kubelet'
    description: |
        Do not disable timeouts on streaming connections.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to a non-zero value in the format of #h#m#s               "streamingConnectionIdleTimeout":
        "4h0m0s"

        You should ensure that the kubelet service file                  /etc/systemd/system/kubelet.service.d/10-kubelet-
        args.conf
         does not specify a                  --streaming-connection-idle-timeout
         argument because it would override the Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --streaming-connection-idle-timeout=4h0m0s
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "streamingConnectionIdleTimeout":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Long-lived connections could be interrupted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.5', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == streamingConnectionIdleTimeout":"[1-9]'

  "3.2.6":
    title: '3.2.6 | Ensure that the --make-iptables-util-chains argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Allow Kubelet to manage iptables.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true               "makeIPTablesUtilChains": true

        Ensure that                  /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         does not set the                  --make-iptables-util-chains
         argument because that would override your Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --make-iptables-util-chains:true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of                  "makeIPTablesUtilChains.: true
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other
        iptables management solution, then there might be some conflicts.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.6', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == makeIPTablesUtilChains.:true'

  "3.2.7":
    title: '3.2.7 | Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture - manual'
    section: 'Kubelet'
    description: |
        Security relevant information should be captured.  The eventRecordQPS on the Kubelet configuration can be used to limit
        the rate at which events are gathered and sets the maximum event creations per second.  Setting this too low could
        result in relevant events not being logged, however the unlimited setting of              0
         could result in a denial of service on the kubelet.
    remediation: |
        If using a Kubelet config file, edit the file to set                  eventRecordQPS:
         to an appropriate level.
        If using command line arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
         on each worker node and set the below parameter in                  KUBELET_SYSTEM_PODS_ARGS
         variable.
        Based on your system, restart the                  kubelet
         service. For example:
        systemctl daemon-reload
        systemctl restart kubelet.service
                       Impact:
        Setting this parameter to                    0
         could result in a denial of service condition due to excessive events being created.  The cluster's event processing
        and storage systems should be scaled to handle expected event loads.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.7', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "eventRecordQPS":[1-9]\d*'

  "3.2.8":
    title: '3.2.8 | Ensure that the --rotate-certificates argument is not present or is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet client certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true               "RotateCertificate":true
         Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not
        set the --RotateCertificate executable argument to false because this would override the Kubelet config file.
        Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --RotateCertificate=true
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.8', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "rotateCertificates":true'

  "3.2.9":
    title: '3.2.9 | Ensure that the RotateKubeletServerCertificate argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet server certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true
        "featureGates": {
          "RotateKubeletServerCertificate":true
        },

        Additionally, ensure that the kubelet service file                  /etc/systemd/system/kubelet.service.d/10-kubelet-
        args.conf
         does not set the                  --rotate-kubelet-server-certificate
         executable argument to false because this would override the Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --rotate-kubelet-server-certificate=true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "RotateKubeletServerCertificate":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediation methods:

        Restart the                  kubelet
         service and check status. The example below is for when using systemctl to manage services:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.9', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == RotateKubeletServerCertificate":true'

  "4.1.1":
    title: '4.1.1 | Ensure that the cluster-admin role is only used where required'
    section: 'RBAC and Service Accounts'
    description: |
        The RBAC role              clusteradmin
         provides wideranging powers over the environment and should be used only where and when needed.
    remediation: |
        Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they
        could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the
        clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
         Impact:
        Care should be taken before removing any                    clusterrolebindings
         from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should
        not be made to                    clusterrolebindings
         with the                    system:
         prefix as they are required for the operation of system components.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.2":
    title: '4.1.2 | Minimize access to secrets'
    section: 'RBAC and Service Accounts'
    description: |
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by
        workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to
        reduce the risk of privilege escalation.
    remediation: |
        Where possible, remove                  get
        ,                  list
         and                  watch
         access to                  secret
         objects in the cluster.               Impact: Care should be taken not to remove access to secrets to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.3":
    title: '4.1.3 | Minimize wildcard use in Roles and ClusterRoles'
    section: 'RBAC and Service Accounts'
    description: |
        Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on
        those objects.  It is possible to set either of these to be the wildcard  which matches all items. Use of wildcards is
        not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are
        added to the Kubernetes API either as CRDs or in later versions of the product.
    remediation: |
        Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.4":
    title: '4.1.4 | Minimize access to create pods'
    section: 'RBAC and Service Accounts'
    description: |
        The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as
        assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data unless Pod
        Security Policies are implemented to restrict this access As such, access to create new pods should be restricted to the
        smallest possible group of users.
    remediation: |
        Where possible, remove                  create
         access to                  pod
         objects in the cluster.               Impact: Care should be taken not to remove access to pods to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.5":
    title: '4.1.5 | Ensure that default service accounts are not actively used.'
    section: 'RBAC and Service Accounts'
    description: |
        The              default
         service account should not be used to ensure that rights granted to applications can be more easily audited and
        reviewed.
    remediation: |
        Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.
        Modify the configuration of each default service account to include this value automountServiceAccountToken: false
         Automatic remediation for the default account: kubectl patch serviceaccount default -p $'automountServiceAccountToken:
        false' Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be
        created.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.5', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.6":
    title: '4.1.6 | Ensure that Service Account Tokens are only mounted where necessary'
    section: 'RBAC and Service Accounts'
    description: |
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to
        communicate with the API server
    remediation: |
        Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
        Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where
        the resource is available to unauthenticated principals.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.6', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.7":
    title: '4.1.7 | Avoid use of systemmasters group'
    section: 'RBAC and Service Accounts'
    description: |
        The special group              systemmasters
         should not be used to grant permissions to any user or service account, except where strictly necessary e.g.
        bootstrapping access prior to RBAC being fully available
    remediation: |
        Remove the                  system:masters
         group from all users in the cluster.               Impact:
        Once the RBAC system is operational in a cluster                    system:masters
         should not be specifically required, as ordinary bindings from principals to the                    cluster-admin
         cluster role can be made where unrestricted access is required.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.7', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.8":
    title: '4.1.8 | Limit use of the Bind Impersonate and Escalate permissions in the Kubernetes cluster - manual'
    section: 'RBAC and Service Accounts'
    description: |
        Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly
        required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly
        granted by cluster administrators
    remediation: |
        Where possible, remove the impersonate, bind and escalate rights from subjects. Impact: There are some cases where these
        permissions are required for cluster service operation, and care should be taken before removing these permissions from
        system service accounts.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.1.8', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.2.1":
    title: '4.2.1 | Minimize the admission of privileged containers'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              securityContext.privileged
         flag set to              true
        .
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged
        containers. To enable PSA for a namespace in your cluster, set the pod-security.kubernetes.io/enforce label with the
        policy value you want to enforce. kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
        The above command enforces the restricted policy for the NAMESPACE namespace. You can also enable Pod Security Admission
        for all your namespaces. For example: kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
        Impact:
        Pods defined with                    spec.containers[].securityContext.privileged: true
        ,                    spec.initContainers[].securityContext.privileged: true
         and                    spec.ephemeralContainers[].securityContext.privileged: true
         will not be permitted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.2":
    title: '4.2.2 | Minimize the admission of containers wishing to share the host process ID namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostPID
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostPID
         containers.               Impact:
        Pods defined with                    spec.hostPID: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.3":
    title: '4.2.3 | Minimize the admission of containers wishing to share the host IPC namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostIPC
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostIPC
         containers.               Impact:
        Pods defined with                    spec.hostIPC: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.4":
    title: '4.2.4 | Minimize the admission of containers wishing to share the host network namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostNetwork
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostNetwork
         containers.               Impact:
        Pods defined with                    spec.hostNetwork: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.5":
    title: '4.2.5 | Minimize the admission of containers with allowPrivilegeEscalation'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              allowPrivilegeEscalation
         flag set to              true
        . Allowing this right can lead to a process running a container getting more rights than it started with.           Its
        important to note that these rights are still constrained by the overall container sandbox, and this setting does not
        relate to the use of privileged containers.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with
        .spec.allowPrivilegeEscalation
         set to                  true
        .               Impact:
        Pods defined with                    spec.allowPrivilegeEscalation: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.5', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.3.1":
    title: '4.3.1 | Ensure CNI plugin supports network policies. - manual'
    section: 'CNI Plugin'
    description: |
        There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may
        not be possible to effectively restrict traffic in the cluster.
    remediation: |
        As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny
        all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.
        Impact: None.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.3.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.3.2":
    title: '4.3.2 | Ensure that all Namespaces have Network Policies defined'
    section: 'CNI Plugin'
    description: |
        Use network policies to isolate traffic in your cluster network.
    remediation: |
        Follow the documentation and create                  NetworkPolicy
         objects as you need them.               Impact: Once there is any Network Policy in a namespace selecting a particular
        pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that
        are not selected by any Network Policy will continue to accept all traffic"
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.3.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.1":
    title: '4.4.1 | Prefer using secrets as files over secrets as environment variables'
    section: 'Secrets Management'
    description: |
        Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment
        variable secrets.
    remediation: |
        If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.
        Impact: Application code which expects to read secrets in the form of environment variables would need modification
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.4.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.2":
    title: '4.4.2 | Consider external secret storage - manual'
    section: 'Secrets Management'
    description: |
        Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if
        you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has
        auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.
    remediation: |
        Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.
        Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.4.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.1":
    title: '4.5.1 | Create administrative boundaries between resources using namespaces - manual'
    section: 'General Policies'
    description: |
        Use namespaces to isolate your Kubernetes objects.
    remediation: |
        Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to
        switch between namespaces for administration.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.5.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.2":
    title: '4.5.2 | Apply Security Context to Your Pods and Containers - manual'
    section: 'General Policies'
    description: |
        Apply Security Context to Your Pods and Containers
    remediation: |
        As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular
        namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we
        recommend implementing a more restrictive policy such as this:
        apiVersion: policy/v1beta1
        kind: PodSecurityPolicy
        metadata:
            name: restricted
            annotations:
            seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
            apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
            seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
            apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
        spec:
            privileged: false
            # Required to prevent escalations to root.
            allowPrivilegeEscalation: false
            # This is redundant with non-root + disallow privilege escalation,
            # but we can provide it for defense in depth.
            requiredDropCapabilities:
            - ALL
            # Allow core volume types.
            volumes:
            - 'configMap'
            - 'emptyDir'
            - 'projected'
            - 'secret'
            - 'downwardAPI'
            # Assume that persistentVolumes set up by the cluster admin are safe to use.
            - 'persistentVolumeClaim'
            hostNetwork: false
            hostIPC: false
            hostPID: false
            runAsUser:
            # Require the container to run without root privileges.
            rule: 'MustRunAsNonRoot'
            seLinux:
            # This policy assumes the nodes are using AppArmor rather than SELinux.
            rule: 'RunAsAny'
            supplementalGroups:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            fsGroup:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            readOnlyRootFilesystem: false
                       This policy prevents pods from running as privileged or escalating privileges. It also restricts the
        types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar,
        approach is to start with policy that locks everything down and incrementally add exceptions for applications that need
        looser restrictions such as logging agents which need the ability to mount a host path. Impact: If you incorrectly apply
        security contexts, you may have trouble running the pods.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.5.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.3":
    title: '4.5.3 | The default namespace should not be used'
    section: 'General Policies'
    description: |
        Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing
        objects in this namespace makes application of RBAC and other controls more difficult.
    remediation: |
        Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new
        resources are created in a specific namespace. Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.5.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.1.1":
    title: '5.1.1 | Ensure Image Vulnerability Scanning using Amazon ECR image scanning or a third party provider'
    section: 'Image Registry and Image Scanning'
    description: |
        Scan images being deployed to Amazon EKS for vulnerabilities.
    remediation: |
        To utilize AWS ECR for Image scanning please follow the steps below: To create a repository configured for scan on push
        (AWS CLI) aws ecr create-repository --repository-name $REPO_NAME --image-scanning-configuration scanOnPush=true --region
        $REGION_CODE
         To edit the settings of an existing repository (AWS CLI) aws ecr put-image-scanning-configuration --repository-name
        $REPO_NAME --image-scanning-configuration scanOnPush=true --region $REGION_CODE
         Use the following steps to start a manual image scan using the AWS Management Console.
        Open the Amazon ECR console at                    https://console.aws.amazon.com/ecr/repositories
        .                 From the navigation bar, choose the Region to create your repository in. In the navigation pane,
        choose Repositories. On the Repositories page, choose the repository that contains the image to scan. On the Images
        page, select the image to scan and then choose Scan. Impact: If you are utilizing AWS ECR The following are common image
        scan failures. You can view errors like this in the Amazon ECR console by displaying the image details or through the
        API or AWS CLI by using the DescribeImageScanFindings API. UnsupportedImageError
        You may get an UnsupportedImageError error when attempting to scan an image that was built using an operating system
        that Amazon ECR doesn't support image scanning for. Amazon ECR supports package vulnerability scanning for major
        versions of Amazon Linux, Amazon Linux 2, Debian, Ubuntu, CentOS, Oracle Linux, Alpine, and RHEL Linux distributions.
        Amazon ECR does not support scanning images built from the Docker scratch image. An UNDEFINED severity level is returned
        You may receive a scan finding that has a severity level of UNDEFINED. The following are the common causes for this: The
        vulnerability was not assigned a priority by the CVE source. The vulnerability was assigned a priority that Amazon ECR
        did not recognize. To determine the severity and description of a vulnerability, you can view the CVE directly from the
        source.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.1.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.1.2":
    title: '5.1.2 | Minimize user access to Amazon ECR - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Restrict user access to Amazon ECR, limiting interaction with build images to only authorized personnel and service
        accounts.
    remediation: |
        Before you use IAM to manage access to Amazon ECR, you should understand what IAM features are available to use with
        Amazon ECR. To get a high-level view of how Amazon ECR and other AWS services work with IAM, see AWS Services That Work
        with IAM in the IAM User Guide. Topics Amazon ECR Identity-Based Policies Amazon ECR Resource-Based Policies
        Authorization Based on Amazon ECR Tags Amazon ECR IAM Roles Amazon ECR Identity-Based Policies With IAM identity-based
        policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are
        allowed or denied. Amazon ECR supports specific actions, resources, and condition keys. To learn about all of the
        elements that you use in a JSON policy, see IAM JSON Policy Elements Reference in the IAM User Guide. Actions

        The Action element of an IAM identity-based policy describes the specific action or actions that will be allowed or
        denied by the policy. Policy actions usually have the same name as the associated AWS API operation. The action is used
        in a policy to grant permissions to perform the associated operation.               Policy actions in Amazon ECR use the
        following prefix before the action: ecr:. For example, to grant someone permission to create an Amazon ECR repository
        with the Amazon ECR CreateRepository API operation, you include the ecr:CreateRepository action in their policy. Policy
        statements must include either an Action or NotAction element. Amazon ECR defines its own set of actions that describe
        tasks that you can perform with this service. To specify multiple actions in a single statement, separate them with
        commas as follows: "Action": [ "ecr:action1", "ecr:action2" You can specify multiple actions using wildcards (*). For
        example, to specify all actions that begin with the word Describe, include the following action: "Action":
        "ecr:Describe*" To see a list of Amazon ECR actions, see Actions, Resources, and Condition Keys for Amazon Elastic
        Container Registry in the IAM User Guide. Resources

        The Resource element specifies the object or objects to which the action applies. Statements must include either a
        Resource or a NotResource element. You specify a resource using an ARN or using the wildcard (*) to indicate that the
        statement applies to all resources.               An Amazon ECR repository resource has the following ARN:
        arn:${Partition}:ecr:${Region}:${Account}:repository/${Repository-name} For more information about the format of ARNs,
        see Amazon Resource Names (ARNs) and AWS Service Namespaces. For example, to specify the my-repo repository in the us-
        east-1 Region in your statement, use the following ARN: "Resource": "arn:aws:ecr:us-east-1:123456789012:repository/my-
        repo" To specify all repositories that belong to a specific account, use the wildcard (*): "Resource": "arn:aws:ecr:us-
        east-1:123456789012:repository/*" To specify multiple resources in a single statement, separate the ARNs with commas.
        "Resource": [ "resource1", "resource2" To see a list of Amazon ECR resource types and their ARNs, see Resources Defined
        by Amazon Elastic Container Registry in the IAM User Guide. To learn with which actions you can specify the ARN of each
        resource, see Actions Defined by Amazon Elastic Container Registry. Condition Keys

        The Condition element (or Condition block) lets you specify conditions in which a statement is in effect. The Condition
        element is optional. You can build conditional expressions that use condition operators, such as equals or less than, to
        match the condition in the policy with values in the request.               If you specify multiple Condition elements
        in a statement, or multiple keys in a single Condition element, AWS evaluates them using a logical AND operation. If you
        specify multiple values for a single condition key, AWS evaluates the condition using a logical OR operation. All of the
        conditions must be met before the statement's permissions are granted. You can also use placeholder variables when you
        specify conditions. For example, you can grant an IAM user permission to access a resource only if it is tagged with
        their IAM user name. For more information, see IAM Policy Elements: Variables and Tags in the IAM User Guide. Amazon ECR
        defines its own set of condition keys and also supports using some global condition keys. To see all AWS global
        condition keys, see AWS Global Condition Context Keys in the IAM User Guide. Most Amazon ECR actions support the
        aws:ResourceTag and ecr:ResourceTag condition keys. For more information, see Using Tag-Based Access Control. To see a
        list of Amazon ECR condition keys, see Condition Keys Defined by Amazon Elastic Container Registry in the IAM User
        Guide. To learn with which actions and resources you can use a condition key, see Actions Defined by Amazon Elastic
        Container Registry. Impact: Care should be taken not to remove access to Amazon ECR for accounts that require this for
        their operation.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.3":
    title: '5.1.3 | Minimize cluster access to read-only for Amazon ECR - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Configure the Cluster Service Account with Storage Object Viewer Role to only allow readonly access to Amazon ECR.
    remediation: |
        You can use your Amazon ECR images with Amazon EKS, but you need to satisfy the following prerequisites. The Amazon EKS
        worker node IAM role (NodeInstanceRole) that you use with your worker nodes must possess the following IAM policy
        permissions for Amazon ECR.
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "ecr:BatchCheckLayerAvailability",
                        "ecr:BatchGetImage",
                        "ecr:GetDownloadUrlForLayer",
                        "ecr:GetAuthorizationToken"
                    ],
                    "Resource": "*"
                }
            ]
        }
                       Impact: A separate dedicated service account may be required for use by build servers and other robot
        users pushing or managing container images.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.4":
    title: '5.1.4 | Minimize Container Registries to only those approved - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Use approved container registries.
    remediation: |
        To minimize AWS ECR container registries to only those approved, you can follow these steps: Define your approval
        criteria: Determine the criteria that containers must meet to be considered approved. This can include factors such as
        security, compliance, compatibility, and other requirements. Identify all existing ECR registries: Identify all ECR
        registries that are currently being used in your organization. Evaluate ECR registries against approval criteria:
        Evaluate each ECR registry against your approval criteria to determine whether it should be approved or not. This can be
        done by reviewing the registry settings and configuration, as well as conducting security assessments and vulnerability
        scans. Establish policies and procedures: Establish policies and procedures that outline how ECR registries will be
        approved, maintained, and monitored. This should include guidelines for developers to follow when selecting a registry
        for their container images. Implement access controls: Implement access controls to ensure that only approved ECR
        registries are used to store and distribute container images. This can be done by setting up IAM policies and roles that
        restrict access to unapproved registries or create a whitelist of approved registries. Monitor and review: Continuously
        monitor and review the use of ECR registries to ensure that they continue to meet your approval criteria. This can
        include regularly reviewing access logs, scanning for vulnerabilities, and conducting periodic audits. By following
        these steps, you can minimize AWS ECR container registries to only those approved, which can help to improve security,
        reduce complexity, and streamline container management in your organization. Additionally, AWS provides several tools
        and services that can help you manage your ECR registries, such as AWS Config, AWS CloudFormation, and AWS Identity and
        Access Management (IAM). Impact: All container images to be deployed to the cluster must be hosted within an approved
        container image registry.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.2.1":
    title: '5.2.1 | Prefer using dedicated EKS Service Accounts'
    section: 'Identity and Access Management (IAM)'
    description: |
        Kubernetes workloads should not use cluster node service accounts to authenticate to Amazon EKS APIs. Each Kubernetes
        workload that needs to authenticate to other AWS services using AWS IAM should be provisioned with a dedicated Service
        account.
    remediation: |
        With IAM roles for service accounts on Amazon EKS clusters, you can associate an IAM role with a Kubernetes service
        account. This service account can then provide AWS permissions to the containers in any pod that uses that service
        account. With this feature, you no longer need to provide extended permissions to the worker node IAM role so that pods
        on that node can call AWS APIs. Applications must sign their AWS API requests with AWS credentials. This feature
        provides a strategy for managing credentials for your applications, similar to the way that Amazon EC2 instance profiles
        provide credentials to Amazon EC2 instances. Instead of creating and distributing your AWS credentials to the containers
        or using the Amazon EC2 instance’s role, you can associate an IAM role with a Kubernetes service account. The
        applications in the pod’s containers can then use an AWS SDK or the AWS CLI to make API requests to authorized AWS
        services. The IAM roles for service accounts feature provides the following benefits: Least privilege — By using the IAM
        roles for service accounts feature, you no longer need to provide extended permissions to the worker node IAM role so
        that pods on that node can call AWS APIs. You can scope IAM permissions to a service account, and only pods that use
        that service account have access to those permissions. This feature also eliminates the need for third-party solutions
        such as kiam or kube2iam. Credential isolation — A container can only retrieve credentials for the IAM role that is
        associated with the service account to which it belongs. A container never has access to credentials that are intended
        for another container that belongs to another pod. Audit-ability — Access and event logging is available through
        CloudTrail to help ensure retrospective auditing. To get started, see list text hereEnabling IAM roles for service
        accounts on your cluster. For an end-to-end walkthrough using eksctl, see Walkthrough: Updating a DaemonSet to use IAM
        for service accounts.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.2.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.3.1":
    title: '5.3.1 | Ensure Kubernetes Secrets are encrypted using Customer Master Keys CMKs managed in AWS KMS - manual'
    section: 'AWS EKS Key Management Service'
    description: |
        Encrypt Kubernetes secrets, stored in etcd, using secrets encryption feature during Amazon EKS cluster creation.
    remediation: |
        This process can only be performed during Cluster Creation. Enable 'Secrets Encryption' during Amazon EKS cluster
        creation as described in the links within the 'References' section.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.3.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.4.1":
    title: '5.4.1 | Restrict Access to the Control Plane Endpoint'
    section: 'Cluster Networking'
    description: |
        Enable Endpoint Private Access to restrict access to the clusters control plane to only an allowlist of authorized IPs.
    remediation: |
        By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API
        server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or
        completely disable internet access to the API server. With this in mind, you can update your cluster accordingly using
        the AWS CLI to ensure that Private Endpoint Access is enabled. If you choose to also enable Public Endpoint Access then
        you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you
        specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP
        addresses by defaulting to ['0.0.0.0/0']. For example, the following command would enable private access to the
        Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix):
        aws eks update-cluster-config --region $AWS_REGION --name $CLUSTER_NAME --resources-vpc-config
        endpointPrivateAccess=true, endpointPrivateAccess=true, publicAccessCidrs="203.0.113.5/32" Note: The CIDR blocks
        specified cannot include reserved addresses.
        There is a maximum number of CIDR blocks that you can specify. For more information, see the EKS Service Quotas link in
        the references section.
        For more detailed information, see the EKS Cluster Endpoint documentation link in the references section. Impact: When
        implementing Endpoint Private Access, be careful to ensure all desired networks are on the allowlist (whitelist) to
        prevent inadvertently blocking external access to your cluster's control plane.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.2":
    title: '5.4.2 | Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled'
    section: 'Cluster Networking'
    description: |
        Disable access to the Kubernetes API from outside the node network if it is not required.
    remediation: |
        By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API
        server stays within your VPC. With this in mind, you can update your cluster accordingly using the AWS CLI to ensure
        that Private Endpoint Access is enabled. For example, the following command would enable private access to the
        Kubernetes API and ensure that no public access is permitted: aws eks update-cluster-config --region $AWS_REGION --name
        $CLUSTER_NAME --resources-vpc-config endpointPrivateAccess=true,endpointPublicAccess=false Note: For more detailed
        information, see the EKS Cluster Endpoint documentation link in the references section. Impact: Configure the EKS
        cluster endpoint to be private. Leave the cluster endpoint public and specify which CIDR blocks can communicate with the
        cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the
        cluster endpoint. Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to
        enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the
        kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when
        the control plane is provisioned.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.2', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.3":
    title: '5.4.3 | Ensure clusters are created with Private Nodes'
    section: 'Cluster Networking'
    description: |
        Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with
        no public IP addresses.
    remediation: |
        aws eks update-cluster-config \
            --region region-code \
            --name my-cluster \
            --resources-vpc-config endpointPublicAccess=true,publicAccessCidrs="203.0.113.5/32",endpointPrivateAccess=true
                       Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range and
        IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound
        Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.3', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.4":
    title: '5.4.4 | Ensure Network Policy is Enabled and set as appropriate'
    section: 'Cluster Networking'
    description: |
        Amazon EKS provides two ways to implement network policy. You choose a network policy option when you create an EKS
        cluster. The policy option cant be changed after the cluster is created
        Calico Network Policies, an opensource network and network security solution founded by Tigera.
        Both implementations use Linux IPTables to enforce the specified policies. Policies are translated into sets of allowed
        and disallowed IP pairs. These pairs are then programmed as IPTable filter rules.
    remediation: |
        Utilize Calico or other network policy engine to segment and isolate your traffic. Impact: Network Policy requires the
        Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an
        existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling
        update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block
        other operations on the cluster (including delete) until it has run to completion. Enabling Network Policy enforcement
        consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by
        approximately 128MB, and requires approximately 300 millicores of CPU.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.4', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.5":
    title: '5.4.5 | Encrypt traffic to HTTPS load balancers with TLS certificates - manual'
    section: 'Cluster Networking'
    description: |
        Encrypt traffic to HTTPS load balancers using TLS certificates.
    remediation: |
        Your load balancer vendor can provide details on configuring HTTPS with TLS.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.4.5', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.5.1":
    title: '5.5.1 | Manage Kubernetes RBAC users with AWS IAM Authenticator for Kubernetes or Upgrade to AWS CLI v1.16.156 or greater - manual'
    section: 'Authentication and Authorization'
    description: |
        Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM Authenticator for
        Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM Authenticator
        for Kubernetes and modifying your kubectl configuration file to use it for authentication.
    remediation: |
        Refer to the '                 Managing users or IAM roles for your cluster
        ' in Amazon EKS documentation.               Note: If using AWS CLI version 1.16.156 or later there is no need to
        install the AWS IAM Authenticator anymore. The relevant AWS CLI commands, depending on the use case, are:
        aws eks update-kubeconfig
        aws eks get-token
                       Impact: Users must now be assigned to the IAM group created to use this namespace and deploy
        applications.  If they are not they will not be able to access the namespace or deploy.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.5.1', 'cis_amazon_elastic_kubernetes_service_(eks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
