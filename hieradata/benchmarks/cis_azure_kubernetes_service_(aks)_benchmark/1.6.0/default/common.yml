---

inspec_rules:
  "2.1.1":
    title: '2.1.1 | Enable audit Logs - manual'
    section: 'Logging'
    description: |
        With Azure Kubernetes Service AKS, the control plane components such as the kubeapiserver and kubecontrollermanager are
        provided as a managed service. You create and manage the nodes that run the kubelet and container runtime, and deploy
        your applications through the managed Kubernetes API server. To help troubleshoot your application and services, you may
        need to view the logs generated by these control plane components. To help collect and review data from multiple
        sources, Azure Monitor logs provides a query language and analytics engine that provides insights to your environment. A
        workspace is used to collate and analyze the data, and can integrate with other Azure services such as Application
        Insights and Security Center.
    remediation: |
        Azure audit logs are enabled and managed in the Azure portal. To enable log collection for the Kubernetes master
        components in your AKS cluster, open the Azure portal in a web browser and complete the following steps: Select the
        resource group for your AKS cluster, such as myResourceGroup. Don't select the resource group that contains your
        individual AKS cluster resources, such as MC_myResourceGroup_myAKSCluster_eastus. On the left-hand side, choose
        Diagnostic settings. Select your AKS cluster, such as myAKSCluster, then choose to Add diagnostic setting. Enter a name,
        such as myAKSClusterLogs, then select the option to Send to Log Analytics. Select an existing workspace or create a new
        one. If you create a workspace, provide a workspace name, a resource group, and a location. In the list of available
        logs, select the logs you wish to enable. For this example, enable the kube-audit and kube-audit-admin logs. Common logs
        include the kube-apiserver, kube-controller-manager, and kube-scheduler. You can return and change the collected logs
        once Log Analytics workspaces are enabled. When ready, select Save to enable collection of the selected logs. Impact:
        What is collected from Kubernetes clusters
        Container insights includes a predefined set of metrics and inventory items collected that are written as log data in
        your Log Analytics workspace. All metrics listed below are collected by default every one minute. Node metrics collected
        The following list is the 24 metrics per node that are collected: cpuUsageNanoCores
        cpuCapacityNanoCores
        cpuAllocatableNanoCores
        memoryRssBytes
        memoryWorkingSetBytes
        memoryCapacityBytes
        memoryAllocatableBytes
        restartTimeEpoch
        used (disk)
        free (disk)
        used_percent (disk)
        io_time (diskio)
        writes (diskio)
        reads (diskio)
        write_bytes (diskio)
        write_time (diskio)
        iops_in_progress (diskio)
        read_bytes (diskio)
        read_time (diskio)
        err_in (net)
        err_out (net)
        bytes_recv (net)
        bytes_sent (net)
        Kubelet_docker_operations (kubelet)
        Container metrics
        The following list is the eight metrics per container collected: cpuUsageNanoCores
        cpuRequestNanoCores
        cpuLimitNanoCores
        memoryRssBytes
        memoryWorkingSetBytes
        memoryRequestBytes
        memoryLimitBytes
        restartTimeEpoch
        Cluster inventory
        The following list is the cluster inventory data collected by default: KubePodInventory – 1 per minute per container
        KubeNodeInventory – 1 per node per minute
        KubeServices – 1 per service per minute
        ContainerInventory – 1 per container per minute
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_2.1.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "3.1.1":
    title: '3.1.1 | Ensure that the kubeconfig file permissions are set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubelet
         is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644
        or more restrictive.
    remediation: |
        Run the below command (based on the file location on your system) on the each worker
        node. For example, chmod 644 <kubeconfig file>
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.2":
    title: '3.1.2 | Ensure that the kubelet kubeconfig file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubelet
         is running, ensure that the file ownership of its kubeconfig file is set to              rootroot
        .
    remediation: |
        Run the below command (based on the file location on your system) on each worker node. For example, chown root:root
        <proxy kubeconfig file>
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == root:root'

  "3.1.3":
    title: '3.1.3 | Ensure that the azure.json file has permissions set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        The azure.json file in an Azure Kubernetes Service AKS cluster is a configuration file used by the Kubernetes cloud
        provider integration for Azure. This file contains essential details that allow the Kubernetes cluster to interact with
        Azure resources effectively. Its part of the Azure Cloud Provider configuration, enabling Kubernetes components to
        communicate with Azure services for features like load balancers, storage, and networking. Ensure the file has
        permissions of 644 or more restrictive.
    remediation: |
        Run the following command (using the config file location identified in the Audit step) chmod 644
        /etc/kubernetes/azure.json
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.4":
    title: '3.1.4 | Ensure that the azure.json file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        The azure.json file in an Azure Kubernetes Service AKS cluster is a configuration file used by the Kubernetes cloud
        provider integration for Azure. This file contains essential details that allow the Kubernetes cluster to interact with
        Azure resources effectively. Its part of the Azure Cloud Provider configuration, enabling Kubernetes components to
        communicate with Azure services for features like load balancers, storage, and networking. Ensure that the file is owned
        by rootroot.
    remediation: |
        Run the following command (using the config file location identified in the Audit step) chown root:root
        /etc/kubernetes/azure.json
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.2.1":
    title: '3.2.1 | Ensure that the --anonymous-auth argument is set to false - manual'
    section: 'Kubelet'
    description: |
        Disable anonymous requests to the Kubelet server.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to false               "anonymous": "enabled": false
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --anonymous-auth=false
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "authentication.*anonymous":{"enabled":false}"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Anonymous requests will be rejected.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*anonymous":{"enabled":false}'

  "3.2.2":
    title: '3.2.2 | Ensure that the --authorization-mode argument is not set to AlwaysAllow - manual'
    section: 'Kubelet'
    description: |
        Do not allow all requests. Enable explicit authorization.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to false               "authentication"... "webhook":{"enabled":true
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --authorization-mode=Webhook
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "authentication.*webhook":{"enabled":true"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Unauthorized requests will be denied.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "authentication.*webhook":{"enabled":true'

  "3.2.3":
    title: '3.2.3 | Ensure that the --client-ca-file argument is set as appropriate - manual'
    section: 'Kubelet'
    description: |
        Enable Kubelet authentication using certificates.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to false               "authentication": { "x509": {"clientCAFile:" to the location of the
        client CA file.
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --client-ca-file=<path/to/client-ca-file>
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "authentication.*x509":("clientCAFile":"/etc/kubernetes/pki/ca.crt"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: You require TLS to be configured on apiserver as well as kubelets.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*clientCAFile":".[a-zA-Z]'

  "3.2.4":
    title: '3.2.4 | Ensure that the --read-only-port is secured'
    section: 'Kubelet'
    description: |
        Disable the readonly port.
    remediation: |
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to false               readOnlyPort to 0

        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --read-only-port=0

        For all remediations:
        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Removal of the read-only port will require that any service which made use of it will need to be
        re-configured to use the main Kubelet API.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "3.2.5":
    title: '3.2.5 | Ensure that the --streaming-connection-idle-timeout argument is not set to 0 - manual'
    section: 'Kubelet'
    description: |
        Do not disable timeouts on streaming connections.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to a non-zero value in the format of #h#m#s               "streamingConnectionIdleTimeout":
        "4h0m0s"

        You should ensure that the kubelet service file                  /etc/systemd/system/kubelet.service.d/10-kubelet-
        args.conf
         does not specify a                  --streaming-connection-idle-timeout
         argument because it would override the Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --streaming-connection-idle-timeout=4h0m0s
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "streamingConnectionIdleTimeout":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Long-lived connections could be interrupted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.5', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == streamingConnectionIdleTimeout":"4h0m0s'

  "3.2.6":
    title: '3.2.6 | Ensure that the --make-iptables-util-chains argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Allow Kubelet to manage iptables.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to false               "makeIPTablesUtilChains": true
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --make-iptables-util-chains:true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of                  "makeIPTablesUtilChains": true
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other
        iptables management solution, then there might be some conflicts.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.6', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == makeIPTablesUtilChains":true'

  "3.2.7":
    title: '3.2.7 | Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture - manual'
    section: 'Kubelet'
    description: |
        Security relevant information should be captured.  The              eventRecordQPS
         flag on the Kubelet can be used to limit the rate at which events are gathered.  Setting this too low could result in
        relevant events not being logged, however the unlimited setting of              0
         could result in a denial of service on the kubelet.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to 5 or a value greater or equal to 0               "eventRecordQPS": 5

        Check that                  /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         does not define an executable argument for                  eventRecordQPS
         because this would override your Kubelet config.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --eventRecordQPS=5
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of                  "eventRecordQPS"
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact:
        Setting this parameter to                    0
         could result in a denial of service condition due to excessive events being created.  The cluster's event processing
        and storage systems should be scaled to handle expected event loads.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_3.2.7', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "eventRecordQPS":[0-9]\d*'

  "3.2.8":
    title: '3.2.8 | Ensure that the --rotate-certificates argument is not set to false - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet client certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true               "RotateCertificate":true
         Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not
        set the --RotateCertificate executable argument to false because this would override the Kubelet config file.
        Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --RotateCertificate=true
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_3.2.8', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "rotateCertificates":true'

  "3.2.9":
    title: '3.2.9 | Ensure that the RotateKubeletServerCertificate argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet server certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true               "RotateKubeletServerCertificate":true
         Remediation Method 2:
        If using a Kubelet config file, edit the file to set                  RotateKubeletServerCertificate to true
        .
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --rotate-kubelet-server-certificate=true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "RotateKubeletServerCertificate":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.9', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == RotateKubeletServerCertificate":true'

  "4.1.1":
    title: '4.1.1 | Ensure that the cluster-admin role is only used where required'
    section: 'RBAC and Service Accounts'
    description: |
        The RBAC role              clusteradmin
         provides wideranging powers over the environment and should be used only where and when needed.
    remediation: |
        Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they
        could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the
        clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
         Impact:
        Care should be taken before removing any                    clusterrolebindings
         from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should
        not be made to                    clusterrolebindings
         with the                    system:
         prefix as they are required for the operation of system components.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.2":
    title: '4.1.2 | Minimize access to secrets'
    section: 'RBAC and Service Accounts'
    description: |
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by
        workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to
        reduce the risk of privilege escalation.
    remediation: |
        Where possible, remove                  get
        ,                  list
         and                  watch
         access to                  secret
         objects in the cluster.               Impact: Care should be taken not to remove access to secrets to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.3":
    title: '4.1.3 | Minimize wildcard use in Roles and ClusterRoles'
    section: 'RBAC and Service Accounts'
    description: |
        Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on
        those objects.  It is possible to set either of these to be the wildcard  which matches all items. Use of wildcards is
        not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are
        added to the Kubernetes API either as CRDs or in later versions of the product.
    remediation: |
        Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.4":
    title: '4.1.4 | Minimize access to create pods'
    section: 'RBAC and Service Accounts'
    description: |
        The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as
        assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data unless Pod
        Security Policies are implemented to restrict this access As such, access to create new pods should be restricted to the
        smallest possible group of users.
    remediation: |
        Where possible, remove                  create
         access to                  pod
         objects in the cluster.               Impact: Care should be taken not to remove access to pods to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.5":
    title: '4.1.5 | Ensure that default service accounts are not actively used'
    section: 'RBAC and Service Accounts'
    description: |
        The              default
         service account should not be used to ensure that rights granted to applications can be more easily audited and
        reviewed.
    remediation: |
        Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.
        Modify the configuration of each default service account to include this value automountServiceAccountToken: false
         Automatic remediation for the default account: kubectl patch serviceaccount default -p $'automountServiceAccountToken:
        false' Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be
        created.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.5', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.6":
    title: '4.1.6 | Ensure that Service Account Tokens are only mounted where necessary'
    section: 'RBAC and Service Accounts'
    description: |
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to
        communicate with the API server
    remediation: |
        Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
        Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where
        the resource is available to unauthenticated principals.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.6', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.2.1":
    title: '4.2.1 | Minimize the admission of privileged containers'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              securityContext.privileged
         flag set to              true
        .
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged
        containers. To enable PSA for a namespace in your cluster, set the pod-security.kubernetes.io/enforce label with the
        policy value you want to enforce. kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
        The above command enforces the restricted policy for the NAMESPACE namespace. You can also enable Pod Security Admission
        for all your namespaces. For example: kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline Pod
        Security Policies and Assignments can be found by searching for Policies in the Azure Portal.  A detailed step-by-step
        guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        Impact:
        Pods defined with                    spec.containers[].securityContext.privileged: true
        ,                    spec.initContainers[].securityContext.privileged: true
         and                    spec.ephemeralContainers[].securityContext.privileged: true
         will not be permitted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.2":
    title: '4.2.2 | Minimize the admission of containers wishing to share the host process ID namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostPID
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostPID
         containers.               Pod Security Policies and Assignments can be found by searching for Policies in the Azure
        Portal.  A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-
        us/azure/governance/policy/concepts/policy-for-kubernetes Impact:
        Pods defined with                    spec.hostPID: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.3":
    title: '4.2.3 | Minimize the admission of containers wishing to share the host IPC namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostIPC
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostIPC
         containers.               Pod Security Policies and Assignments can be found by searching for Policies in the Azure
        Portal.  A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-
        us/azure/governance/policy/concepts/policy-for-kubernetes Impact:
        Pods defined with                    spec.hostIPC: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.4":
    title: '4.2.4 | Minimize the admission of containers wishing to share the host network namespace'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              hostNetwork
         flag set to true.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of
        hostNetwork
         containers.               Pod Security Policies and Assignments can be found by searching for Policies in the Azure
        Portal.  A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-
        us/azure/governance/policy/concepts/policy-for-kubernetes Impact:
        Pods defined with                    spec.hostNetwork: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.2.5":
    title: '4.2.5 | Minimize the admission of containers with allowPrivilegeEscalation'
    section: 'Pod Security Standards'
    description: |
        Do not generally permit containers to be run with the              allowPrivilegeEscalation
         flag set to              true
        . Allowing this right can lead to a process running a container getting more rights than it started with.           Its
        important to note that these rights are still constrained by the overall container sandbox, and this setting does not
        relate to the use of privileged containers.
    remediation: |
        Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with
        .spec.allowPrivilegeEscalation
         set to                  true
        .               Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal.  A
        detailed step-by-step guide can be found here: https://learn.microsoft.com/en-
        us/azure/governance/policy/concepts/policy-for-kubernetes Impact:
        Pods defined with                    spec.allowPrivilegeEscalation: true
         will not be permitted unless they are run under a specific policy.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.2.5', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.1":
    title: '4.4.1 | Ensure latest CNI version is used'
    section: 'CNI Plugin'
    description: |
        There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may
        not be possible to effectively restrict traffic in the cluster.
    remediation: |
        As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny
        all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.
        Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.4.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.2":
    title: '4.4.2 | Ensure that all Namespaces have Network Policies defined'
    section: 'CNI Plugin'
    description: |
        Use network policies to isolate traffic in your cluster network.
    remediation: |
        Follow the documentation and create                  NetworkPolicy
         objects as you need them.               Impact: Once there is any Network Policy in a namespace selecting a particular
        pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that
        are not selected by any Network Policy will continue to accept all traffic"
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.4.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.5.1":
    title: '4.5.1 | Prefer using secrets as files over secrets as environment variables'
    section: 'Secrets Management'
    description: |
        Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment
        variable secrets.
    remediation: |
        If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.
        Impact: Application code which expects to read secrets in the form of environment variables would need modification
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.5.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "4.5.2":
    title: '4.5.2 | Consider external secret storage - manual'
    section: 'Secrets Management'
    description: |
        Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if
        you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has
        auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.
    remediation: |
        Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.
        Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.5.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.1":
    title: '4.6.1 | Create administrative boundaries between resources using namespaces - manual'
    section: 'General Policies'
    description: |
        Use namespaces to isolate your Kubernetes objects.
    remediation: |
        Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to
        switch between namespaces for administration.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.6.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.2":
    title: '4.6.2 | Apply Security Context to Your Pods and Containers - manual'
    section: 'General Policies'
    description: |
        Apply Security Context to Your Pods and Containers
    remediation: |
        As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular
        namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we
        recommend implementing a more restrictive policy such as this:
        apiVersion: policy/v1beta1
        kind: PodSecurityPolicy
        metadata:
            name: restricted
            annotations:
            seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
            apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
            seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
            apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
        spec:
            privileged: false
            # Required to prevent escalations to root.
            allowPrivilegeEscalation: false
            # This is redundant with non-root + disallow privilege escalation,
            # but we can provide it for defense in depth.
            requiredDropCapabilities:
            - ALL
            # Allow core volume types.
            volumes:
            - 'configMap'
            - 'emptyDir'
            - 'projected'
            - 'secret'
            - 'downwardAPI'
            # Assume that persistentVolumes set up by the cluster admin are safe to use.
            - 'persistentVolumeClaim'
            hostNetwork: false
            hostIPC: false
            hostPID: false
            runAsUser:
            # Require the container to run without root privileges.
            rule: 'MustRunAsNonRoot'
            seLinux:
            # This policy assumes the nodes are using AppArmor rather than SELinux.
            rule: 'RunAsAny'
            supplementalGroups:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            fsGroup:
            rule: 'MustRunAs'
            ranges:
                # Forbid adding the root group.
                - min: 1
                max: 65535
            readOnlyRootFilesystem: false
                       This policy prevents pods from running as privileged or escalating privileges. It also restricts the
        types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar,
        approach is to start with policy that locks everything down and incrementally add exceptions for applications that need
        looser restrictions such as logging agents which need the ability to mount a host path. Impact: If you incorrectly apply
        security contexts, you may have trouble running the pods.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.6.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.3":
    title: '4.6.3 | The default namespace should not be used'
    section: 'General Policies'
    description: |
        Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing
        objects in this namespace makes application of RBAC and other controls more difficult.
    remediation: |
        Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new
        resources are created in a specific namespace. Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.6.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.1.1":
    title: '5.1.1 | Ensure Image Vulnerability Scanning using Microsoft Defender for Cloud MDC image scanning or a third party provider'
    section: 'Image Registry and Image Scanning'
    description: |
        Scan images being deployed to Azure AKS for vulnerabilities. Vulnerability scanning for images stored in Microsoft
        Defender for Cloud MDC. This capability is powered by Microsoft Defender for Endpoints MDVM, a leading provider of
        information security. When you push an image to Container Registry, MDC automatically scans it, then checks for known
        vulnerabilities in packages or dependencies defined in the file. When the scan completes after about 10 minutes, MDC
        provides details and a security classification for each vulnerability detected, along with guidance on how to remediate
        issues and protect vulnerable attack surfaces.
    remediation: |
        Enable MDC for Container Registries: If you find that container registries is not enabled and you wish to enable it, you
        can do so using the following command: az security pricing create --name ContainerRegistry --tier Standard or az
        resource update --ids /subscriptions/{subscription-id}/resourceGroups/{resource-group-
        name}/providers/Microsoft.ContainerRegistry/registries/{registry-name} --set properties.enabled=true replacing
        subscription-id, resource-group-name and registry-name with the appropriate values. Please note, enabling MDC for
        container registries incurs additional costs, so be sure to review the pricing details on the official Azure
        documentation before enabling it. Impact: When using an MDC, you might occasionally encounter problems. For example, you
        might not be able to pull a container image because of an issue with Docker in your local environment. Or, a network
        issue might prevent you from connecting to the registry.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.1.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.1.2":
    title: '5.1.2 | Minimize user access to Azure Container Registry ACR - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Restrict user access to Azure Container Registry ACR, limiting interaction with build images to only authorized
        personnel and service accounts.
    remediation: |
        Azure Container Registry
        If you use Azure Container Registry (ACR) as your container image store, you need to grant permissions to the service
        principal for your AKS cluster to read and pull images. Currently, the recommended configuration is to use the az aks
        create or az aks update command to integrate with a registry and assign the appropriate role for the service principal.
        For detailed steps, see Authenticate with Azure Container Registry from Azure Kubernetes Service. To avoid needing an
        Owner or Azure account administrator role, you can configure a service principal manually or use an existing service
        principal to authenticate ACR from AKS. For more information, see ACR authentication with service principals or
        Authenticate from Kubernetes with a pull secret. Impact: Care should be taken not to remove access to Azure ACR for
        accounts that require this for their operation.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.3":
    title: '5.1.3 | Minimize cluster access to read-only for Azure Container Registry ACR - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Configure the Cluster Service Account with Storage Object Viewer Role to only allow readonly access to Azure Container
        Registry ACR
    remediation: |
        Impact: A separate dedicated service account may be required for use by build servers and other robot users pushing or
        managing container images.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.1.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.4":
    title: '5.1.4 | Minimize Container Registries to only those approved - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Use approved container registries.
    remediation: |
        If you are using Azure Container Registry you have this option:
                         https://docs.microsoft.com/en-us/azure/container-registry/container-registry-firewall-access-rules
                              For other non-AKS repos using admission controllers or Azure Policy will also work.
        Limiting or locking down egress traffic is also recommended:
                         https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic
                              Impact: All container images to be deployed to the cluster must be hosted within an approved
        container image registry.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.2.1":
    title: '5.2.1 | Prefer using dedicated AKS Service Accounts - manual'
    section: 'Access and identity options for Azure Kubernetes Service (AKS)'
    description: |
        Kubernetes workloads should not use cluster node service accounts to authenticate to Azure AKS APIs. Each Kubernetes
        workload that needs to authenticate to other Azure Web Services using IAM should be provisioned with a dedicated Service
        account.
    remediation: |
        Azure Active Directory integration
        The security of AKS clusters can be enhanced with the integration of Azure Active Directory (AD). Built on decades of
        enterprise identity management, Azure AD is a multi-tenant, cloud-based directory, and identity management service that
        combines core directory services, application access management, and identity protection. With Azure AD, you can
        integrate on-premises identities into AKS clusters to provide a single source for account management and security. Azure
        Active Directory integration with AKS clusters With Azure AD-integrated AKS clusters, you can grant users or groups
        access to Kubernetes resources within a namespace or across the cluster. To obtain a kubectl configuration context, a
        user can run the az aks get-credentials command. When a user then interacts with the AKS cluster with kubectl, they're
        prompted to sign in with their Azure AD credentials. This approach provides a single source for user account management
        and password credentials. The user can only access the resources as defined by the cluster administrator. Azure AD
        authentication is provided to AKS clusters with OpenID Connect. OpenID Connect is an identity layer built on top of the
        OAuth 2.0 protocol. For more information on OpenID Connect, see the Open ID connect documentation. From inside of the
        Kubernetes cluster, Webhook Token Authentication is used to verify authentication tokens. Webhook token authentication
        is configured and managed as part of the AKS cluster.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.2.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.3.1":
    title: '5.3.1 | Ensure Kubernetes Secrets are encrypted - manual'
    section: 'Key Management Service (KMS)'
    description: |
        Encryption at Rest is a common security requirement. In Azure, organizations can encrypt data at rest without the risk
        or cost of a custom key management solution. Organizations have the option of letting Azure completely manage Encryption
        at Rest. Additionally, organizations have various options to closely manage encryption or encryption keys.
    remediation: |
        
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_5.3.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.4.1":
    title: '5.4.1 | Restrict Access to the Control Plane Endpoint'
    section: 'Cluster Networking'
    description: |
        Enable Endpoint Private Access to restrict access to the clusters control plane to only an allowlist of authorized IPs.
    remediation: |
        By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API
        server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or
        completely disable internet access to the API server. With this in mind, you can update your cluster accordingly using
        the AKS CLI to ensure that Private Endpoint Access is enabled. If you choose to also enable Public Endpoint Access then
        you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you
        specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP
        addresses by defaulting to ['0.0.0.0/0']. For example, the following command would enable private access to the
        Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix):
        Impact: When implementing Endpoint Private Access, be careful to ensure all desired networks are on the allowlist
        (whitelist) to prevent inadvertently blocking external access to your cluster's control plane. Limitations
        IP authorized ranges can't be applied to the private api server endpoint, they only apply to the public API server
        Availability Zones are currently supported for certain regions.
        Azure Private Link service limitations apply to private clusters.
        No support for Azure DevOps Microsoft-hosted Agents with private clusters. Consider to use Self-hosted Agents.
        For customers that need to enable Azure Container Registry to work with private AKS, the Container Registry virtual
        network must be peered with the agent cluster virtual network.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.2":
    title: '5.4.2 | Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled'
    section: 'Cluster Networking'
    description: |
        Disable access to the Kubernetes API from outside the node network if it is not required.
    remediation: |
        To use a private endpoint, create a new private endpoint in your virtual network then create a link between your virtual
        network and a new private DNS zone
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.3":
    title: '5.4.3 | Ensure clusters are created with Private Nodes'
    section: 'Cluster Networking'
    description: |
        Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with
        no public IP addresses.
    remediation: |
        az aks create \
        --resource-group <private-cluster-resource-group> \
        --name <private-cluster-name> \
        --load-balancer-sku standard \
        --enable-private-cluster \
        --network-plugin azure \
        --vnet-subnet-id <subnet-id> \
        --docker-bridge-address \
        --dns-service-ip \
        --service-cidr

        Where                  --enable-private-cluster
         is a mandatory flag for a private cluster.               Impact: To enable Private Nodes, the cluster has to also be
        configured with a private master IP range and IP Aliasing enabled. Private Nodes do not have outbound access to the
        public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you
        can manage your own NAT gateway.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.3', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.4":
    title: '5.4.4 | Ensure Network Policy is Enabled and set as appropriate'
    section: 'Cluster Networking'
    description: |
        When you run modern, microservicesbased applications in Kubernetes, you often want to control which components can
        communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in
        an Azure Kubernetes Service AKS cluster. Lets say you likely want to block traffic directly to backend applications. The
        Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster.
    remediation: |
        Utilize Calico or other network policy engine to segment and isolate your traffic. Impact: Network Policy requires the
        Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an
        existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling
        update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block
        other operations on the cluster (including delete) until it has run to completion.
        If Network Policy is used, a cluster must have at least 2 nodes of type                    n1-standard-1
         or higher. The recommended minimum size cluster to run Network Policy enforcement is 3                    n1-standard-1
         instances.
        Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory
        footprint of the                    kube-system
         process by approximately 128MB, and requires approximately 300 millicores of CPU.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.4.4', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.4.5":
    title: '5.4.5 | Encrypt traffic to HTTPS load balancers with TLS certificates - manual'
    section: 'Cluster Networking'
    description: |
        Encrypt traffic to HTTPS load balancers using TLS certificates.
    remediation: |
        
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.4.5', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.5.1":
    title: '5.5.1 | Manage Kubernetes RBAC users with Azure AD - manual'
    section: 'Authentication and Authorization'
    description: |
        Azure Kubernetes Service AKS can be configured to use Azure Active Directory AD for user authentication. In this
        configuration, you sign in to an AKS cluster using an Azure AD authentication token. You can also configure Kubernetes
        rolebased access control Kubernetes RBAC to limit access to cluster resources based a users identity or group
        membership.
    remediation: |
        
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.5.1', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.5.2":
    title: '5.5.2 | Use Azure RBAC for Kubernetes Authorization - manual'
    section: 'Authentication and Authorization'
    description: |
        The ability to manage RBAC for Kubernetes resources from Azure gives you the choice to manage RBAC for the cluster
        resources either using Azure or native Kubernetes mechanisms. When enabled, Azure AD principals will be validated
        exclusively by Azure RBAC while regular Kubernetes users and service accounts are exclusively validated by Kubernetes
        RBAC. Azure rolebased access control RBAC is an authorization system built on Azure Resource Manager that provides
        finegrained access management of Azure resources. With Azure RBAC, you create a role definition that outlines the
        permissions to be applied. You then assign a user or group this role definition via a role assignment for a particular
        scope. The scope can be an individual resource, a resource group, or across the subscription.
    remediation: |
        
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.5.2', 'cis_azure_kubernetes_service_(aks)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
