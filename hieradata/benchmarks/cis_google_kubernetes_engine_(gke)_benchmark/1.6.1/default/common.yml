---

inspec_rules:
  "2.1.1":
    title: '2.1.1 | Client certificate authentication should not be used for users'
    section: 'Authentication and Authorization'
    description: |
        Kubernetes provides the option to use client certificates for user authentication.  However as there is no way to revoke
        these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose.
        It is not possible to fully disable client certificate use within a cluster as it is used for component to component
        authentication.
    remediation: |
        Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client
        certificates. You can remediate the availability of client certificates in your GKE cluster. See Recommendation 5.8.1.
        Impact: External mechanisms for authentication generally require additional software to be deployed.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_2.1.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.1.1":
    title: '3.1.1 | Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubeproxy
         is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644
        or more restrictive.
    remediation: |
        Run the below command (based on the file location on your system) on the each worker
        node. For example, chmod 644 <proxy kubeconfig file>
         Impact: Overly permissive file permissions increase security risk to the platform.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.2":
    title: '3.1.2 | Ensure that the proxy kubeconfig file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        If              kubeproxy
         is running, ensure that the file ownership of its kubeconfig file is set to              rootroot
        .
    remediation: |
        Run the below command (based on the file location on your system) on each worker node. For example, chown root:root
        <proxy kubeconfig file>
         Impact: Overly permissive file access increases the security risk to the platform.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.1.3":
    title: '3.1.3 | Ensure that the kubelet configuration file has permissions set to 600'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet configuration file exists, it has permissions of 600.
    remediation: |
        Run the following command (using the kubelet config file location): chmod 600 <kubelet_config_file>
         Impact: Overly permissive file access increases the security risk to the platform.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "3.1.4":
    title: '3.1.4 | Ensure that the kubelet configuration file ownership is set to rootroot'
    section: 'Worker Node Configuration Files'
    description: |
        Ensure that if the kubelet configuration file exists, it is owned by rootroot.
    remediation: |
        Run the following command (using the config file location identified in the Audit step): chown root:root
        <kubelet_config_file>
         Impact: Overly permissive file access increases the security risk to the platform.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.1.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "3.2.1":
    title: '3.2.1 | Ensure that the Anonymous Auth is Not Enabled Draft - manual'
    section: 'Kubelet'
    description: |
        Disable anonymous requests to the Kubelet server.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Disable Anonymous Authentication by setting the following parameter: "authentication": { "anonymous": { "enabled":
        false } }
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:               --anonymous-auth=false
         For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Anonymous requests will be rejected.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*anonymous.*{.enabled.:false}'

  "3.2.2":
    title: '3.2.2 | Ensure that the --authorization-mode argument is not set to AlwaysAllow - manual'
    section: 'Kubelet'
    description: |
        Do not allow all requests. Enable explicit authorization.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Enable Webhook Authentication by setting the following parameter: "authentication": { "webhook": { "enabled": true } }

        Next, set the Authorization Mode to                  Webhook
         by setting the following parameter:               "authorization": { "mode": "Webhook }

        Finer detail of the                  authentication
         and                  authorization
         fields can be found in the                  Kubelet Configuration documentation
        .               Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:
        --authentication-token-webhook
        --authorization-mode=Webhook
                       For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Unauthorized requests will be denied.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*"webhook":{"enabled":true'

  "3.2.3":
    title: '3.2.3 | Ensure that a Client CA File is Configured - manual'
    section: 'Kubelet'
    description: |
        Enable Kubelet authentication using certificates.
    remediation: |
        Remediation Method 1: If configuring via the Kubelet config file, you first need to locate the file. To do this, SSH to
        each node and execute the following command to find the kubelet process: ps -ef | grep kubelet

        The output of the above command provides details of the active kubelet process, from which we can see the location of
        the configuration file provided to the kubelet service with the                  --config
         argument. The file can be viewed with a command such as                  more
         or                  less
        , like so:               sudo less /path/to/kubelet-config.json
         Configure the client certificate authority file by setting the following parameter appropriately: "authentication": {
        "x509": {"clientCAFile": <path/to/client-ca-file> } }"
         Remediation Method 2:
        If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are
        part of the                  KUBELET_ARGS
         variable string.
        For systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
        . Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager
        is configured:               --client-ca-file=<path/to/client-ca-file>
         For Both Remediation Steps:
        Based on your system, restart the                  kubelet
         service and check the service status.
        The following example is for operating systems using                  systemd
        , such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the                  systemctl
         command. If                  systemctl
         is not available then you will need to look up documentation for your chosen operating system to determine which
        service manager is configured:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: You require TLS to be configured on apiserver as well as kubelets.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == authentication.*clientCAFile":".[a-zA-Z]'

  "3.2.4":
    title: '3.2.4 | Ensure that the --read-only-port is disabled'
    section: 'Kubelet'
    description: |
        Disable the readonly port.
    remediation: |
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to 0               "readOnlyPort": 0

        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --read-only-port=0

        For each remediation:
        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Removal of the read-only port will require that any service which made use of it will need to be
        re-configured to use the main Kubelet API.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "3.2.5":
    title: '3.2.5 | Ensure that the --streaming-connection-idle-timeout argument is not set to 0 - manual'
    section: 'Kubelet'
    description: |
        Do not disable timeouts on streaming connections.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file                  /etc/kubernetes/kubelet-
        config.yaml
         and set the below parameter to a non-zero value in the format of #h#m#s               "streamingConnectionIdleTimeout":
        "4h0m0s"

        You should ensure that the kubelet service file                  /etc/systemd/system/kubelet.service.d/10-kubelet-
        args.conf
         does not specify a                  --streaming-connection-idle-timeout
         argument because it would override the Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --streaming-connection-idle-timeout=4h0m0s
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "streamingConnectionIdleTimeout":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Long-lived connections could be interrupted.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.5', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == streamingConnectionIdleTimeout":"[1-9]'

  "3.2.6":
    title: '3.2.6 | Ensure that the --make-iptables-util-chains argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Allow Kubelet to manage iptables.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file
        /etc/kubernetes/kubelet/kubelet-config.json
         and set the below parameter to true               "makeIPTablesUtilChains": true

        Ensure that                  /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         does not set the                  --make-iptables-util-chains
         argument because that would override your Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --make-iptables-util-chains:true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of                  "makeIPTablesUtilChains.: true
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediations:

        Based on your system, restart the                  kubelet
         service and check status
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other
        iptables management solution, then there might be some conflicts.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.6', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == makeIPTablesUtilChains.:true'

  "3.2.7":
    title: '3.2.7 | Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture - manual'
    section: 'Kubelet'
    description: |
        Security relevant information should be captured.  The eventRecordQPS on the Kubelet configuration can be used to limit
        the rate at which events are gathered and sets the maximum event creations per second.  Setting this too low could
        result in relevant events not being logged, however the unlimited setting of              0
         could result in a denial of service on the kubelet.
    remediation: |
        If using a Kubelet config file, edit the file to set                  eventRecordQPS:
         to an appropriate level.
        If using command line arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
         on each worker node and set the below parameter in                  KUBELET_SYSTEM_PODS_ARGS
         variable.
        Based on your system, restart the                  kubelet
         service. For example:
        systemctl daemon-reload
        systemctl restart kubelet.service
                       Impact:
        Setting this parameter to                    0
         could result in a denial of service condition due to excessive events being created.  The cluster's event processing
        and storage systems should be scaled to handle expected event loads.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.7', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "eventRecordQPS":[1-9]\d*'

  "3.2.8":
    title: '3.2.8 | Ensure that the --rotate-certificates argument is not present or is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet client certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.yaml file
        /etc/kubernetes/kubelet/kubelet-config.yaml
         and set the below parameter to true               "RotateCertificate":true
         Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not
        set the --RotateCertificate executable argument to false because this would override the Kubelet config file.
        Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --RotateCertificate=true
         Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.8', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == "rotateCertificates":true'

  "3.2.9":
    title: '3.2.9 | Ensure that the RotateKubeletServerCertificate argument is set to true - manual'
    section: 'Kubelet'
    description: |
        Enable kubelet server certificate rotation.
    remediation: |
        Remediation Method 1:
        If modifying the Kubelet config file, edit the kubelet-config.json file                  /etc/kubernetes/kubelet-
        config.yaml
         and set the below parameter to true
        "featureGates": {
          "RotateKubeletServerCertificate":true
        },

        Additionally, ensure that the kubelet service file                  /etc/systemd/system/kubelet.service.d/10-kubelet-
        args.conf
         does not set the                  --rotate-kubelet-server-certificate
         executable argument to false because this would override the Kubelet config file.               Remediation Method 2:
        If using executable arguments, edit the kubelet service file
        /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf
         on each worker node and add the below parameter at the end of the                  KUBELET_ARGS
         variable string.               --rotate-kubelet-server-certificate=true
         Remediation Method 3:
        If using the api configz endpoint consider searching for the status of
        "RotateKubeletServerCertificate":
         by extracting the live configuration from the nodes running kubelet.
        **See detailed step-by-step configmap procedures in                  Reconfigure a Node's Kubelet in a Live Cluster
        , and then rerun the curl statement from audit process to check for kubelet configuration changes
        kubectl proxy --port=8001 &

        export HOSTNAME_PORT=localhost:8001 (example host and port number)
        export NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from "kubectl get nodes")

        curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
                       For all three remediation methods:

        Restart the                  kubelet
         service and check status. The example below is for when using systemctl to manage services:
        systemctl daemon-reload
        systemctl restart kubelet.service
        systemctl status kubelet -l
                       Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_3.2.9', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'should cmp == RotateKubeletServerCertificate":true'

  "4.1.1":
    title: '4.1.1 | Ensure that the cluster-admin role is only used where required'
    section: 'RBAC and Service Accounts'
    description: |
        The RBAC role              clusteradmin
         provides wideranging powers over the environment and should be used only where and when needed.
    remediation: |
        Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they
        could use a role with fewer privileges. Where possible, first bind users to a lower-privileged role and then remove the
        clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]
         Impact:
        Care should be taken before removing any                    clusterrolebindings
         from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should
        not be made to                    clusterrolebindings
         with the                    system:
         prefix as they are required for the operation of system components.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.2":
    title: '4.1.2 | Minimize access to secrets'
    section: 'RBAC and Service Accounts'
    description: |
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by
        workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to
        reduce the risk of privilege escalation.
    remediation: |
        Where possible, remove                  get
        ,                  list
         and                  watch
         access to                  secret
         objects in the cluster.               Impact: Care should be taken not to remove access to secrets to system components
        which require this for their operation
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.3":
    title: '4.1.3 | Minimize wildcard use in Roles and ClusterRoles'
    section: 'RBAC and Service Accounts'
    description: |
        Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on
        those objects.  It is possible to set either of these to be the wildcard , which matches all items. Use of wildcards is
        not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are
        added to the Kubernetes API either as CRDs or in later versions of the product.
    remediation: |
        Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.4":
    title: '4.1.4 | Ensure that default service accounts are not actively used'
    section: 'RBAC and Service Accounts'
    description: |
        The              default
         service account should not be used to ensure that rights granted to applications can be more easily audited and
        reviewed.
    remediation: |
        Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.
        Modify the configuration of each default service account to include this value automountServiceAccountToken: false
         Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be
        created.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.5":
    title: '4.1.5 | Ensure that Service Account Tokens are only mounted where necessary'
    section: 'RBAC and Service Accounts'
    description: |
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to
        communicate with the API server
    remediation: |
        Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
        Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where
        the resource is available to unauthenticated principals.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.5', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.1.6":
    title: '4.1.6 | Avoid use of systemmasters group'
    section: 'RBAC and Service Accounts'
    description: |
        The special group              systemmasters
         should not be used to grant permissions to any user or service account, except where strictly necessary e.g.
        bootstrapping access prior to RBAC being fully available
    remediation: |
        Remove the                  system:masters
         group from all users in the cluster.               Impact:
        Once the RBAC system is operational in a cluster                    system:masters
         should not be specifically required, as ordinary bindings from principals to the                    cluster-admin
         cluster role can be made where unrestricted access is required.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.6', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.7":
    title: '4.1.7 | Limit use of the Bind Impersonate and Escalate permissions in the Kubernetes cluster - manual'
    section: 'RBAC and Service Accounts'
    description: |
        Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly
        required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly
        granted by cluster administrators
    remediation: |
        Where possible, remove the impersonate, bind and escalate rights from subjects. Impact: There are some cases where these
        permissions are required for cluster service operation, and care should be taken before removing these permissions from
        system service accounts.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.1.7', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.1.8":
    title: '4.1.8 | Avoid bindings to systemanonymous'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid ClusterRoleBindings nor RoleBindings with the user              systemanonymous
        .
    remediation: |
        Identify all                  clusterrolebindings
         and                  rolebindings
         to the user system:anonymous. Check if they are used and review the permissions associated with the binding using the
        commands in the Audit section above or refer to GKE                  documentation
        .               Strongly consider replacing unsafe bindings with an authenticated, user-defined group. Where possible,
        bind to non-default, user-defined groups with least-privilege roles.
        If there are any unsafe bindings to the user                  system:anonymous
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact: Unauthenticated users will have privileges and permissions associated with roles associated with
        the configured bindings.
        Care should be taken before removing any                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Use a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.1.8', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "4.1.9":
    title: '4.1.9 | Avoid non-default bindings to systemunauthenticated'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid nondefault              ClusterRoleBindings
         and              RoleBindings
         with the group              systemunauthenticated
        , except the              ClusterRoleBinding
                           systempublicinfoviewer
        .
    remediation: |
        Identify all non-default                  clusterrolebindings
         and                  rolebindings
         to the group                  system:unauthenticated
        . Check if they are used and review the permissions associated with the binding using the commands in the Audit section
        above or refer to GKE                  documentation
        .               Strongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group.
        Where possible, bind to non-default, user-defined groups with least-privilege roles.
        If there are any non-default, unsafe bindings to the group                  system:unauthenticated
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --
        namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact: Unauthenticated users will have privileges and permissions associated with roles associated with
        the configured bindings.
        Care should be taken before removing any non-default                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.9', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.1.10":
    title: '4.1.10 | Avoid non-default bindings to systemauthenticated'
    section: 'RBAC and Service Accounts'
    description: |
        Avoid nondefault              ClusterRoleBindings
         and              RoleBindings
         with the group              systemauthenticated
        , except the              ClusterRoleBindings
                           systembasicuser
        ,              systemdiscovery
        , and              systempublicinfoviewer
        .
        Googles approach to authentication is to make authenticating to Google Cloud and GKE as simple and secure as possible
        without adding complex configuration steps. The group              systemauthenticated
         includes all users with a Google account, which includes all Gmail accounts. Consider your authorization controls with
        this extended group scope when granting permissions. Thus, group              systemauthenticated
         is not recommended for nondefault use.
    remediation: |
        Identify all non-default                  clusterrolebindings
         and                  rolebindings
         to the group                  system:authenticated
        . Check if they are used and review the permissions associated with the binding using the commands in the Audit section
        above or refer to GKE documentation.               Strongly consider replacing non-default, unsafe bindings with an
        authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.
        If there are any non-default, unsafe bindings to the group                  system:authenticated
        , proceed to delete them after consideration for cluster operations with only necessary, safer bindings.
        kubectl delete clusterrolebinding
        [CLUSTER_ROLE_BINDING_NAME]

        kubectl delete rolebinding
        [ROLE_BINDING_NAME]
        --namespace
        [ROLE_BINDING_NAMESPACE]
                       Impact:
        Authenticated users in group                    system:authenticated
         should be treated similarly to users in                    system:unauthenticated
        , having privileges and permissions associated with roles associated with the configured bindings.
        Care should be taken before removing any non-default                    clusterrolebindings
         or                    rolebindings
         from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and
        authenticated user for cluster operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_4.1.10', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "4.2.1":
    title: '4.2.1 | Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces. - manual'
    section: 'Pod Security Standards'
    description: |
        The Pod Security Standard Baseline profile defines a baseline for container security. You can enforce this by using the
        builtin Pod Security Admission controller.
    remediation: |
        Ensure that Pod Security Admission is in place for every namespace which contains user workloads. Run the following
        command to enforce the Baseline profile in a namespace: kubectl label namespace  pod-
        security.kubernetes.io/enforce=baseline Impact: Enforcing a baseline profile will limit the use of containers.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.2.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.3.1":
    title: '4.3.1 | Ensure that the CNI in use supports Network Policies - manual'
    section: 'Network Policies and CNI'
    description: |
        There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may
        not be possible to effectively restrict traffic in the cluster.
    remediation: |
        To use a CNI plugin with Network Policy, enable Network Policy in GKE, and the CNI plugin will be updated. See
        recommendation 5.6.7. Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.3.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.3.2":
    title: '4.3.2 | Ensure that all Namespaces have Network Policies defined'
    section: 'Network Policies and CNI'
    description: |
        Use network policies to isolate traffic in the cluster network.
    remediation: |
        Follow the documentation and create                  NetworkPolicy
         objects as needed.
        See:                  https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy
         for more information.               Impact: Once network policies are in use within a given namespace, traffic not
        explicitly allowed by a network policy will be denied.  As such it is important to ensure that, when introducing network
        policies, legitimate traffic is not blocked.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.3.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.1":
    title: '4.4.1 | Prefer using secrets as files over secrets as environment variables'
    section: 'Secrets Management'
    description: |
        Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment
        variable secrets.
    remediation: |
        If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.
        Impact: Application code which expects to read secrets in the form of environment variables would need modification
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.4.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.4.2":
    title: '4.4.2 | Consider external secret storage - manual'
    section: 'Secrets Management'
    description: |
        Consider the use of an external secrets storage and management system instead of using Kubernetes Secrets directly, if
        more complex secret management is required. Ensure the solution requires authentication to access secrets, has auditing
        of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.
    remediation: |
        Refer to the secrets management options offered by the cloud service provider or a third-party secrets management
        solution. Impact: None
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.4.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.5.1":
    title: '4.5.1 | Configure Image Provenance using ImagePolicyWebhook admission controller - manual'
    section: 'Extensible Admission Control'
    description: |
        Configure Image Provenance for the deployment.
    remediation: |
        Follow the Kubernetes documentation and setup image provenance. Also see recommendation 5.10.4. Impact: Regular
        maintenance for the provenance configuration should be carried out, based on container image updates.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.5.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.1":
    title: '4.6.1 | Create administrative boundaries between resources using namespaces - manual'
    section: 'General Policies'
    description: |
        Use namespaces to isolate your Kubernetes objects.
    remediation: |
        Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to
        switch between namespaces for administration.
    type: Undefined
    impact: '0.0'
    tags: ['level1', 'rule_4.6.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.2":
    title: '4.6.2 | Ensure that the seccomp profile is set to RuntimeDefault in the pod definitions'
    section: 'General Policies'
    description: |
        Enable              RuntimeDefault
         seccomp profile in the pod definitions.
    remediation: |
        Use security context to enable the                  RuntimeDefault
         seccomp profile in your pod definitions. An example is as below:
        {
          "namespace": "kube-system",
          "name": "metrics-server-v0.7.0-dbcc8ddf6-gz7d4",
          "seccompProfile": "RuntimeDefault"
        }
                       Impact:
        If the                    RuntimeDefault
         seccomp profile is too restrictive for you, you would have to create/manage your own                    Localhost
         seccomp profiles.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.6.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "4.6.3":
    title: '4.6.3 | Apply Security Context to Pods and Containers - manual'
    section: 'General Policies'
    description: |
        Apply Security Context to Pods and Containers
    remediation: |
        Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts,
        you may refer to the CIS Google Container-Optimized OS Benchmark. Impact: If you incorrectly apply security contexts,
        there may be issues running the pods.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_4.6.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "4.6.4":
    title: '4.6.4 | The default namespace should not be used'
    section: 'General Policies'
    description: |
        Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing
        objects in this namespace makes application of RBAC and other controls more difficult.
    remediation: |
        Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new
        resources are created in a specific namespace. Impact: None
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_4.6.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.1.1":
    title: '5.1.1 | Ensure Image Vulnerability Scanning is enabled'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, being superseded by Artifact Registry starting 15th May 2024. Runtime Vulnerability scanning
        is available via GKE Security Posture Scan images stored in Google Container Registry GCR or Artifact Registry AR for
        vulnerabilities.
    remediation: |
        For Images Hosted in GCR: Using Google Cloud Console
        Go to GCR by visiting:                    https://console.cloud.google.com/gcr
                                 Select Settings and, under the Vulnerability Scanning heading, click the TURN ON button. Using
        Command Line gcloud services enable containeranalysis.googleapis.com
         For Images Hosted in AR: Using Google Cloud Console
        Go to GCR by visiting:                    https://console.cloud.google.com/artifacts
                                 Select Settings and, under the Vulnerability Scanning heading, click the ENABLE button. Using
        Command Line gcloud services enable containerscanning.googleapis.com
         Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.1.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.1.2":
    title: '5.1.2 | Minimize user access to Container Image repositories - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, see the references for more details. Restrict user access to GCR or AR, limiting interaction
        with build images to only authorized personnel and service accounts.
    remediation: |
        For Images Hosted in AR: Using Google Cloud Console:
        Go to Artifacts Browser by visiting                    https://console.cloud.google.com/artifacts

        From the list of artifacts select each repository with format                    Docker
                                 Under the Permissions tab, modify the roles for each member and ensure only authorized users
        have the Artifact Registry Administrator, Artifact Registry Reader, Artifact Registry Repository Administrator and
        Artifact Registry Writer roles. Using Command Line: gcloud artifacts repositories set-iam-policy <repository-name>
        <path-to-policy-file> --location <repository-location>

        To learn how to configure policy files see:                  https://cloud.google.com/artifact-registry/docs/access-
        control#grant
                              For Images Hosted in GCR: Using Google Cloud Console: To modify roles granted at the GCR bucket
        level:
        Go to Storage Browser by visiting:                    https://console.cloud.google.com/storage/browser
        .
        From the list of storage buckets, select                    artifacts.<project_id>.appspot.com
         for the GCR bucket
        Under the Permissions tab, modify permissions of the identified member via the drop-down role menu and change the Role
        to                    Storage Object Viewer
         for read-only access.
        For a User or Service account with Project level permissions inherited by the GCR bucket, or the
        Service Account User Role
        :
        Go to IAM by visiting:                    https://console.cloud.google.com/iam-admin/iam
                                 Find the User or Service account to be modified and click on the corresponding pencil icon.
        Remove the                    create
        /                   modify
         role (                   Storage Admin
         /                    Storage Object Admin
         /                    Storage Object Creator
         /                    Service Account User
        ) on the user or service account.
        If required add the                    Storage Object Viewer
         role - note with caution that this permits the account to view all objects stored in GCS for the project.
        Using Command Line: To change roles at the GCR bucket level:
        Firstly, run the following if read permissions are required: gsutil iam ch <type>:<email_address>:objectViewer
        gs://artifacts.<project_id>.appspot.com

        Then remove the excessively privileged role (                 Storage Admin
         /                  Storage Object Admin
         /                  Storage Object Creator
        ) using:               gsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com
         where: <type>
         can be one of the following:
                           user
        , if the                        <email_address>
         is a Google account.                     serviceAccount
        , if                        <email_address>
         specifies a Service account.                     <email_address>
         can be one of the following:

        a Google account (for example,                            someone@example.com
        ).                         a Cloud IAM service account.

                                 To modify roles defined at the project level and subsequently inherited within the GCR bucket,
        or the Service Account User role, extract the IAM policy file, modify it accordingly and apply it using: gcloud projects
        set-iam-policy <project_id> <policy_file>
         Impact: Care should be taken not to remove access to GCR or AR for accounts that require this for their operation.
        Any account granted the Storage Object Viewer role at the project level can view all objects stored in GCS for the
        project.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.3":
    title: '5.1.3 | Minimize cluster access to read-only for Container Image repositories - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Note GCR is now deprecated, see the references for more details. Configure the Cluster Service Account with Artifact
        Registry Viewer Role to only allow readonly access to AR repositories.
        Configure the Cluster Service Account with Storage Object Viewer Role to only allow readonly access to GCR.
    remediation: |
        For Images Hosted in AR: Using Google Cloud Console:
        Go to Artifacts Browser by visiting                    https://console.cloud.google.com/artifacts
                                 From the list of repositories, for each repository with Format Docker Under the Permissions
        tab, modify the permissions for GKE Service account and ensure that only the Artifact Registry Viewer role is set. Using
        Command Line:
        Add artifactregistry.reader role
        gcloud artifacts repositories add-iam-policy-binding <repository> \
        --location=<repository-location> \
        --member='serviceAccount:<email-address>' \
        --role='roles/artifactregistry.reader'

        Remove any roles other than                  artifactregistry.reader

        gcloud artifacts repositories remove-iam-policy-binding <repository> \
        --location <repository-location> \
        --member='serviceAccount:<email-address>' \
        --role='<role-name>'
                       For Images Hosted in GCR: Using Google Cloud Console: For an account explicitly granted access to the
        bucket:
        Go to Storage Browser by visiting:                    https://console.cloud.google.com/storage/browser
        .
        From the list of storage buckets, select                    artifacts.<project_id>.appspot.com
         for the GCR bucket.
        Under the Permissions tab, modify permissions of the identified GKE Service Account via the drop-down role menu and
        change to the Role to                    Storage Object Viewer
         for read-only access.                 For an account that inherits access to the bucket through Project level
        permissions:
        Go to IAM console by visiting:                    https://console.cloud.google.com/iam-admin
        .                 From the list of accounts, identify the required service account and select the corresponding pencil
        icon.
        Remove the                    Storage Admin
         /                    Storage Object Admin
         /                    Storage Object Creator
         roles.
        Add the                    Storage Object Viewer
         role - note with caution that this permits the account to view all objects stored in GCS for the project.
        Click                    SAVE
        .                 Using Command Line: For an account explicitly granted to the bucket:
        Firstly add read access to the Kubernetes Service Account: gsutil iam ch <type>:<email_address>:objectViewer
        gs://artifacts.<project_id>.appspot.com
         where: <type>
         can be one of the following:
                           user
        , if the                        <email_address>
         is a Google account.                     serviceAccount
        , if                        <email_address>
         specifies a Service account.                     <email_address>
         can be one of the following:

        a Google account (for example,                            someone@example.com
        ).                         a Cloud IAM service account.


        Then remove the excessively privileged role (                 Storage Admin
         /                  Storage Object Admin
         /                  Storage Object Creator
        ) using:               gsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com
         For an account that inherits access to the GCR Bucket through Project level permissions, modify the Projects IAM policy
        file accordingly, then upload it using: gcloud projects set-iam-policy <project_id> <policy_file>
         Impact: A separate dedicated service account may be required for use by build servers and other robot users pushing or
        managing container images. Any account granted the Storage Object Viewer role at the project level can view all objects
        stored in GCS for the project.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.1.4":
    title: '5.1.4 | Ensure only trusted container images are used - manual'
    section: 'Image Registry and Image Scanning'
    description: |
        Use Binary Authorization to allowlist whitelist only approved container registries.
    remediation: |
        Using Google Cloud Console:
        Go to Binary Authorization by visiting:                    https://console.cloud.google.com/security/binary-
        authorization
                                 Enable Binary Authorization API (if disabled).
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes cluster for which Binary Authorization is disabled.
        Within the                    Details
         pane, under the                    Security
         heading, click on the pencil icon called                    Edit binary authorization
        .
        Ensure that                    Enable Binary Authorization
         is checked.
        Click                    SAVE CHANGES
        .
        Return to the Binary Authorization by visiting:                    https://console.cloud.google.com/security/binary-
        authorization
        .                 Set an appropriate policy for the cluster and enter the approved container registries under Image
        paths. Using Command Line: Update the cluster to enable Binary Authorization: gcloud container cluster update
        <cluster_name> --enable-binauthz

        Create a Binary Authorization Policy using the Binary Authorization Policy Reference:
        https://cloud.google.com/binary-authorization/docs/policy-yaml-reference
         for guidance.               Import the policy file into Binary Authorization: gcloud container binauthz policy import
        <yaml_policy>
         Impact: All container images to be deployed to the cluster must be hosted within an approved container image registry.
        If public registries are not on the allowlist, a process for bringing commonly used container images into an approved
        private registry and keeping them up to date will be required.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.1.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.2.1":
    title: '5.2.1 | Ensure GKE clusters are not running using the Compute Engine default service account'
    section: 'Identity and Access Management (IAM)'
    description: |
        Create and use minimally privileged Service accounts to run GKE cluster nodes instead of using the Compute Engine
        default Service account. Unnecessary permissions could be abused in the case of a node compromise.
    remediation: |
        Using Google Cloud Console: To create a minimally privileged service account:
        Go to Service Accounts by visiting:                    https://console.cloud.google.com/iam-admin/serviceaccounts
        .
        Click on                    CREATE SERVICE ACCOUNT
        .                 Enter Service Account Details.
        Click                    CREATE AND CONTINUE
        .
        Within Service Account permissions add the following roles:
                           Logs Writer
        .                     Monitoring Metric Writer
        .                     `Monitoring Viewer.

        Click                    CONTINUE
        .                 Grant users access to this service account and create keys as required.
        Click                    DONE
        .                 To create a Node pool to use the Service account:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Click on the cluster name within which the Node pool will be launched.
        Click on                    ADD NODE POOL
        .
        Within the Node Pool details, select the                    Security
         subheading, and under `Identity defaults, select the minimally privileged service account from the Service Account
        drop-down.                 Click `CREATE to launch the Node pool. Note: The workloads will need to be migrated to the
        new Node pool, and the old node pools that use the default service account should be deleted to complete the
        remediation. Using Command Line: To create a minimally privileged service account:
        gcloud iam service-accounts create <node_sa_name> --display-name "GKE Node Service Account"
        export NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service
        Account'
                       Grant the following roles to the service account:
        export PROJECT_ID=gcloud config get-value project
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/monitoring.metricWriter
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/monitoring.viewer
        gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role
        roles/logging.logWriter
                       To create a new Node pool using the Service account, run the following command: gcloud container node-
        pools create <node_pool> --service-account=<sa_name>@<project_id>.iam.gserviceaccount.com--cluster=<cluster_name> --zone
        <compute_zone>
         Note: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service
        account should be deleted to complete the remediation. Impact:
        Instances are automatically granted the                    https://www.googleapis.com/auth/cloud-platform
         scope to allow full access to all Google Cloud APIs. This is so that the IAM permissions of the instance are completely
        determined by the IAM roles of the Service account. Thus if Kubernetes workloads were using cluster access scopes to
        perform actions using Google APIs, they may no longer be able to, if not permitted by the permissions of the Service
        account. To remediate, follow recommendation 5.2.2.                 The Service account roles listed here are the
        minimum required to run the cluster. Additional roles may be required to pull from a private instance of Google
        Container Registry (GCR).
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.2.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.2.2":
    title: '5.2.2 | Prefer using dedicated GCP Service Accounts and Workload Identity - manual'
    section: 'Identity and Access Management (IAM)'
    description: |
        Kubernetes workloads should not use cluster node service accounts to authenticate to Google Cloud APIs. Each Kubernetes
        Workload that needs to authenticate to other Google services using Cloud IAM should be provisioned a dedicated Service
        account. Enabling Workload Identity manages the distribution and rotation of Service account keys for the workloads to
        use.
    remediation: |
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 From the list of clusters, select the cluster for which Workload Identity is disabled.
        Within the                    Details
         pane, under the                    Security
         section, click on the pencil icon named                    Edit workload identity
        .
        Enable Workload Identity and set the workload pool to the namespace of the Cloud project containing the cluster, for
        example:                    <project_id>.svc.id.goog
        .
        Click                    SAVE CHANGES
         and wait for the cluster to update.                 Once the cluster has updated, select each Node pool within the
        cluster Details page.
        For each Node pool, select                    EDIT
         within the Node pool Details page
        Within the Edit node pool pane, check the 'Enable GKE Metadata Server' checkbox and click                    SAVE
        .                 Using Command Line: gcloud container clusters update <cluster_name> --zone <cluster_zone> --workload-
        pool <project_id>.svc.id.goog

        Note that existing Node pools are unaffected. New Node pools default to                  --workload-metadata-from-
        node=GKE_METADATA_SERVER
        .
        Then, modify existing Node pools to enable                  GKE_METADATA_SERVER
        :               gcloud container node-pools update <node_pool_name> --cluster <cluster_name> --zone <cluster_zone>
        --workload-metadata=GKE_METADATA

        Workloads may need to be modified in order for them to use Workload Identity as described within:
        https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
        . Also consider the effects on the availability of hosted workloads as Node pools are updated. It may be more
        appropriate to create new Node Pools.               Impact: Workload Identity replaces the need to use Metadata
        Concealment and as such, the two approaches are incompatible. The sensitive metadata protected by Metadata Concealment
        is also protected by Workload Identity.
        When Workload Identity is enabled, the Compute Engine default Service account can not be used. Correspondingly, Workload
        Identity can't be used with Pods running in the host network. Workloads may also need to be modified in order for them
        to use Workload Identity, as described within:                    https://cloud.google.com/kubernetes-engine/docs/how-
        to/workload-identity
                                 GKE infrastructure pods such as Stackdriver will continue to use the Node's Service account.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.2.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.3.1":
    title: '5.3.1 | Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS'
    section: 'Cloud Key Management Service (Cloud KMS)'
    description: |
        Encrypt Kubernetes secrets, stored in etcd, at the applicationlayer using a customermanaged key in Cloud KMS.
    remediation: |
        To enable Application-layer Secrets Encryption, several configuration items are required. These include: A key ring A
        key
        A GKE service account with                    Cloud KMS CryptoKey Encrypter/Decrypter
         role                 Once these are created, Application-layer Secrets Encryption can be enabled on an existing or new
        cluster. Using Google Cloud Console: To create a key
        Go to Cloud KMS by visiting                    https://console.cloud.google.com/security/kms
        .
        Select                    CREATE KEY RING
        .                 Enter a Key ring name and the region where the keys will be stored.
        Click                    CREATE
        .                 Enter a Key name and appropriate rotation period within the Create key pane.
        Click                    CREATE
        .                 To enable on a new cluster
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .
        Click                    CREATE CLUSTER
        , and choose the required cluster mode.
        Within the                    Security
         heading, under                    CLUSTER
        , check                    Encrypt secrets at the application layer
         checkbox.                 Select the kms key as the customer-managed key and, if prompted, grant permissions to the GKE
        Service account.
        Click                    CREATE
        .                 To enable on an existing cluster
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the cluster to be updated. Under the Details pane, within the Security heading, click on the
        pencil named Application-layer secrets encryption.
        Enable                    Encrypt secrets at the application layer
         and choose a kms key.
        Click                    SAVE CHANGES
        .                 Using Command Line: To create a key:
        Create a key ring: gcloud kms keyrings create <ring_name> --location <location> --project <key_project_id>
         Create a key: gcloud kms keys create <key_name> --location <location> --keyring <ring_name> --purpose encryption
        --project <key_project_id>

        Grant the Kubernetes Engine Service Agent service account the                  Cloud KMS CryptoKey Encrypter/Decrypter
         role:               gcloud kms keys add-iam-policy-binding <key_name> --location <location> --keyring <ring_name>
        --member serviceAccount:<service_account_name> --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project
        <key_project_id>
         To create a new cluster with Application-layer Secrets Encryption: gcloud container clusters create <cluster_name>
        --cluster-version=latest --zone <zone> --database-encryption-key
        projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project <cluster_project_id>
         To enable on an existing cluster: gcloud container clusters update <cluster_name> --zone <zone> --database-encryption-
        key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project
        <cluster_project_id>
         Impact: To use the Cloud KMS CryptoKey to protect etcd in the cluster, the 'Kubernetes Engine Service Agent' Service
        account must hold the 'Cloud KMS CryptoKey Encrypter/Decrypter' role.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.3.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.4.1":
    title: '5.4.1 | Ensure the GKE Metadata Server is Enabled'
    section: 'Node Metadata'
    description: |
        Running the GKE Metadata Server prevents workloads from accessing sensitive instance metadata and facilitates Workload
        Identity.
    remediation: |
        The GKE Metadata Server requires Workload Identity to be enabled on a cluster. Modify the cluster to enable Workload
        Identity and enable the GKE Metadata Server. Using Google Cloud Console
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 From the list of clusters, select the cluster for which Workload Identity is disabled.
        Under the                    DETAILS
         pane, navigate down to the                    Security
         subsection.
        Click on the pencil icon named                    Edit Workload Identity
        , click on                    Enable Workload Identity
         in the pop-up window, and select a workload pool from the drop-down box. By default, it will be the namespace of the
        Cloud project containing the cluster, for example:                    <project_id>.svc.id.goog
        .
        Click                    SAVE CHANGES
         and wait for the cluster to update.                 Once the cluster has updated, select each Node pool within the
        cluster Details page.
        For each Node pool, select                    EDIT
         within the Node pool details page.
        Within the                    Edit node pool
         pane, check the                    Enable GKE Metadata Server
         checkbox.
        Click                    SAVE
        .                 Using Command Line gcloud container clusters update <cluster_name> --identity-
        namespace=<project_id>.svc.id.goog

        Note that existing Node pools are unaffected. New Node pools default to                  --workload-metadata-from-
        node=GKE_METADATA_SERVER
        .               To modify an existing Node pool to enable GKE Metadata Server: gcloud container node-pools update
        <node_pool_name> --cluster=<cluster_name> --workload-metadata-from-node=GKE_METADATA_SERVER

        Workloads may need modification in order for them to use Workload Identity as described within:
        https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
        .               Impact: The GKE Metadata Server must be run when using Workload Identity. Because Workload Identity
        replaces the need to use Metadata Concealment, the two approaches are incompatible. When the GKE Metadata Server and
        Workload Identity are enabled, unless the Pod is running on the host network, Pods cannot use the the Compute Engine
        default service account.
        Workloads may need modification in order for them to use Workload Identity as described within:
        https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
        .
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.4.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.1":
    title: '5.5.1 | Ensure Container-Optimized OS cos containerd is used for GKE node images'
    section: 'Node Configuration and Maintenance'
    description: |
        Use ContainerOptimized OS coscontainerd as a managed, optimized and hardened base OS that limits the hosts attack
        surface.
    remediation: |
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the Kubernetes cluster which does not use COS. Under the Node pools heading, select the Node
        Pool that requires alteration.
        Click                    EDIT
        .
        Under the Image Type heading click                    CHANGE
        .
        From the pop-up menu select                    Container-optimised OS with containerd (cos_containerd) (default)
         and click                    CHANGE
                                 Repeat for all non-compliant Node pools. Using Command Line:
        To set the node image to                  cos
         for an existing cluster's Node pool:               gcloud container clusters upgrade <cluster_name> --image-type
        cos_containerd --zone <compute_zone> --node-pool <node_pool_name>
         Impact: If modifying an existing cluster's Node pool to run COS, the upgrade operation used is long-running and will
        block other operations on the cluster (including delete) until it has run to completion.
        COS nodes also provide an option with                    containerd
         as the main container runtime directly integrated with Kubernetes instead of                    docker
        . Thus, on these nodes, Docker cannot view or access containers or images managed by Kubernetes. Applications should not
        interact with Docker directly. For general troubleshooting or debugging, use crictl instead.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.5.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.2":
    title: '5.5.2 | Ensure Node Auto-Repair is enabled for GKE nodes'
    section: 'Node Configuration and Maintenance'
    description: |
        Nodes in a degraded state are an unknown quantity and so may pose a security risk.
    remediation: |
        Using Google Cloud Console
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
                                 Select the Kubernetes cluster containing the node pool for which auto-repair is disabled.
        Select the Node pool by clicking on the name of the pool.
        Navigate to the Node pool details pane and click                    EDIT
        .
        Under the                    Management
         heading, check the                    Enable auto-repair
         box.
        Click                    SAVE
        .                 Repeat steps 2-6 for every cluster and node pool with auto-upgrade disabled. Using Command Line To
        enable node auto-repair for an existing cluster's Node pool: gcloud container node-pools update <node_pool_name>
        --cluster <cluster_name> --zone <compute_zone> --enable-autorepair
         Impact: If multiple nodes require repair, Kubernetes Engine might repair them in parallel. Kubernetes Engine limits
        number of repairs depending on the size of the cluster (bigger clusters have a higher limit) and the number of broken
        nodes in the cluster (limit decreases if many nodes are broken). Node auto-repair is not available on Alpha Clusters.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.5.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.3":
    title: '5.5.3 | Ensure Node Auto-Upgrade is enabled for GKE nodes'
    section: 'Node Configuration and Maintenance'
    description: |
        Node autoupgrade keeps nodes at the current Kubernetes and OS security patch level to mitigate known vulnerabilities.
    remediation: |
        Using Google Cloud Console
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the Kubernetes cluster containing the node pool for which auto-upgrade disabled. Select the
        Node pool by clicking on the name of the pool.
        Navigate to the Node pool details pane and click                    EDIT
        .
        Under the Management heading, check the                    Enable auto-repair
         box.
        Click                    SAVE
        .                 Repeat steps 2-6 for every cluster and node pool with auto-upgrade disabled. Using Command Line To
        enable node auto-upgrade for an existing cluster's Node pool, run the following command: gcloud container node-pools
        update <node_pool_name> --cluster <cluster_name> --zone <cluster_zone> --enable-autoupgrade
         Impact: Enabling node auto-upgrade does not cause the nodes to upgrade immediately. Automatic upgrades occur at regular
        intervals at the discretion of the Kubernetes Engine team. To prevent upgrades occurring during a peak period for the
        cluster, a maintenance window should be defined. A maintenance window is a four-hour timeframe that can be chosen,
        during which automatic upgrades should occur. Upgrades can occur on any day of the week, and at any time within the
        timeframe. To prevent upgrades from occurring during certain dates, a maintenance exclusion should be defined. A
        maintenance exclusion can span multiple days.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.5.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.4":
    title: '5.5.4 | When creating New Clusters - Automate GKE version management using Release Channels'
    section: 'Node Configuration and Maintenance'
    description: |
        Subscribe to the Regular or Stable Release Channel to automate version upgrades to the GKE cluster and to reduce version
        management complexity to the number of features and level of stability required.
    remediation: |
        Currently, cluster Release Channels are only configurable at cluster provisioning time. Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .
        Click                    CREATE
        , and choose                    CONFIGURE
         for the required cluster mode.
        Under the Control plane version heading, click the                    Release Channels
         button.
        Select the                    Regular
         or                    Stable
         channels from the Release Channel drop-down menu.                 Configure the rest of the cluster settings as
        required.
        Click                    CREATE
        .                 Using Command Line:
        Create a new cluster by running the following command: gcloud container clusters create <cluster_name> --zone
        <cluster_zone> --release-channel <release_channel>

        where                  <release_channel>
         is                  stable
         or                  regular
        , according to requirements.               Impact:
        Once release channels are enabled on a cluster, they cannot be disabled. To stop using release channels, the cluster
        must be recreated without the                    --release-channel
         flag.                 Node auto-upgrade is enabled (and cannot be disabled), so the cluster is updated automatically
        from releases available in the chosen release channel.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.5.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.5.5":
    title: '5.5.5 | Ensure Shielded GKE Nodes are Enabled'
    section: 'Node Configuration and Maintenance'
    description: |
        Shielded GKE Nodes provides verifiable integrity via secure boot, virtual trusted platform module vTPMenabled measured
        boot, and integrity monitoring.
    remediation: |
        Note: From version 1.18, clusters will have Shielded GKE nodes enabled by default. Using Google Cloud Console: To update
        an existing cluster to use Shielded GKE nodes:
        Navigate to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .
        Select the cluster which for which                    Shielded GKE Nodes
         is to be enabled.
        With in the                    Details
         pane, under the                    Security
         heading, click on the pencil icon named                    Edit Shields GKE nodes
        .
        Check the box named                    Enable Shield GKE nodes
        .
        Click                    SAVE CHANGES
        .                 Using Command Line:
        To migrate an existing cluster, the flag                  --enable-shielded-nodes
         needs to be specified in the cluster update command:               gcloud container clusters update <cluster_name>
        --zone <cluster_zone> --enable-shielded-nodes
         Impact: After Shielded GKE Nodes is enabled in a cluster, any nodes created in a Node pool without Shielded GKE Nodes
        enabled, or created outside of any Node pool, aren't able to join the cluster. Shielded GKE Nodes can only be used with
        Container-Optimized OS (COS), COS with containerd, and Ubuntu node images.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.5.5', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.6":
    title: '5.5.6 | Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled'
    section: 'Node Configuration and Maintenance'
    description: |
        Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.
    remediation: |
        Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created
        within the cluster with Integrity Monitoring enabled. Using Google Cloud Console
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list

        From the list of clusters, click on the cluster requiring the update and click                    ADD NODE POOL
        .                 Ensure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading.
        Click                    SAVE
        .                 Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node
        pool, then delete non-conforming Node pools to complete the remediation Using Command Line To create a Node pool within
        the cluster with Integrity Monitoring enabled, run the following command: gcloud container node-pools create
        <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-integrity-monitoring
         Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete
        non-conforming Node pools to complete the remediation Impact: None.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.5.6', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.5.7":
    title: '5.5.7 | Ensure Secure Boot for Shielded GKE Nodes is Enabled'
    section: 'Node Configuration and Maintenance'
    description: |
        Enable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.
    remediation: |
        Once a Node pool is provisioned, it cannot be updated to enable Secure Boot. New Node pools must be created within the
        cluster with Secure Boot enabled. Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list

        From the list of clusters, click on the cluster requiring the update and click                    ADD NODE POOL
        .
        Ensure that the                    Secure boot
         checkbox is checked under the                    Shielded options
         Heading.
        Click                    SAVE
        .                 Workloads will need to be migrated from existing non-conforming Node pools to the newly created Node
        pool, then delete the non-conforming pools. Using Command Line: To create a Node pool within the cluster with Secure
        Boot enabled, run the following command: gcloud container node-pools create <node_pool_name> --cluster <cluster_name>
        --zone <compute_zone> --shielded-secure-boot
         Workloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete
        the non-conforming pools. Impact: Secure Boot will not permit the use of third-party unsigned kernel modules.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.5.7', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.1":
    title: '5.6.1 | Enable VPC Flow Logs and Intranode Visibility'
    section: 'Cluster Networking'
    description: |
        Enable VPC Flow Logs and Intranode Visibility to see podlevel traffic, even for traffic within a worker node.
    remediation: |
        Enable Intranode Visibility:
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes clusters for which intranode visibility is disabled.
        Within the                    Details
         pane, under the                    Network
         section, click on the pencil icon named                    Edit intranode visibility
        .
        Check the box next to                    Enable Intranode visibility
        .
        Click                    SAVE CHANGES
        .                 Using Command Line: To enable intranode visibility on an existing cluster, run the following command:
        gcloud container clusters update <cluster_name> --enable-intra-node-visibility
         Enable VPC Flow Logs:
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes clusters for which VPC Flow Logs are disabled.
        Select                    Nodes
         tab.                 Select Node Pool without VPC Flow Logs enabled. Select an Instance Group within the node pool.
        Select an                    Instance Group Member
        .
        Select the                    Subnetwork
         under Network Interfaces.
        Click on                    EDIT
        .
        Set Flow logs to                    On
        .
        Click                    SAVE
        .                 Using Command Line: Find the subnetwork name associated with the cluster. gcloud container clusters
        describe <cluster_name> --region <cluster_region> --format json | jq '.subnetwork'
         Update the subnetwork to enable VPC Flow Logs. gcloud compute networks subnets update <subnet_name> --enable-flow-logs
         Impact: Enabling it on existing cluster causes the cluster master and the cluster nodes to restart, which might cause
        disruption.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.6.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.2":
    title: '5.6.2 | Ensure use of VPC-native clusters'
    section: 'Cluster Networking'
    description: |
        Create Alias IPs for the node network CIDR range in order to subsequently configure IPbased policies and firewalling for
        pods. A cluster that uses Alias IPs is called a VPCnative cluster.
    remediation: |
        Alias IPs cannot be enabled on an existing cluster. To create a new cluster using Alias IPs, follow the instructions
        below. Using Google Cloud Console: If using Standard configuration mode:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list

        Click                    CREATE CLUSTER
        , and select Standard configuration mode.
        Configure your cluster as desired , then, click                    Networking
         under                    CLUSTER
         in the navigation pane.                 In the 'VPC-native' section, leave 'Enable VPC-native (using alias IP)'
        selected Click CREATE. If using Autopilot configuration mode: Note that this is VPC-native only and cannot be disable:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Click CREATE CLUSTER, and select Autopilot configuration mode. Configure your cluster as required
        Click                    CREATE
        .                 Using Command Line To enable Alias IP on a new cluster, run the following command: gcloud container
        clusters create <cluster_name> --zone <compute_zone> --enable-ip-alias
         If using Autopilot configuration mode: gcloud container clusters create-auto <cluster_name> --zone <compute_zone>
         Impact: You cannot currently migrate an existing cluster that uses routes for Pod routing to a cluster that uses Alias
        IPs. Cluster IPs for internal services remain only available from within the cluster. If you want to access a Kubernetes
        Service from within the VPC, but from outside of the cluster, use an internal load balancer.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.6.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.3":
    title: '5.6.3 | Ensure Control Plane Authorized Networks is Enabled'
    section: 'Cluster Networking'
    description: |
        Enable Control Plane Authorized Networks to restrict access to the clusters control plane to only an allowlist of
        authorized IPs.
    remediation: |
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 Select Kubernetes clusters for which Control Plane Authorized Networks is disabled Within the
        Details pane, under the Networking heading, click on the pencil icon named Edit control plane authorised networks. Check
        the box next to Enable control plane authorised networks. Click SAVE CHANGES. Using Command Line: To enable Control
        Plane Authorized Networks for an existing cluster, run the following command: gcloud container clusters update
        <cluster_name> --zone <compute_zone> --enable-master-authorized-networks

        Along with this, you can list authorized networks using the                  --master-authorized-networks
         flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's control plane
        through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as
        90.90.100.0/24
        ).               Impact: When implementing Control Plane Authorized Networks, be careful to ensure all desired networks
        are on the allowlist to prevent inadvertently blocking external access to your cluster's control plane.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.6.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.4":
    title: '5.6.4 | Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled'
    section: 'Cluster Networking'
    description: |
        Disable access to the Kubernetes API from outside the node network if it is not required.
    remediation: |
        Once a cluster is created without enabling Private Endpoint only, it cannot be remediated. Rather, the cluster must be
        recreated. Using Google Cloud Console:
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 Click CREATE CLUSTER, and choose CONFIGURE for the Standard mode cluster. Configure the cluster
        as required then click Networking under CLUSTER in the navigation pane. Under IPv4 network access, click the Private
        cluster radio button. Uncheck the Access control plane using its external IP address checkbox. In the Control plane IP
        range textbox, provide an IP range for the control plane. Configure the other settings as required, and click CREATE.
        Using Command Line:
        Create a cluster with a Private Endpoint enabled and Public Access disabled by including the                  --enable-
        private-endpoint
         flag within the cluster create command:               gcloud container clusters create <cluster_name> --enable-private-
        endpoint

        Setting this flag also requires the setting of                  --enable-private-nodes
        ,                  --enable-ip-alias
         and                  --master-ipv4-cidr=<master_cidr_range>
        .               Impact: To enable a Private Endpoint, the cluster has to also be configured with private nodes, a
        private master IP range and IP aliasing enabled.
        If the Private Endpoint flag                    --enable-private-endpoint
         is passed to the gcloud CLI, or the external IP address undefined in the Google Cloud Console during cluster creation,
        then all access from a public IP address is prohibited.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.6.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.5":
    title: '5.6.5 | Ensure clusters are created with Private Nodes'
    section: 'Cluster Networking'
    description: |
        Private Nodes are nodes with no public IP addresses. Disable public IP addresses for cluster nodes, so that they only
        have private IP addresses.
    remediation: |
        Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated.
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Click CREATE CLUSTER. Configure the cluster as required then click Networking under CLUSTER in the
        navigation pane. Under IPv4 network access, click the Private cluster radio button. Configure the other settings as
        required, and click CREATE. Using Command Line:
        To create a cluster with Private Nodes enabled, include the                  --enable-private-nodes
         flag within the cluster create command:               gcloud container clusters create <cluster_name> --enable-private-
        nodes

        Setting this flag also requires the setting of                  --enable-ip-alias
         and                  --master-ipv4-cidr=<master_cidr_range>
        .               Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range
        and IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide
        outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway. To access
        Google Cloud APIs and services from private nodes, Private Google Access needs to be set on Kubernetes Engine Cluster
        Subnets.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.6.5', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.6.6":
    title: '5.6.6 | Consider firewalling GKE worker nodes - manual'
    section: 'Cluster Networking'
    description: |
        Reduce the network attack surface of GKE nodes by using Firewalls to restrict ingress and egress traffic.
    remediation: |
        Using Google Cloud Console:
        Go to Firewall Rules by visiting:                    https://console.cloud.google.com/networking/firewalls/list
                                 Click CREATE FIREWALL RULE.
        Configure the firewall rule as required. Ensure the firewall targets the nodes correctly, either selecting the nodes
        using tags (under Targets, select Specified target tags, and set Target tags to                    <tag>
        ), or using the Service account associated with node (under Targets, select Specified service account, set Service
        account scope as appropriate, and Target service account to                    <service_account>
        ).
        Click                    CREATE
        .                 Using Command Line: Use the following command to generate firewall rules, setting the variables as
        appropriate: gcloud compute firewall-rules create <firewall_rule_name> --network <network> --priority <priority>
        --direction <direction> --action <action> --target-tags <tag> --target-service-accounts <service_account> --source-
        ranges <source_cidr_range> --source-tags <source_tags> --source-service-accounts <source_service_account> --destination-
        ranges <destination_cidr_range> --rules <rules>
         Impact: All instances targeted by a firewall rule, either using a tag or a service account will be affected. Ensure
        there are no adverse effects on other instances using the target tag or service account before implementing the firewall
        rule.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.6.6', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.6.7":
    title: '5.6.7 | Ensure use of Google-managed SSL Certificates'
    section: 'Cluster Networking'
    description: |
        Encrypt traffic to HTTPS load balancers using Googlemanaged SSL certificates.
    remediation: |
        If services of                  type:LoadBalancer
         are discovered, consider replacing the Service with an Ingress.
        To configure the Ingress and use Google-managed SSL certificates, follow the instructions as listed at:
        https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
        .               Impact: Google-managed SSL Certificates are less flexible than certificates that are self obtained and
        managed. Managed certificates support a single, non-wildcard domain. Self-managed certificates can support wildcards and
        multiple subject alternative names (SANs).
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.6.7', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "5.7.1":
    title: '5.7.1 | Ensure Logging and Cloud Monitoring is Enabled'
    section: 'Logging'
    description: |
        Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.
    remediation: |
        Using Google Cloud Console:
        To enable Logging:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the cluster for which Logging is disabled.
        Under the details pane, within the Features section, click on the pencil icon named                    Edit logging
        .
        Check the box next to                    Enable Logging
        .                 In the drop-down Components box, select the components to be logged.
        Click                    SAVE CHANGES
        , and wait for the cluster to update.                 To enable Cloud Monitoring:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the cluster for which Logging is disabled.
        Under the details pane, within the Features section, click on the pencil icon named                    Edit Cloud
        Monitoring
        .
        Check the box next to                    Enable Cloud Monitoring
        .                 In the drop-down Components box, select the components to be logged.
        Click                    SAVE CHANGES
        , and wait for the cluster to update.                 Using Command Line:
        To enable Logging for an existing cluster, run the following command: gcloud container clusters update <cluster_name>
        --zone <compute_zone> --logging=<components_to_be_logged>
        See                  https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging
         for a list of available components for logging.               To enable Cloud Monitoring for an existing cluster, run
        the following command: gcloud container clusters update <cluster_name> --zone <compute_zone>
        --monitoring=<components_to_be_logged>
        See                  https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--monitoring
         for a list of available components for Cloud Monitoring.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.7.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.7.2":
    title: '5.7.2 | Enable Linux auditd logging - manual'
    section: 'Logging'
    description: |
        Run the auditd logging daemon to obtain verbose operating system logs from GKE nodes running ContainerOptimized OS COS.
    remediation: |
        Using Command Line: Download the example manifests: curl https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-node-
        tools/master/os-audit/cos-auditd-logging.yaml > cos-auditd-logging.yaml
         Edit the example manifests if needed. Then, deploy them: kubectl apply -f cos-auditd-logging.yaml

        Verify that the logging Pods have started. If a different Namespace was defined in the manifests, replace
        cos-auditd
         with the name of the namespace being used:               kubectl get pods --namespace=cos-auditd
         Impact: Increased logging activity on a node increases resource usage on that node, which may affect the performance of
        the workload and may incur additional resource costs. Audit logs sent to Stackdriver consume log quota from the project.
        The log quota may require increasing and storage to accommodate the additional logs. Note that the provided logging
        daemonset only works on nodes running Container-Optimized OS (COS).
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.7.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.8.1":
    title: '5.8.1 | Ensure authentication using Client Certificates is Disabled'
    section: 'Authentication and Authorization'
    description: |
        Disable Client Certificates, which require certificate rotation, for authentication. Instead, use another authentication
        method like OpenID Connect.
    remediation: |
        Currently, there is no way to remove a client certificate from an existing cluster. Thus a new cluster must be created.
        Using Google Cloud Console
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/list
                                 Click CREATE CLUSTER Configure as required and the click on 'Availability, networking,
        security, and additional features' section Ensure that the 'Issue a client certificate' checkbox is not ticked Click
        CREATE. Using Command Line Create a new cluster without a Client Certificate:
        gcloud container clusters create [CLUSTER_NAME] \
         --no-issue-client-certificate
                       Impact: Users will no longer be able to authenticate with the pre-provisioned x509 certificate. You will
        have to configure and use alternate authentication mechanisms, such as OpenID Connect tokens.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.8.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.8.2":
    title: '5.8.2 | Manage Kubernetes RBAC users with Google Groups for GKE - manual'
    section: 'Authentication and Authorization'
    description: |
        Cluster Administrators should leverage G Suite Groups and Cloud IAM to assign Kubernetes user roles to a collection of
        users, instead of to individual emails using only Cloud IAM.
    remediation: |
        Follow the G Suite Groups instructions at:                  https://cloud.google.com/kubernetes-engine/docs/how-to/role-
        based-access-control#google-groups-for-gke
        .               Then, create a cluster with: gcloud container clusters create <cluster_name> --security-group
        <security_group_name>

        Finally create                  Roles
        ,                  ClusterRoles
        ,                  RoleBindings
        , and                  ClusterRoleBindings
         that reference the G Suite Groups.               Impact:
        When migrating to using security groups, an audit of                    RoleBindings
         and                    ClusterRoleBindings
         is required to ensure all users of the cluster are managed using the new groups and not individually.
        When managing                    RoleBindings
         and                    ClusterRoleBindings
        , be wary of inadvertently removing bindings required by service accounts.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.8.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.8.3":
    title: '5.8.3 | Ensure Legacy Authorization ABAC is Disabled'
    section: 'Authentication and Authorization'
    description: |
        Legacy Authorization, also known as AttributeBased Access Control ABAC has been superseded by RoleBased Access Control
        RBAC and is not under active development.
        RBAC is the recommended way to manage permissions in Kubernetes.
    remediation: |
        Using Google Cloud Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select Kubernetes clusters for which Legacy Authorization is enabled. Click EDIT. Set 'Legacy
        Authorization' to 'Disabled'. Click SAVE. Using Command Line: To disable Legacy Authorization for an existing cluster,
        run the following command: gcloud container clusters update <cluster_name> --zone <compute_zone> --no-enable-legacy-
        authorization
         Impact: Once the cluster has the legacy authorizer disabled, the user must be granted the ability to create
        authorization roles using RBAC to ensure that the role-based access control permissions take effect.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.8.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.9.1":
    title: '5.9.1 | Enable Customer-Managed Encryption Keys CMEK for GKE Persistent Disks PD - manual'
    section: 'Storage'
    description: |
        Use CustomerManaged Encryption Keys CMEK to encrypt dynamicallyprovisioned attached Google Compute Engine Persistent
        Disks PDs using keys managed within Cloud Key Management Service Cloud KMS.
    remediation: |
        This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster
        created. Using Google Cloud Console: This is not possible using Google Cloud Console. Using Command Line:
        Follow the instructions detailed at:                  https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek
        .               Impact: Encryption of dynamically-provisioned attached disks requires the use of the self-provisioned
        Compute Engine Persistent Disk CSI Driver v0.5.1 or higher. If CMEK is being configured with a regional cluster, the
        cluster must run GKE 1.14 or higher.
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.9.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:

  "5.9.2":
    title: '5.9.2 | Enable Customer-Managed Encryption Keys CMEK for Boot Disks'
    section: 'Storage'
    description: |
        Use CustomerManaged Encryption Keys CMEK to encrypt node boot disks using keys managed within Cloud Key Management
        Service Cloud KMS.
    remediation: |
        This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster
        created. Using Google Cloud Console: To create a new node pool:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
                                 Select Kubernetes clusters for which node boot disk CMEK is disabled.
        Click                    ADD NODE POOL
        .
        In the Nodes section, under machine configuration, ensure Boot disk type is                    Standard persistent disk
         or                    SSD persistent disk
        .
        Select                    Enable customer-managed encryption for Boot Disk
         and select the Cloud KMS encryption key to be used.
        Click                    CREATE
        .                 To create a new cluster:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list

        Click                    CREATE
         and click `CONFIGURE for the required cluster mode.
        Under                    NODE POOLS, expand the default-pool list and click
        Nodes.
        In the Configure node settings pane, select                    Standard persistent disk
         or                    SSD Persistent Disk
         as the Boot disk type.
        Select                    Enable customer-managed encryption for Boot Disk
         check box and choose the Cloud KMS encryption key to be used.                 Configure the rest of the cluster
        settings as required.
        Click                    CREATE
        .                 Using Command Line:
        Create a new node pool using customer-managed encryption keys for the node boot disk, of                  <disk_type>
         either                  pd-standard
         or                  pd-ssd
        :               gcloud container node-pools create <cluster_name> --disk-type <disk_type> --boot-disk-kms-key
        projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name>

        Create a cluster using customer-managed encryption keys for the node boot disk, of                  <disk_type>
         either                  pd-standard
         or                  pd-ssd
        :               gcloud container clusters create <cluster_name> --disk-type <disk_type> --boot-disk-kms-key
        projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name>
         Impact: Encryption of dynamically-provisioned attached disks requires the use of the self-provisioned Compute Engine
        Persistent Disk CSI Driver v0.5.1 or higher. If CMEK is being configured with a regional cluster, the cluster must run
        GKE 1.14 or higher.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.9.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'
            - name: Undefined
              rule: 'Undefined'

  "5.10.1":
    title: '5.10.1 | Ensure Kubernetes Web UI is Disabled'
    section: 'Other Cluster Configurations'
    description: |
        Note The Kubernetes web UI Dashboard does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI
        is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI addon KubernetesDashboard
        is no longer supported as a managed addon. The Kubernetes Web UI Dashboard has been a historical source of vulnerability
        and should only be deployed when necessary.
    remediation: |
        Using Google Cloud Console: Currently not possible, due to the add-on having been removed. Must use the command line.
        Using Command Line: To disable the Kubernetes Dashboard on an existing cluster, run the following command: gcloud
        container clusters update <cluster_name> --zone <zone> --update-addons=KubernetesDashboard=DISABLED
         Impact:
        Users will be required to manage cluster resources using the Google Cloud Console or the command line. These require
        appropriate permissions. To use the command line, this requires the installation of the command line client,
        kubectl
        , on the user's device (this is already included in Cloud Shell) and knowledge of command line operations.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.10.1', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.10.2":
    title: '5.10.2 | Ensure that Alpha clusters are not used for production workloads'
    section: 'Other Cluster Configurations'
    description: |
        Alpha clusters are not covered by an SLA and are not productionready.
    remediation: |
        Alpha features cannot be disabled. To remediate, a new cluster must be created. Using Google Cloud Console
        Go to Kubernetes Engine by visiting                    https://console.cloud.google.com/kubernetes/
                                 Click CREATE CLUSTER, and choose "SWITCH TO STANDARD CLUSTER" in the upper right corner of the
        screen. Under Features in the the CLUSTER section, "Enable Kubernetes alpha features in this cluster" will not be
        available by default and to use Kubernetes alpha features in this cluster, first disable release channels.
        Note: It will only be available if the cluster is created with a Static version for the Control plane version, along
        with both Automatically upgrade nodes to the next available version and Enable auto-repair being checked under the Node
        pool details for each node. Configure the other settings as required and click CREATE. Using Command Line: Upon creating
        a new cluster
        gcloud container clusters create [CLUSTER_NAME] \
          --zone [COMPUTE_ZONE]
                       Do not use the --enable-kubernetes-alpha argument. Impact: Users and workloads will not be able to take
        advantage of features included within Alpha clusters.
    type: Undefined
    impact: '1.0'
    tags: ['level1', 'rule_5.10.2', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.10.3":
    title: '5.10.3 | Consider GKE Sandbox for running untrusted workloads'
    section: 'Other Cluster Configurations'
    description: |
        Use GKE Sandbox to restrict untrusted workloads as an additional layer of protection when running in a multitenant
        environment.
    remediation: |
        Once a node pool is created, GKE Sandbox cannot be enabled, rather a new node pool is required. The default node pool
        (the first node pool in your cluster, created when the cluster is created) cannot use GKE Sandbox. Using Google Cloud
        Console:
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/
        .
        Select a cluster and click                    ADD NODE POOL
        .
        Configure the Node pool with following settings:

        For the node version, select                        v1.12.6-gke.8
         or higher.
        For the node image, select                        Container-Optimized OS with Containerd (cos_containerd) (default)
        .
        Under                        Security
        , select                        Enable sandbox with gVisor
        .
                                 Configure other Node pool settings as required.
        Click                    SAVE
        .                 Using Command Line: To enable GKE Sandbox on an existing cluster, a new Node pool must be created,
        which can be done using:   gcloud container node-pools create <node_pool_name> --zone <compute-zone> --cluster
        <cluster_name> --image-type=cos_containerd --sandbox="type=gvisor"
         Impact:
        Using GKE Sandbox requires the node image to be set to Container-Optimized OS with containerd (
        cos_containerd
        ).                 It is not currently possible to use GKE Sandbox along with the following Kubernetes features:
        Accelerators such as GPUs or TPUs Istio Monitoring statistics at the level of the Pod or container Hostpath storage Per-
        container PID namespace CPU and memory limits are only applied for Guaranteed Pods and Burstable Pods, and only when CPU
        and memory limits are specified for all containers running in the Pod Pods using PodSecurityPolicies that specify host
        namespaces, such as hostNetwork, hostPID, or hostIPC Pods using PodSecurityPolicy settings such as privileged mode
        VolumeDevices Portforward Linux kernel security modules such as Seccomp, Apparmor, or Selinux Sysctl, NoNewPrivileges,
        bidirectional MountPropagation, FSGroup, or ProcMount
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.10.3', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:

  "5.10.4":
    title: '5.10.4 | Ensure use of Binary Authorization'
    section: 'Other Cluster Configurations'
    description: |
        Binary Authorization helps to protect supplychain security by only allowing images with verifiable cryptographically
        signed metadata into the cluster.
    remediation: |
        Using Google Cloud Console
        Go to Binary Authorization by visiting:                    https://console.cloud.google.com/security/binary-
        authorization
        .                 Enable the Binary Authorization API (if disabled).
        Create an appropriate policy for use with the cluster. See                    https://cloud.google.com/binary-
        authorization/docs/policy-yaml-reference
         for guidance.
        Go to Kubernetes Engine by visiting:                    https://console.cloud.google.com/kubernetes/list
        .                 Select the cluster for which Binary Authorization is disabled.
        Under the details pane, within the Security section, click on the pencil icon named                    Edit Binary
        Authorization
        .
        Check the box next to                    Enable Binary Authorization
        .
        Choose                    Enforce
         policy and provide a directory for the policy to be used.
        Click                    SAVE CHANGES
        .                 Using Command Line: Update the cluster to enable Binary Authorization:
        gcloud container cluster update <cluster_name> --zone <compute_zone> --binauthz-evaluation-mode=<evaluation_mode>

        Example:
        gcloud container clusters update $CLUSTER_NAME --zone $COMPUTE_ZONE --binauthz-evaluation-
        mode=PROJECT_SINGLETON_POLICY_ENFORCE

        See:                  https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--binauthz-evaluation-mode
         for more details around the evaluation modes available.
        Create a Binary Authorization Policy using the Binary Authorization Policy Reference:
        https://cloud.google.com/binary-authorization/docs/policy-yaml-reference
         for guidance.               Import the policy file into Binary Authorization: gcloud container binauthz policy import
        <yaml_policy>
         Impact: Care must be taken when defining policy in order to prevent inadvertent denial of container image deployments.
        Depending on policy, attestations for existing container images running within the cluster may need to be created before
        those images are redeployed or pulled as part of the pod churn. To prevent key system images from being denied
        deployment, consider the use of global policy evaluation mode, which uses a global policy provided by Google and exempts
        a list of Google-provided system images from further policy evaluation.
    type: Undefined
    impact: '1.0'
    tags: ['level2', 'rule_5.10.4', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: true
    properties:
      match: all
      rules:
        - checks:
            - name: Undefined
              rule: 'Undefined'

  "5.10.5":
    title: '5.10.5 | Enable Security Posture - manual'
    section: 'Other Cluster Configurations'
    description: |
        
    remediation: |
        Enable security posture via the UI, gCloud or API.
                         https://cloud.google.com/kubernetes-engine/docs/how-to/protect-workload-configuration
                              Impact:
        GKE security posture configuration auditing checks your workloads against a set of defined best practices.  Each
        configuration check has its own impact or risk.  Learn more about the checks:
        https://cloud.google.com/kubernetes-engine/docs/concepts/about-configuration-scanning
                                 Example: The host namespace check identifies pods that share host namespaces.  Pods that share
        host namespaces allow Pod processes to communicate with host processes and gather host information, which could lead to
        a container escape
    type: Undefined
    impact: '0.0'
    tags: ['level2', 'rule_5.10.5', 'cis_google_kubernetes_engine_(gke)_benchmark']
    enabled: false
    properties:
      match: all
      rules:
